{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Gateways - List Gateway Members\n","# Command:  GET https://api.fabric.microsoft.com/v1/gateways/{gatewayId}/members\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/fabric/core/gateways/list-gateway-members\n","\n","# Loads table: fabric_onprem_gateway_members\n","\n","# Note: this queries the [fabric_onprem_gateways] table to get a list of [gatewayId] values for the API calls. See line 189 for query details."],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"532ecbb8-0b9d-4b9c-9fb8-7c5d4b6d25c5","normalized_state":"finished","queued_time":"2025-07-16T16:42:16.6042212Z","session_start_time":"2025-07-16T16:42:16.6051633Z","execution_start_time":"2025-07-16T16:42:30.139602Z","execution_finish_time":"2025-07-16T16:42:30.5196734Z","parent_msg_id":"e9161d70-cfb9-4054-b421-fb17ce653873"},"text/plain":"StatementMeta(, 532ecbb8-0b9d-4b9c-9fb8-7c5d4b6d25c5, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36ea8069-5f1d-4f96-8644-0b67776b0ec9"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Gateway Members to Delta Lake - PySpark Notebook\n","# This notebook retrieves Microsoft Fabric on-premises gateway members and loads them into a Delta Lake table\n","# with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json, array_join\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType, IntegerType, ArrayType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"FabricGatewayMembersToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"MAX_RETRIES\": 5,  # Increased number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"PAGE_SIZE\": 50,  # Reduced number of items per page to avoid hitting rate limits\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"GATEWAY_TABLE_NAME\": \"fabric_onprem_gateways\",  # Name of the source gateways Delta table\n","    \"MEMBERS_TABLE_NAME\": \"fabric_onprem_gateway_members\",  # Name of the target members Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\"  # Default Tables folder in Fabric Lakehouse\n","}\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/gateways/{gatewayId}/members\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    import random\n","    import time\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            return response.json()\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Read Gateway IDs from Delta Table\n","# ==================================\n","def get_gateway_ids_from_delta():\n","    \"\"\"\n","    Retrieve all gateway IDs from the existing gateway Delta table.\n","    \n","    This function queries the fabric_onprem_gateways Delta table to get\n","    the list of gateway IDs that we need to retrieve members for.\n","    \n","    Returns:\n","        list: A list of gateway ID strings\n","    \"\"\"\n","    try:\n","        # Check if the gateway table exists\n","        spark.sql(f\"DESCRIBE TABLE {CONFIG['GATEWAY_TABLE_NAME']}\")\n","        \n","        # Query the table to get all gateway IDs\n","        gateway_ids_df = spark.sql(f\"SELECT id FROM {CONFIG['GATEWAY_TABLE_NAME']}\")\n","        \n","        # Convert to a Python list\n","        gateway_ids = [row.id for row in gateway_ids_df.collect()]\n","        \n","        logger.info(f\"Retrieved {len(gateway_ids)} gateway IDs from Delta table\")\n","        return gateway_ids\n","        \n","    except Exception as e:\n","        logger.error(f\"Failed to get gateway IDs from Delta table: {str(e)}\")\n","        logger.warning(\"No existing gateways found in Delta table. Please run the gateway extraction first.\")\n","        return []\n","# ==================================\n","\n","\n","# CELL 8 - Get Gateway Members Function\n","# ==================================\n","def get_gateway_members(gateway_id: str, access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all members for a specific gateway, handling pagination if necessary.\n","    \n","    This function calls the Gateway Members API endpoint for a specific gateway ID\n","    and handles pagination to get all members.\n","    \n","    Args:\n","        gateway_id: The ID of the gateway to get members for\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all gateway member objects for the specified gateway\n","    \"\"\"\n","    all_members = []\n","    continuation_token = None\n","    \n","    while True:\n","        # Set up parameters for the API call\n","        params = {\"top\": CONFIG['PAGE_SIZE']}\n","        if continuation_token:\n","            params[\"continuationToken\"] = continuation_token\n","        \n","        # Call the API with the gateway ID in the path\n","        endpoint = f\"/gateways/{gateway_id}/members\"\n","        try:\n","            response = call_fabric_api(endpoint, access_token, params)\n","            \n","            # Extract members from the response\n","            members = response.get(\"value\", [])\n","            all_members.extend(members)\n","            \n","            logger.info(f\"Retrieved {len(members)} members for gateway {gateway_id}. Running total: {len(all_members)}\")\n","            \n","            # Check if there are more pages\n","            continuation_token = response.get(\"continuationToken\")\n","            if not continuation_token:\n","                break\n","                \n","        except requests.exceptions.RequestException as e:\n","            # Log the error but don't fail the entire process\n","            logger.error(f\"Failed to get members for gateway {gateway_id}: {str(e)}\")\n","            \n","            # If we already have some members, return those rather than an empty list\n","            if all_members:\n","                logger.warning(f\"Returning partial results ({len(all_members)} members) for gateway {gateway_id}\")\n","                return all_members\n","            \n","            # Otherwise, return empty list\n","            logger.warning(f\"Returning empty list for gateway {gateway_id} due to API error\")\n","            return []\n","    \n","    logger.info(f\"Finished retrieving gateway members for gateway {gateway_id}. Total count: {len(all_members)}\")\n","    return all_members\n","# ==================================\n","\n","\n","# CELL 9 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_gateway_members_dataframe(members_data: List[Dict], gateway_id: str) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the gateway members data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the gateway member data\n","    - Extracts only the required fields as specified (displayName, enabled, id, version)\n","    - Adds metadata columns for tracking\n","    - Adds the parent gateway ID for relationship tracking\n","    \n","    Args:\n","        members_data: List of gateway member dictionaries from the API\n","        gateway_id: The ID of the parent gateway\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Extract only the fields we need from each member\n","    simplified_members = []\n","    \n","    for member in members_data:\n","        simplified_member = {\n","            \"id\": member.get(\"id\"),\n","            \"displayName\": member.get(\"displayName\"),\n","            \"enabled\": member.get(\"enabled\"),\n","            \"version\": member.get(\"version\"),\n","            \"gatewayId\": gateway_id  # Add the parent gateway ID for relationship tracking\n","        }\n","        simplified_members.append(simplified_member)\n","    \n","    # Define the schema with the specific fields we need\n","    schema = StructType([\n","        StructField(\"id\", StringType(), False),  # False = not nullable\n","        StructField(\"displayName\", StringType(), True),\n","        StructField(\"enabled\", BooleanType(), True),\n","        StructField(\"version\", StringType(), True),\n","        StructField(\"gatewayId\", StringType(), False),  # Parent gateway ID\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Convert the data to a DataFrame\n","    if not simplified_members:\n","        logger.warning(f\"No members found for gateway {gateway_id}. Creating empty DataFrame.\")\n","        # Create an empty DataFrame with the schema\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        enhanced_df = spark.createDataFrame(empty_rdd, schema)\n","        return enhanced_df\n","    \n","    # Convert to pandas DataFrame first\n","    pandas_df = pd.DataFrame(simplified_members)\n","    \n","    # Create the initial Spark DataFrame\n","    # We don't include extraction_timestamp here as we'll add it next\n","    required_columns = [\"id\", \"displayName\", \"enabled\", \"version\", \"gatewayId\"]\n","    \n","    # Ensure all columns exist in the pandas DataFrame\n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","    \n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Add metadata column for tracking when this data was extracted\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 10 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table (without partitioning)\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new gateway member data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if member ID and gateway ID match\n","    - Inserts new records if member ID and gateway ID don't exist together\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"gateway_member_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    if spark.table(table_name).count() == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Perform the merge operation\n","    # Note: We match on both id and gatewayId to handle the case where the same member ID \n","    # could appear in multiple gateways\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING gateway_member_updates AS source\n","    ON target.id = source.id AND target.gatewayId = source.gatewayId\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.displayName = source.displayName,\n","            target.enabled = source.enabled,\n","            target.version = source.version,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(\"Merge operation completed successfully\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Updates table statistics for query optimization\n","    - Uses a more compatible method for Microsoft Fabric\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # Note: In Microsoft Fabric, Delta table optimization may be handled automatically\n","        # or through different commands than traditional Delta Lake\n","        # The standard OPTIMIZE and ZORDER commands might not be available\n","        \n","        # Alternative approach: Use Delta table properties to hint at optimization\n","        delta_table = DeltaTable.forName(spark, table_name)\n","        delta_table_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n","        \n","        logger.info(\"Delta table optimization completed via statistics computation\")\n","        logger.info(\"Note: Microsoft Fabric may automatically optimize Delta tables\")\n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 11 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves gateway IDs from the existing Delta table\n","    3. For each gateway ID, retrieves its members from the API\n","    4. Creates an enhanced PySpark DataFrame for each gateway's members\n","    5. Merges all member data into a single DataFrame\n","    6. Loads data into a Delta Lake table\n","    7. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric Gateway Members to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve gateway IDs from the Delta table\n","        logger.info(\"Retrieving gateway IDs from Delta table...\")\n","        gateway_ids = get_gateway_ids_from_delta()\n","        \n","        if not gateway_ids:\n","            logger.warning(\"No gateway IDs found in the source table. Please run the gateway extraction first.\")\n","            return None\n","        \n","        logger.info(f\"Retrieved {len(gateway_ids)} gateway IDs\")\n","        \n","        # Step 3: For each gateway ID, retrieve its members and create DataFrames\n","        all_members_dfs = []\n","        total_gateways = len(gateway_ids)\n","        \n","        # Add delay between gateway processing to manage API rate limits\n","        import time\n","        import random\n","        \n","        for idx, gateway_id in enumerate(gateway_ids):\n","            logger.info(f\"Processing gateway ID: {gateway_id} ({idx+1}/{total_gateways})\")\n","            \n","            # Get members for this gateway\n","            members_data = get_gateway_members(gateway_id, access_token)\n","            \n","            # Create DataFrame for this gateway's members\n","            members_df = create_enhanced_gateway_members_dataframe(members_data, gateway_id)\n","            \n","            # Add to our list of DataFrames\n","            if members_df.count() > 0:\n","                all_members_dfs.append(members_df)\n","                logger.info(f\"Added {members_df.count()} members for gateway {gateway_id}\")\n","            \n","            # Don't add delay after the last gateway\n","            if idx < total_gateways - 1:\n","                # Add a small random delay between gateway processing to avoid hitting rate limits\n","                delay = random.uniform(0.5, 2.0)\n","                logger.info(f\"Pausing for {delay:.2f} seconds before processing next gateway...\")\n","                time.sleep(delay)\n","        \n","        # Step 4: Merge all member DataFrames into a single DataFrame\n","        if not all_members_dfs:\n","            logger.warning(\"No gateway members found across all gateways\")\n","            # Create empty dataframe with schema for consistent table structure\n","            empty_schema = StructType([\n","                StructField(\"id\", StringType(), False),\n","                StructField(\"displayName\", StringType(), True),\n","                StructField(\"enabled\", BooleanType(), True),\n","                StructField(\"version\", StringType(), True),\n","                StructField(\"gatewayId\", StringType(), False),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            combined_members_df = spark.createDataFrame([], empty_schema)\n","        else:\n","            # Union all DataFrames\n","            combined_members_df = all_members_dfs[0]\n","            for df in all_members_dfs[1:]:\n","                combined_members_df = combined_members_df.unionByName(df)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of enhanced gateway members data:\")\n","        combined_members_df.show(5, truncate=False)\n","        \n","        # Step 5: Prepare Delta table\n","        table_name = CONFIG[\"MEMBERS_TABLE_NAME\"]\n","        ensure_delta_table_exists(table_name, combined_members_df.schema)\n","        \n","        # Step 6: Merge data into Delta table (if we have data)\n","        if all_members_dfs:\n","            merge_data_to_delta(combined_members_df, table_name)\n","            \n","            # Step 7: Optimize the Delta table\n","            optimize_delta_table(table_name)\n","        \n","        # Step 8: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        spark.sql(f\"DESCRIBE DETAIL {table_name}\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"Total rows in {table_name}: {row_count}\")\n","        \n","        # Show summary statistics\n","        summary_stats = spark.sql(f\"\"\"\n","            SELECT \n","                COUNT(DISTINCT id) as unique_members,\n","                COUNT(DISTINCT gatewayId) as unique_gateways,\n","                COUNT(DISTINCT version) as member_versions,\n","                SUM(CASE WHEN enabled = true THEN 1 ELSE 0 END) as enabled_members,\n","                MAX(extraction_timestamp) as last_updated\n","            FROM {table_name}\n","        \"\"\")\n","        \n","        logger.info(\"Summary statistics:\")\n","        summary_stats.show(truncate=False)\n","        \n","        return combined_members_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 12 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    members_df = main()\n","# ==================================\n","\n","\n","# CELL 13 - Maintenance and Best Practices\n","# ==================================\n","\"\"\"\n","MAINTENANCE AND BEST PRACTICES:\n","\n","1. SCHEDULED UPDATES:\n","   - Schedule this notebook to run after the gateway extraction notebook\n","   - Configure dependencies in Fabric pipelines to ensure proper sequence\n","   - Consider daily/weekly runs to track member changes over time\n","\n","2. DELTA LAKE MAINTENANCE:\n","   - Run VACUUM periodically to clean old files (if supported in your Fabric environment):\n","     spark.sql(f\"VACUUM {CONFIG['MEMBERS_TABLE_NAME']} RETAIN 168 HOURS\")\n","   - Monitor history retention and storage usage\n","   - Review table properties and statistics\n","\n","3. MONITORING AND ALERTING:\n","   - Set up alerts for member changes or disabled members\n","   - Monitor for members with outdated versions\n","   - Track enabled/disabled member counts for operational health\n","\n","4. POWER BI INTEGRATION:\n","   - Create dashboards showing gateway-to-member relationships\n","   - Monitor member versions for outdated installations\n","   - Visualize enabled vs. disabled distribution \n","\n","5. DATA SECURITY:\n","   - Implement appropriate access controls on the Delta table\n","   - Consider sensitive information in member metadata\n","   - Document security implications of member settings\n","\n","6. PERFORMANCE OPTIMIZATION:\n","   - Consider partitioning strategies if data grows significantly\n","   - Create joined views with the gateways table for common analytics\n","   - Use caching for frequently accessed data\n","\n","Example maintenance query - Find disabled gateway members:\n","```sql\n","SELECT \n","  g.displayName as gateway_name,\n","  m.displayName as member_name, \n","  m.version,\n","  m.enabled,\n","  m.extraction_timestamp\n","FROM fabric_onprem_gateway_members m\n","JOIN fabric_onprem_gateways g ON m.gatewayId = g.id\n","WHERE m.enabled = false\n","ORDER BY g.displayName, m.displayName\n","```\n","\n","7. ERROR RECOVERY:\n","   - Use Delta time travel for recovery (if supported in your Fabric environment):\n","     spark.read.option(\"versionAsOf\", 1).table(CONFIG['MEMBERS_TABLE_NAME'])\n","   - Implement logging for all member changes\n","   - Create snapshots before major gateway updates\n","\"\"\"\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"532ecbb8-0b9d-4b9c-9fb8-7c5d4b6d25c5","normalized_state":"finished","queued_time":"2025-07-16T16:42:16.6681981Z","session_start_time":null,"execution_start_time":"2025-07-16T16:42:30.5230447Z","execution_finish_time":"2025-07-16T16:44:15.2260704Z","parent_msg_id":"52f9cc82-c13f-489b-a87c-2539e9ec9a8e"},"text/plain":"StatementMeta(, 532ecbb8-0b9d-4b9c-9fb8-7c5d4b6d25c5, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-16 16:42:31,090 - INFO - Starting Fabric Gateway Members to Delta Lake process\n2025-07-16 16:42:31,090 - INFO - Getting access token...\n2025-07-16 16:42:31,755 - INFO - Successfully obtained access token\n2025-07-16 16:42:31,756 - INFO - Retrieving gateway IDs from Delta table...\n2025-07-16 16:42:46,111 - INFO - Retrieved 14 gateway IDs from Delta table\n2025-07-16 16:42:46,112 - INFO - Retrieved 14 gateway IDs\n2025-07-16 16:42:46,112 - INFO - Processing gateway ID: 2d42f13d-1ae8-4d16-9726-82385a865ddf (1/14)\n2025-07-16 16:42:46,113 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/2d42f13d-1ae8-4d16-9726-82385a865ddf/members (Attempt 1)\n2025-07-16 16:42:46,689 - INFO - Retrieved 1 members for gateway 2d42f13d-1ae8-4d16-9726-82385a865ddf. Running total: 1\n2025-07-16 16:42:46,689 - INFO - Finished retrieving gateway members for gateway 2d42f13d-1ae8-4d16-9726-82385a865ddf. Total count: 1\n2025-07-16 16:42:48,188 - INFO - Added 1 members for gateway 2d42f13d-1ae8-4d16-9726-82385a865ddf\n2025-07-16 16:42:48,189 - INFO - Pausing for 1.38 seconds before processing next gateway...\n2025-07-16 16:42:49,565 - INFO - Processing gateway ID: 211a986c-1e4a-4029-a064-126e534f40bb (2/14)\n2025-07-16 16:42:49,566 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/211a986c-1e4a-4029-a064-126e534f40bb/members (Attempt 1)\n2025-07-16 16:42:49,848 - INFO - Retrieved 1 members for gateway 211a986c-1e4a-4029-a064-126e534f40bb. Running total: 1\n2025-07-16 16:42:49,848 - INFO - Finished retrieving gateway members for gateway 211a986c-1e4a-4029-a064-126e534f40bb. Total count: 1\n2025-07-16 16:42:50,043 - INFO - Added 1 members for gateway 211a986c-1e4a-4029-a064-126e534f40bb\n2025-07-16 16:42:50,044 - INFO - Pausing for 0.61 seconds before processing next gateway...\n2025-07-16 16:42:50,657 - INFO - Processing gateway ID: ebbbe00f-2b98-441b-92c0-1d820f987d8a (3/14)\n2025-07-16 16:42:50,658 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/ebbbe00f-2b98-441b-92c0-1d820f987d8a/members (Attempt 1)\n2025-07-16 16:42:51,424 - INFO - Retrieved 3 members for gateway ebbbe00f-2b98-441b-92c0-1d820f987d8a. Running total: 3\n2025-07-16 16:42:51,425 - INFO - Finished retrieving gateway members for gateway ebbbe00f-2b98-441b-92c0-1d820f987d8a. Total count: 3\n2025-07-16 16:42:51,636 - INFO - Added 3 members for gateway ebbbe00f-2b98-441b-92c0-1d820f987d8a\n2025-07-16 16:42:51,637 - INFO - Pausing for 0.69 seconds before processing next gateway...\n2025-07-16 16:42:52,325 - INFO - Processing gateway ID: 354cf43d-a895-4cab-b3ae-0c622559760b (4/14)\n2025-07-16 16:42:52,326 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/354cf43d-a895-4cab-b3ae-0c622559760b/members (Attempt 1)\n2025-07-16 16:42:52,601 - INFO - Retrieved 3 members for gateway 354cf43d-a895-4cab-b3ae-0c622559760b. Running total: 3\n2025-07-16 16:42:52,601 - INFO - Finished retrieving gateway members for gateway 354cf43d-a895-4cab-b3ae-0c622559760b. Total count: 3\n2025-07-16 16:42:52,820 - INFO - Added 3 members for gateway 354cf43d-a895-4cab-b3ae-0c622559760b\n2025-07-16 16:42:52,823 - INFO - Pausing for 1.00 seconds before processing next gateway...\n2025-07-16 16:42:53,820 - INFO - Processing gateway ID: 98fd034b-823c-4e36-8088-c85b8bdd8950 (5/14)\n2025-07-16 16:42:53,821 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/98fd034b-823c-4e36-8088-c85b8bdd8950/members (Attempt 1)\n2025-07-16 16:42:54,077 - INFO - Retrieved 1 members for gateway 98fd034b-823c-4e36-8088-c85b8bdd8950. Running total: 1\n2025-07-16 16:42:54,078 - INFO - Finished retrieving gateway members for gateway 98fd034b-823c-4e36-8088-c85b8bdd8950. Total count: 1\n2025-07-16 16:42:54,299 - INFO - Added 1 members for gateway 98fd034b-823c-4e36-8088-c85b8bdd8950\n2025-07-16 16:42:54,300 - INFO - Pausing for 1.28 seconds before processing next gateway...\n2025-07-16 16:42:55,582 - INFO - Processing gateway ID: 538397fa-2a60-4114-a959-b59cd76eb5e6 (6/14)\n2025-07-16 16:42:55,582 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/538397fa-2a60-4114-a959-b59cd76eb5e6/members (Attempt 1)\n2025-07-16 16:42:55,789 - INFO - Retrieved 1 members for gateway 538397fa-2a60-4114-a959-b59cd76eb5e6. Running total: 1\n2025-07-16 16:42:55,789 - INFO - Finished retrieving gateway members for gateway 538397fa-2a60-4114-a959-b59cd76eb5e6. Total count: 1\n2025-07-16 16:42:55,971 - INFO - Added 1 members for gateway 538397fa-2a60-4114-a959-b59cd76eb5e6\n2025-07-16 16:42:55,971 - INFO - Pausing for 1.53 seconds before processing next gateway...\n2025-07-16 16:42:57,499 - INFO - Processing gateway ID: ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194 (7/14)\n2025-07-16 16:42:57,499 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194/members (Attempt 1)\n2025-07-16 16:42:57,679 - INFO - Retrieved 1 members for gateway ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194. Running total: 1\n2025-07-16 16:42:57,680 - INFO - Finished retrieving gateway members for gateway ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194. Total count: 1\n2025-07-16 16:42:57,820 - INFO - Added 1 members for gateway ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194\n2025-07-16 16:42:57,821 - INFO - Pausing for 1.86 seconds before processing next gateway...\n2025-07-16 16:42:59,679 - INFO - Processing gateway ID: 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca (8/14)\n2025-07-16 16:42:59,680 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/47d3ea9b-fb98-4e41-8516-2aa7c02b7eca/members (Attempt 1)\n2025-07-16 16:42:59,854 - INFO - Retrieved 1 members for gateway 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca. Running total: 1\n2025-07-16 16:42:59,854 - INFO - Finished retrieving gateway members for gateway 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca. Total count: 1\n2025-07-16 16:43:00,016 - INFO - Added 1 members for gateway 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca\n2025-07-16 16:43:00,017 - INFO - Pausing for 0.58 seconds before processing next gateway...\n2025-07-16 16:43:00,593 - INFO - Processing gateway ID: 59ce4f51-a88c-4609-a60a-848454a36b90 (9/14)\n2025-07-16 16:43:00,594 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/59ce4f51-a88c-4609-a60a-848454a36b90/members (Attempt 1)\n2025-07-16 16:43:00,799 - INFO - Retrieved 1 members for gateway 59ce4f51-a88c-4609-a60a-848454a36b90. Running total: 1\n2025-07-16 16:43:00,799 - INFO - Finished retrieving gateway members for gateway 59ce4f51-a88c-4609-a60a-848454a36b90. Total count: 1\n2025-07-16 16:43:01,003 - INFO - Added 1 members for gateway 59ce4f51-a88c-4609-a60a-848454a36b90\n2025-07-16 16:43:01,003 - INFO - Pausing for 0.59 seconds before processing next gateway...\n2025-07-16 16:43:01,591 - INFO - Processing gateway ID: 08beb069-2a73-4be8-ac0b-1f76bbc12c0c (10/14)\n2025-07-16 16:43:01,591 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/08beb069-2a73-4be8-ac0b-1f76bbc12c0c/members (Attempt 1)\n2025-07-16 16:43:01,802 - INFO - Retrieved 2 members for gateway 08beb069-2a73-4be8-ac0b-1f76bbc12c0c. Running total: 2\n2025-07-16 16:43:01,803 - INFO - Finished retrieving gateway members for gateway 08beb069-2a73-4be8-ac0b-1f76bbc12c0c. Total count: 2\n2025-07-16 16:43:01,996 - INFO - Added 2 members for gateway 08beb069-2a73-4be8-ac0b-1f76bbc12c0c\n2025-07-16 16:43:01,996 - INFO - Pausing for 0.74 seconds before processing next gateway...\n2025-07-16 16:43:02,736 - INFO - Processing gateway ID: c4d85121-85cf-4cad-8fd5-cd375d1781f1 (11/14)\n2025-07-16 16:43:02,737 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/c4d85121-85cf-4cad-8fd5-cd375d1781f1/members (Attempt 1)\n2025-07-16 16:43:02,962 - WARNING - Rate limit exceeded (429). Waiting 44.00 seconds before retry.\n2025-07-16 16:43:46,963 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/c4d85121-85cf-4cad-8fd5-cd375d1781f1/members (Attempt 2)\n2025-07-16 16:43:47,270 - INFO - Retrieved 1 members for gateway c4d85121-85cf-4cad-8fd5-cd375d1781f1. Running total: 1\n2025-07-16 16:43:47,271 - INFO - Finished retrieving gateway members for gateway c4d85121-85cf-4cad-8fd5-cd375d1781f1. Total count: 1\n2025-07-16 16:43:47,415 - INFO - Added 1 members for gateway c4d85121-85cf-4cad-8fd5-cd375d1781f1\n2025-07-16 16:43:47,416 - INFO - Pausing for 1.47 seconds before processing next gateway...\n2025-07-16 16:43:48,887 - INFO - Processing gateway ID: 96262afb-757f-4d58-a258-b8faddca905f (12/14)\n2025-07-16 16:43:48,888 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/96262afb-757f-4d58-a258-b8faddca905f/members (Attempt 1)\n2025-07-16 16:43:49,140 - INFO - Retrieved 3 members for gateway 96262afb-757f-4d58-a258-b8faddca905f. Running total: 3\n2025-07-16 16:43:49,141 - INFO - Finished retrieving gateway members for gateway 96262afb-757f-4d58-a258-b8faddca905f. Total count: 3\n2025-07-16 16:43:49,294 - INFO - Added 3 members for gateway 96262afb-757f-4d58-a258-b8faddca905f\n2025-07-16 16:43:49,295 - INFO - Pausing for 1.59 seconds before processing next gateway...\n2025-07-16 16:43:50,887 - INFO - Processing gateway ID: 7505cd2d-8438-4fac-9b32-d43a1182d32e (13/14)\n2025-07-16 16:43:50,888 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/7505cd2d-8438-4fac-9b32-d43a1182d32e/members (Attempt 1)\n2025-07-16 16:43:51,136 - INFO - Retrieved 1 members for gateway 7505cd2d-8438-4fac-9b32-d43a1182d32e. Running total: 1\n2025-07-16 16:43:51,136 - INFO - Finished retrieving gateway members for gateway 7505cd2d-8438-4fac-9b32-d43a1182d32e. Total count: 1\n2025-07-16 16:43:51,275 - INFO - Added 1 members for gateway 7505cd2d-8438-4fac-9b32-d43a1182d32e\n2025-07-16 16:43:51,276 - INFO - Pausing for 1.66 seconds before processing next gateway...\n2025-07-16 16:43:55,464 - INFO - Delta table 'fabric_onprem_gateway_members' already exists\n2025-07-16 16:43:55,465 - INFO - Starting merge operation for fabric_onprem_gateway_members\n2025-07-16 16:44:07,060 - INFO - Merge operation completed successfully\n2025-07-16 16:44:07,061 - INFO - Optimizing Delta table 'fabric_onprem_gateway_members'\n2025-07-16 16:44:10,485 - INFO - Table statistics updated successfully\n"]},{"output_type":"stream","name":"stdout","text":["+--------------+---------------+---------------+---------------+-------------------------+\n|unique_members|unique_gateways|member_versions|enabled_members|last_updated             |\n+--------------+---------------+---------------+---------------+-------------------------+\n|22            |14             |8              |22             |2025-07-16 16:43:58.33783|\n+--------------+---------------+---------------+---------------+-------------------------+\n\n"]},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'\\nMAINTENANCE AND BEST PRACTICES:\\n\\n1. SCHEDULED UPDATES:\\n   - Schedule this notebook to run after the gateway extraction notebook\\n   - Configure dependencies in Fabric pipelines to ensure proper sequence\\n   - Consider daily/weekly runs to track member changes over time\\n\\n2. DELTA LAKE MAINTENANCE:\\n   - Run VACUUM periodically to clean old files (if supported in your Fabric environment):\\n     spark.sql(f\"VACUUM {CONFIG[\\'MEMBERS_TABLE_NAME\\']} RETAIN 168 HOURS\")\\n   - Monitor history retention and storage usage\\n   - Review table properties and statistics\\n\\n3. MONITORING AND ALERTING:\\n   - Set up alerts for member changes or disabled members\\n   - Monitor for members with outdated versions\\n   - Track enabled/disabled member counts for operational health\\n\\n4. POWER BI INTEGRATION:\\n   - Create dashboards showing gateway-to-member relationships\\n   - Monitor member versions for outdated installations\\n   - Visualize enabled vs. disabled distribution \\n\\n5. DATA SECURITY:\\n   - Implement appropriate access controls on the Delta table\\n   - Consider sensitive information in member metadata\\n   - Document security implications of member settings\\n\\n6. PERFORMANCE OPTIMIZATION:\\n   - Consider partitioning strategies if data grows significantly\\n   - Create joined views with the gateways table for common analytics\\n   - Use caching for frequently accessed data\\n\\nExample maintenance query - Find disabled gateway members:\\n```sql\\nSELECT \\n  g.displayName as gateway_name,\\n  m.displayName as member_name, \\n  m.version,\\n  m.enabled,\\n  m.extraction_timestamp\\nFROM fabric_onprem_gateway_members m\\nJOIN fabric_onprem_gateways g ON m.gatewayId = g.id\\nWHERE m.enabled = false\\nORDER BY g.displayName, m.displayName\\n```\\n\\n7. ERROR RECOVERY:\\n   - Use Delta time travel for recovery (if supported in your Fabric environment):\\n     spark.read.option(\"versionAsOf\", 1).table(CONFIG[\\'MEMBERS_TABLE_NAME\\'])\\n   - Implement logging for all member changes\\n   - Create snapshots before major gateway updates\\n'"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"585b029b-0e09-4e18-9528-8d71b52e2e65"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE fabric_onprem_gateway_members\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"532ecbb8-0b9d-4b9c-9fb8-7c5d4b6d25c5","normalized_state":"finished","queued_time":"2025-07-16T16:42:16.7253208Z","session_start_time":null,"execution_start_time":"2025-07-16T16:44:15.2284681Z","execution_finish_time":"2025-07-16T16:44:15.5787531Z","parent_msg_id":"c2f94821-2672-4e8c-92d1-9ed381561f9f"},"text/plain":"StatementMeta(, 532ecbb8-0b9d-4b9c-9fb8-7c5d4b6d25c5, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3e4f6ab8-d36d-45f5-aaa3-787ef3ec841f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}