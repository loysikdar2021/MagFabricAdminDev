{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Gateways - List Gateway Role Assignments\n","# Command:  GET https://api.fabric.microsoft.com/v1/gateways/{gatewayId}/roleAssignments\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/fabric/core/gateways/list-gateway-role-assignments\n","\n","# Loads table: fabric_onprem_gateway_role_assignments\n","\n","# Note: this queries the fabric_onprem_gateways table to get a list of gatewayId values for the API calls. See line 189."],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"11ec3ce4-fd3e-4b9d-84ac-f70fa01ab54a","normalized_state":"finished","queued_time":"2025-07-16T16:46:16.6006434Z","session_start_time":"2025-07-16T16:46:16.6017377Z","execution_start_time":"2025-07-16T16:46:31.6151194Z","execution_finish_time":"2025-07-16T16:46:31.996006Z","parent_msg_id":"27fbe3e7-bf26-420a-b4c7-3e19569a50e7"},"text/plain":"StatementMeta(, 11ec3ce4-fd3e-4b9d-84ac-f70fa01ab54a, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8bba5f7e-f287-4d6f-a2f8-5cb5b0f1917e"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Gateway Role Assignments to Delta Lake - PySpark Notebook\n","# This notebook retrieves Microsoft Fabric on-premises gateway role assignments and loads them into a Delta Lake table\n","# with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json, explode\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType, IntegerType, ArrayType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"FabricGatewayRoleAssignmentsToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"MAX_RETRIES\": 5,  # Increased number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"PAGE_SIZE\": 50,  # Reduced number of items per page to avoid hitting rate limits\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"GATEWAY_TABLE_NAME\": \"fabric_onprem_gateways\",  # Name of the source gateways Delta table\n","    \"ROLE_ASSIGNMENTS_TABLE_NAME\": \"fabric_onprem_gateway_role_assignments\",  # Name of the target role assignments Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\"  # Default Tables folder in Fabric Lakehouse\n","}\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/gateways/{gatewayId}/roleAssignments\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            return response.json()\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Read Gateway IDs from Delta Table\n","# ==================================\n","def get_gateway_ids_from_delta():\n","    \"\"\"\n","    Retrieve all gateway IDs from the existing gateway Delta table.\n","    \n","    This function queries the fabric_onprem_gateways Delta table to get\n","    the list of gateway IDs that we need to retrieve role assignments for.\n","    \n","    Returns:\n","        list: A list of gateway ID strings\n","    \"\"\"\n","    try:\n","        # Check if the gateway table exists\n","        spark.sql(f\"DESCRIBE TABLE {CONFIG['GATEWAY_TABLE_NAME']}\")\n","        \n","        # Query the table to get all gateway IDs\n","        gateway_ids_df = spark.sql(f\"SELECT id FROM {CONFIG['GATEWAY_TABLE_NAME']}\")\n","        \n","        # Convert to a Python list\n","        gateway_ids = [row.id for row in gateway_ids_df.collect()]\n","        \n","        logger.info(f\"Retrieved {len(gateway_ids)} gateway IDs from Delta table\")\n","        return gateway_ids\n","        \n","    except Exception as e:\n","        logger.error(f\"Failed to get gateway IDs from Delta table: {str(e)}\")\n","        logger.warning(\"No existing gateways found in Delta table. Please run the gateway extraction first.\")\n","        return []\n","# ==================================\n","\n","\n","# CELL 8 - Get Gateway Role Assignments Function\n","# ==================================\n","def get_gateway_role_assignments(gateway_id: str, access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all role assignments for a specific gateway, handling pagination if necessary.\n","    \n","    This function calls the Gateway Role Assignments API endpoint for a specific gateway ID\n","    and handles pagination to get all role assignments.\n","    \n","    Args:\n","        gateway_id: The ID of the gateway to get role assignments for\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all gateway role assignment objects for the specified gateway\n","    \"\"\"\n","    all_role_assignments = []\n","    continuation_token = None\n","    \n","    while True:\n","        # Set up parameters for the API call\n","        params = {\"top\": CONFIG['PAGE_SIZE']}\n","        if continuation_token:\n","            params[\"continuationToken\"] = continuation_token\n","        \n","        # Call the API with the gateway ID in the path\n","        endpoint = f\"/gateways/{gateway_id}/roleAssignments\"\n","        try:\n","            response = call_fabric_api(endpoint, access_token, params)\n","            \n","            # Extract role assignments from the response\n","            role_assignments = response.get(\"value\", [])\n","            all_role_assignments.extend(role_assignments)\n","            \n","            logger.info(f\"Retrieved {len(role_assignments)} role assignments for gateway {gateway_id}. Running total: {len(all_role_assignments)}\")\n","            \n","            # Check if there are more pages\n","            continuation_token = response.get(\"continuationToken\")\n","            if not continuation_token:\n","                break\n","                \n","        except requests.exceptions.RequestException as e:\n","            # Log the error but don't fail the entire process\n","            logger.error(f\"Failed to get role assignments for gateway {gateway_id}: {str(e)}\")\n","            \n","            # If we already have some role assignments, return those rather than an empty list\n","            if all_role_assignments:\n","                logger.warning(f\"Returning partial results ({len(all_role_assignments)} role assignments) for gateway {gateway_id}\")\n","                return all_role_assignments\n","            \n","            # Otherwise, return empty list\n","            logger.warning(f\"Returning empty list for gateway {gateway_id} due to API error\")\n","            return []\n","    \n","    logger.info(f\"Finished retrieving gateway role assignments for gateway {gateway_id}. Total count: {len(all_role_assignments)}\")\n","    return all_role_assignments\n","# ==================================\n","\n","\n","# CELL 9 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_gateway_role_assignments_dataframe(role_assignments_data: List[Dict], gateway_id: str) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the gateway role assignments data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the gateway role assignment data\n","    - Extracts and flattens nested fields (principal object)\n","    - Adds metadata columns for tracking\n","    - Adds the parent gateway ID for relationship tracking\n","    \n","    Args:\n","        role_assignments_data: List of gateway role assignment dictionaries from the API\n","        gateway_id: The ID of the parent gateway\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Extract and flatten the fields we need from each role assignment\n","    simplified_role_assignments = []\n","    \n","    for role_assignment in role_assignments_data:\n","        # Handle the nested principal object\n","        principal = role_assignment.get(\"principal\", {})\n","        \n","        simplified_role_assignment = {\n","            \"id\": role_assignment.get(\"id\"),\n","            \"principalId\": principal.get(\"id\"),\n","            \"principalType\": principal.get(\"type\"),\n","            \"role\": role_assignment.get(\"role\"),\n","            \"gatewayId\": gateway_id  # Add the parent gateway ID for relationship tracking\n","        }\n","        simplified_role_assignments.append(simplified_role_assignment)\n","    \n","    # Define the schema with the specific fields we need\n","    schema = StructType([\n","        StructField(\"id\", StringType(), False),  # False = not nullable\n","        StructField(\"principalId\", StringType(), True),\n","        StructField(\"principalType\", StringType(), True),\n","        StructField(\"role\", StringType(), True),\n","        StructField(\"gatewayId\", StringType(), False),  # Parent gateway ID\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Convert the data to a DataFrame\n","    if not simplified_role_assignments:\n","        logger.warning(f\"No role assignments found for gateway {gateway_id}. Creating empty DataFrame.\")\n","        # Create an empty DataFrame with the schema\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        enhanced_df = spark.createDataFrame(empty_rdd, schema)\n","        return enhanced_df\n","    \n","    # Convert to pandas DataFrame first\n","    pandas_df = pd.DataFrame(simplified_role_assignments)\n","    \n","    # Create the initial Spark DataFrame\n","    # We don't include extraction_timestamp here as we'll add it next\n","    required_columns = [\"id\", \"principalId\", \"principalType\", \"role\", \"gatewayId\"]\n","    \n","    # Ensure all columns exist in the pandas DataFrame\n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","    \n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Add metadata column for tracking when this data was extracted\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 10 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table (without partitioning)\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new gateway role assignment data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if role assignment ID and gateway ID match\n","    - Inserts new records if role assignment ID and gateway ID don't exist together\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"gateway_role_assignment_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    if spark.table(table_name).count() == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Perform the merge operation\n","    # Note: We match on both id and gatewayId to handle the case where the same role assignment ID \n","    # could appear in multiple gateways\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING gateway_role_assignment_updates AS source\n","    ON target.id = source.id AND target.gatewayId = source.gatewayId\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.principalId = source.principalId,\n","            target.principalType = source.principalType,\n","            target.role = source.role,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(\"Merge operation completed successfully\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Updates table statistics for query optimization\n","    - Uses a more compatible method for Microsoft Fabric\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # Note: In Microsoft Fabric, Delta table optimization may be handled automatically\n","        # or through different commands than traditional Delta Lake\n","        # The standard OPTIMIZE and ZORDER commands might not be available\n","        \n","        # Alternative approach: Use Delta table properties to hint at optimization\n","        delta_table = DeltaTable.forName(spark, table_name)\n","        delta_table_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n","        \n","        logger.info(\"Delta table optimization completed via statistics computation\")\n","        logger.info(\"Note: Microsoft Fabric may automatically optimize Delta tables\")\n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 11 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves gateway IDs from the existing Delta table\n","    3. For each gateway ID, retrieves its role assignments from the API\n","    4. Creates an enhanced PySpark DataFrame for each gateway's role assignments\n","    5. Merges all role assignment data into a single DataFrame\n","    6. Loads data into a Delta Lake table\n","    7. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric Gateway Role Assignments to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve gateway IDs from the Delta table\n","        logger.info(\"Retrieving gateway IDs from Delta table...\")\n","        gateway_ids = get_gateway_ids_from_delta()\n","        \n","        if not gateway_ids:\n","            logger.warning(\"No gateway IDs found in the source table. Please run the gateway extraction first.\")\n","            return None\n","        \n","        logger.info(f\"Retrieved {len(gateway_ids)} gateway IDs\")\n","        \n","        # Step 3: For each gateway ID, retrieve its role assignments and create DataFrames\n","        all_role_assignments_dfs = []\n","        total_gateways = len(gateway_ids)\n","        \n","        for idx, gateway_id in enumerate(gateway_ids):\n","            logger.info(f\"Processing gateway ID: {gateway_id} ({idx+1}/{total_gateways})\")\n","            \n","            # Get role assignments for this gateway\n","            role_assignments_data = get_gateway_role_assignments(gateway_id, access_token)\n","            \n","            # Create DataFrame for this gateway's role assignments\n","            role_assignments_df = create_enhanced_gateway_role_assignments_dataframe(role_assignments_data, gateway_id)\n","            \n","            # Add to our list of DataFrames\n","            if role_assignments_df.count() > 0:\n","                all_role_assignments_dfs.append(role_assignments_df)\n","                logger.info(f\"Added {role_assignments_df.count()} role assignments for gateway {gateway_id}\")\n","            \n","            # Don't add delay after the last gateway\n","            if idx < total_gateways - 1:\n","                # Add a small random delay between gateway processing to avoid hitting rate limits\n","                delay = random.uniform(0.5, 2.0)\n","                logger.info(f\"Pausing for {delay:.2f} seconds before processing next gateway...\")\n","                time.sleep(delay)\n","        \n","        # Step 4: Merge all role assignment DataFrames into a single DataFrame\n","        if not all_role_assignments_dfs:\n","            logger.warning(\"No gateway role assignments found across all gateways\")\n","            # Create empty dataframe with schema for consistent table structure\n","            empty_schema = StructType([\n","                StructField(\"id\", StringType(), False),\n","                StructField(\"principalId\", StringType(), True),\n","                StructField(\"principalType\", StringType(), True),\n","                StructField(\"role\", StringType(), True),\n","                StructField(\"gatewayId\", StringType(), False),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            combined_role_assignments_df = spark.createDataFrame([], empty_schema)\n","        else:\n","            # Union all DataFrames\n","            combined_role_assignments_df = all_role_assignments_dfs[0]\n","            for df in all_role_assignments_dfs[1:]:\n","                combined_role_assignments_df = combined_role_assignments_df.unionByName(df)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of enhanced gateway role assignments data:\")\n","        combined_role_assignments_df.show(5, truncate=False)\n","        \n","        # Step 5: Prepare Delta table\n","        table_name = CONFIG[\"ROLE_ASSIGNMENTS_TABLE_NAME\"]\n","        ensure_delta_table_exists(table_name, combined_role_assignments_df.schema)\n","        \n","        # Step 6: Merge data into Delta table (if we have data)\n","        if all_role_assignments_dfs:\n","            merge_data_to_delta(combined_role_assignments_df, table_name)\n","            \n","            # Step 7: Optimize the Delta table\n","            optimize_delta_table(table_name)\n","        \n","        # Step 8: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        spark.sql(f\"DESCRIBE DETAIL {table_name}\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"Total rows in {table_name}: {row_count}\")\n","        \n","        # Show summary statistics for role assignments\n","        summary_stats = spark.sql(f\"\"\"\n","            SELECT \n","                COUNT(DISTINCT id) as unique_role_assignments,\n","                COUNT(DISTINCT gatewayId) as unique_gateways,\n","                COUNT(DISTINCT principalId) as unique_principals,\n","                COUNT(DISTINCT role) as unique_roles,\n","                COUNT(DISTINCT principalType) as principal_types,\n","                MAX(extraction_timestamp) as last_updated\n","            FROM {table_name}\n","        \"\"\")\n","        \n","        logger.info(\"Summary statistics:\")\n","        summary_stats.show(truncate=False)\n","        \n","        # Show role distribution\n","        role_distribution = spark.sql(f\"\"\"\n","            SELECT \n","                role,\n","                COUNT(*) as assignment_count,\n","                COUNT(DISTINCT principalId) as unique_principals\n","            FROM {table_name}\n","            GROUP BY role\n","            ORDER BY assignment_count DESC\n","        \"\"\")\n","        \n","        logger.info(\"Role distribution:\")\n","        role_distribution.show(truncate=False)\n","        \n","        return combined_role_assignments_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 12 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    role_assignments_df = main()\n","# ==================================\n","\n","\n","# CELL 13 - Maintenance and Best Practices\n","# ==================================\n","\"\"\"\n","MAINTENANCE AND BEST PRACTICES:\n","\n","1. SCHEDULED UPDATES:\n","   - Schedule this notebook to run after the gateway extraction notebook\n","   - Configure dependencies in Fabric pipelines to ensure proper sequence\n","   - Consider daily/weekly runs to track role assignment changes over time\n","\n","2. DELTA LAKE MAINTENANCE:\n","   - Run VACUUM periodically to clean old files (if supported in your Fabric environment):\n","     spark.sql(f\"VACUUM {CONFIG['ROLE_ASSIGNMENTS_TABLE_NAME']} RETAIN 168 HOURS\")\n","   - Monitor history retention and storage usage\n","   - Review table properties and statistics\n","\n","3. MONITORING AND ALERTING:\n","   - Set up alerts for role assignment changes\n","   - Monitor for unusual permission patterns\n","   - Track role distribution for security audit purposes\n","\n","4. POWER BI INTEGRATION:\n","   - Create dashboards showing gateway permission structures\n","   - Visualize principal to gateway relationships\n","   - Create security reports for compliance purposes\n","\n","5. DATA SECURITY:\n","   - Implement appropriate access controls on the Delta table\n","   - Consider sensitive information in role assignment data\n","   - Document security implications of role assignments\n","\n","6. PERFORMANCE OPTIMIZATION:\n","   - Consider partitioning strategies if data grows significantly\n","   - Create joined views with the gateways table for common analytics\n","   - Use caching for frequently accessed data\n","\n","Example analytics query - Find principals with elevated permissions across multiple gateways:\n","```sql\n","SELECT \n","  principalId,\n","  principalType,\n","  role,\n","  COUNT(DISTINCT gatewayId) as gateway_count,\n","  COLLECT_LIST(gatewayId) as gateway_list\n","FROM fabric_onprem_gateway_role_assignments\n","WHERE role IN ('Admin', 'GatewayAdmin')\n","GROUP BY principalId, principalType, role\n","HAVING COUNT(DISTINCT gatewayId) > 1\n","ORDER BY gateway_count DESC\n","\"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"11ec3ce4-fd3e-4b9d-84ac-f70fa01ab54a","normalized_state":"finished","queued_time":"2025-07-16T16:46:16.6124272Z","session_start_time":null,"execution_start_time":"2025-07-16T16:46:31.999367Z","execution_finish_time":"2025-07-16T16:48:21.197005Z","parent_msg_id":"6432ab2c-40fb-42c0-a956-96603c8a4635"},"text/plain":"StatementMeta(, 11ec3ce4-fd3e-4b9d-84ac-f70fa01ab54a, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-16 16:46:32,595 - INFO - Starting Fabric Gateway Role Assignments to Delta Lake process\n2025-07-16 16:46:32,596 - INFO - Getting access token...\n2025-07-16 16:46:33,366 - INFO - Successfully obtained access token\n2025-07-16 16:46:33,367 - INFO - Retrieving gateway IDs from Delta table...\n2025-07-16 16:46:49,467 - INFO - Retrieved 14 gateway IDs from Delta table\n2025-07-16 16:46:49,469 - INFO - Retrieved 14 gateway IDs\n2025-07-16 16:46:49,469 - INFO - Processing gateway ID: 2d42f13d-1ae8-4d16-9726-82385a865ddf (1/14)\n2025-07-16 16:46:49,470 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/2d42f13d-1ae8-4d16-9726-82385a865ddf/roleAssignments (Attempt 1)\n2025-07-16 16:46:49,868 - INFO - Retrieved 3 role assignments for gateway 2d42f13d-1ae8-4d16-9726-82385a865ddf. Running total: 3\n2025-07-16 16:46:49,869 - INFO - Finished retrieving gateway role assignments for gateway 2d42f13d-1ae8-4d16-9726-82385a865ddf. Total count: 3\n2025-07-16 16:46:51,489 - INFO - Added 3 role assignments for gateway 2d42f13d-1ae8-4d16-9726-82385a865ddf\n2025-07-16 16:46:51,490 - INFO - Pausing for 1.20 seconds before processing next gateway...\n2025-07-16 16:46:52,687 - INFO - Processing gateway ID: 211a986c-1e4a-4029-a064-126e534f40bb (2/14)\n2025-07-16 16:46:52,687 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/211a986c-1e4a-4029-a064-126e534f40bb/roleAssignments (Attempt 1)\n2025-07-16 16:46:52,970 - INFO - Retrieved 3 role assignments for gateway 211a986c-1e4a-4029-a064-126e534f40bb. Running total: 3\n2025-07-16 16:46:52,972 - INFO - Finished retrieving gateway role assignments for gateway 211a986c-1e4a-4029-a064-126e534f40bb. Total count: 3\n2025-07-16 16:46:53,189 - INFO - Added 3 role assignments for gateway 211a986c-1e4a-4029-a064-126e534f40bb\n2025-07-16 16:46:53,191 - INFO - Pausing for 1.50 seconds before processing next gateway...\n2025-07-16 16:46:54,689 - INFO - Processing gateway ID: ebbbe00f-2b98-441b-92c0-1d820f987d8a (3/14)\n2025-07-16 16:46:54,689 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/ebbbe00f-2b98-441b-92c0-1d820f987d8a/roleAssignments (Attempt 1)\n2025-07-16 16:46:54,956 - INFO - Retrieved 10 role assignments for gateway ebbbe00f-2b98-441b-92c0-1d820f987d8a. Running total: 10\n2025-07-16 16:46:54,957 - INFO - Finished retrieving gateway role assignments for gateway ebbbe00f-2b98-441b-92c0-1d820f987d8a. Total count: 10\n2025-07-16 16:46:55,162 - INFO - Added 10 role assignments for gateway ebbbe00f-2b98-441b-92c0-1d820f987d8a\n2025-07-16 16:46:55,162 - INFO - Pausing for 1.88 seconds before processing next gateway...\n2025-07-16 16:46:57,042 - INFO - Processing gateway ID: 354cf43d-a895-4cab-b3ae-0c622559760b (4/14)\n2025-07-16 16:46:57,042 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/354cf43d-a895-4cab-b3ae-0c622559760b/roleAssignments (Attempt 1)\n2025-07-16 16:46:57,272 - INFO - Retrieved 12 role assignments for gateway 354cf43d-a895-4cab-b3ae-0c622559760b. Running total: 12\n2025-07-16 16:46:57,272 - INFO - Finished retrieving gateway role assignments for gateway 354cf43d-a895-4cab-b3ae-0c622559760b. Total count: 12\n2025-07-16 16:46:57,512 - INFO - Added 12 role assignments for gateway 354cf43d-a895-4cab-b3ae-0c622559760b\n2025-07-16 16:46:57,513 - INFO - Pausing for 1.40 seconds before processing next gateway...\n2025-07-16 16:46:58,912 - INFO - Processing gateway ID: 98fd034b-823c-4e36-8088-c85b8bdd8950 (5/14)\n2025-07-16 16:46:58,913 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/98fd034b-823c-4e36-8088-c85b8bdd8950/roleAssignments (Attempt 1)\n2025-07-16 16:46:59,086 - INFO - Retrieved 5 role assignments for gateway 98fd034b-823c-4e36-8088-c85b8bdd8950. Running total: 5\n2025-07-16 16:46:59,087 - INFO - Finished retrieving gateway role assignments for gateway 98fd034b-823c-4e36-8088-c85b8bdd8950. Total count: 5\n2025-07-16 16:46:59,293 - INFO - Added 5 role assignments for gateway 98fd034b-823c-4e36-8088-c85b8bdd8950\n2025-07-16 16:46:59,294 - INFO - Pausing for 1.99 seconds before processing next gateway...\n2025-07-16 16:47:01,282 - INFO - Processing gateway ID: 538397fa-2a60-4114-a959-b59cd76eb5e6 (6/14)\n2025-07-16 16:47:01,283 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/538397fa-2a60-4114-a959-b59cd76eb5e6/roleAssignments (Attempt 1)\n2025-07-16 16:47:01,571 - INFO - Retrieved 5 role assignments for gateway 538397fa-2a60-4114-a959-b59cd76eb5e6. Running total: 5\n2025-07-16 16:47:01,571 - INFO - Finished retrieving gateway role assignments for gateway 538397fa-2a60-4114-a959-b59cd76eb5e6. Total count: 5\n2025-07-16 16:47:01,786 - INFO - Added 5 role assignments for gateway 538397fa-2a60-4114-a959-b59cd76eb5e6\n2025-07-16 16:47:01,787 - INFO - Pausing for 0.69 seconds before processing next gateway...\n2025-07-16 16:47:02,482 - INFO - Processing gateway ID: ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194 (7/14)\n2025-07-16 16:47:02,482 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194/roleAssignments (Attempt 1)\n2025-07-16 16:47:03,021 - INFO - Retrieved 4 role assignments for gateway ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194. Running total: 4\n2025-07-16 16:47:03,022 - INFO - Finished retrieving gateway role assignments for gateway ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194. Total count: 4\n2025-07-16 16:47:03,209 - INFO - Added 4 role assignments for gateway ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194\n2025-07-16 16:47:03,210 - INFO - Pausing for 1.63 seconds before processing next gateway...\n2025-07-16 16:47:04,840 - INFO - Processing gateway ID: 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca (8/14)\n2025-07-16 16:47:04,841 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/47d3ea9b-fb98-4e41-8516-2aa7c02b7eca/roleAssignments (Attempt 1)\n2025-07-16 16:47:05,018 - INFO - Retrieved 21 role assignments for gateway 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca. Running total: 21\n2025-07-16 16:47:05,019 - INFO - Finished retrieving gateway role assignments for gateway 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca. Total count: 21\n2025-07-16 16:47:05,255 - INFO - Added 21 role assignments for gateway 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca\n2025-07-16 16:47:05,256 - INFO - Pausing for 1.70 seconds before processing next gateway...\n2025-07-16 16:47:06,954 - INFO - Processing gateway ID: 59ce4f51-a88c-4609-a60a-848454a36b90 (9/14)\n2025-07-16 16:47:06,954 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/59ce4f51-a88c-4609-a60a-848454a36b90/roleAssignments (Attempt 1)\n2025-07-16 16:47:07,139 - INFO - Retrieved 11 role assignments for gateway 59ce4f51-a88c-4609-a60a-848454a36b90. Running total: 11\n2025-07-16 16:47:07,140 - INFO - Finished retrieving gateway role assignments for gateway 59ce4f51-a88c-4609-a60a-848454a36b90. Total count: 11\n2025-07-16 16:47:07,340 - INFO - Added 11 role assignments for gateway 59ce4f51-a88c-4609-a60a-848454a36b90\n2025-07-16 16:47:07,341 - INFO - Pausing for 1.30 seconds before processing next gateway...\n2025-07-16 16:47:08,644 - INFO - Processing gateway ID: 08beb069-2a73-4be8-ac0b-1f76bbc12c0c (10/14)\n2025-07-16 16:47:08,645 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/08beb069-2a73-4be8-ac0b-1f76bbc12c0c/roleAssignments (Attempt 1)\n2025-07-16 16:47:08,822 - INFO - Retrieved 12 role assignments for gateway 08beb069-2a73-4be8-ac0b-1f76bbc12c0c. Running total: 12\n2025-07-16 16:47:08,824 - INFO - Finished retrieving gateway role assignments for gateway 08beb069-2a73-4be8-ac0b-1f76bbc12c0c. Total count: 12\n2025-07-16 16:47:09,075 - INFO - Added 12 role assignments for gateway 08beb069-2a73-4be8-ac0b-1f76bbc12c0c\n2025-07-16 16:47:09,076 - INFO - Pausing for 0.52 seconds before processing next gateway...\n2025-07-16 16:47:09,598 - INFO - Processing gateway ID: c4d85121-85cf-4cad-8fd5-cd375d1781f1 (11/14)\n2025-07-16 16:47:09,599 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/c4d85121-85cf-4cad-8fd5-cd375d1781f1/roleAssignments (Attempt 1)\n2025-07-16 16:47:09,841 - WARNING - Rate limit exceeded (429). Waiting 40.00 seconds before retry.\n2025-07-16 16:47:49,842 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/c4d85121-85cf-4cad-8fd5-cd375d1781f1/roleAssignments (Attempt 2)\n2025-07-16 16:47:50,110 - INFO - Retrieved 3 role assignments for gateway c4d85121-85cf-4cad-8fd5-cd375d1781f1. Running total: 3\n2025-07-16 16:47:50,111 - INFO - Finished retrieving gateway role assignments for gateway c4d85121-85cf-4cad-8fd5-cd375d1781f1. Total count: 3\n2025-07-16 16:47:50,301 - INFO - Added 3 role assignments for gateway c4d85121-85cf-4cad-8fd5-cd375d1781f1\n2025-07-16 16:47:50,302 - INFO - Pausing for 0.83 seconds before processing next gateway...\n2025-07-16 16:47:51,132 - INFO - Processing gateway ID: 96262afb-757f-4d58-a258-b8faddca905f (12/14)\n2025-07-16 16:47:51,133 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/96262afb-757f-4d58-a258-b8faddca905f/roleAssignments (Attempt 1)\n2025-07-16 16:47:51,387 - INFO - Retrieved 17 role assignments for gateway 96262afb-757f-4d58-a258-b8faddca905f. Running total: 17\n2025-07-16 16:47:51,388 - INFO - Finished retrieving gateway role assignments for gateway 96262afb-757f-4d58-a258-b8faddca905f. Total count: 17\n2025-07-16 16:47:51,623 - INFO - Added 17 role assignments for gateway 96262afb-757f-4d58-a258-b8faddca905f\n2025-07-16 16:47:51,624 - INFO - Pausing for 0.67 seconds before processing next gateway...\n2025-07-16 16:47:52,297 - INFO - Processing gateway ID: 7505cd2d-8438-4fac-9b32-d43a1182d32e (13/14)\n2025-07-16 16:47:52,298 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/7505cd2d-8438-4fac-9b32-d43a1182d32e/roleAssignments (Attempt 1)\n2025-07-16 16:47:52,560 - INFO - Retrieved 3 role assignments for gateway 7505cd2d-8438-4fac-9b32-d43a1182d32e. Running total: 3\n2025-07-16 16:47:52,560 - INFO - Finished retrieving gateway role assignments for gateway 7505cd2d-8438-4fac-9b32-d43a1182d32e. Total count: 3\n2025-07-16 16:47:52,757 - INFO - Added 3 role assignments for gateway 7505cd2d-8438-4fac-9b32-d43a1182d32e\n2025-07-16 16:47:52,758 - INFO - Pausing for 0.63 seconds before processing next gateway...\n2025-07-16 16:47:53,388 - INFO - Processing gateway ID: 4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c (14/14)\n2025-07-16 16:47:53,388 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways/4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c/roleAssignments (Attempt 1)\n2025-07-16 16:47:53,629 - INFO - Retrieved 6 role assignments for gateway 4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c. Running total: 6\n2025-07-16 16:47:53,630 - INFO - Finished retrieving gateway role assignments for gateway 4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c. Total count: 6\n2025-07-16 16:47:53,821 - INFO - Added 6 role assignments for gateway 4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c\n2025-07-16 16:47:53,964 - INFO - Sample of enhanced gateway role assignments data:\n2025-07-16 16:47:56,749 - INFO - Delta table 'fabric_onprem_gateway_role_assignments' already exists\n2025-07-16 16:47:56,750 - INFO - Starting merge operation for fabric_onprem_gateway_role_assignments\n2025-07-16 16:48:10,074 - INFO - Merge operation completed successfully\n2025-07-16 16:48:10,075 - INFO - Optimizing Delta table 'fabric_onprem_gateway_role_assignments'\n2025-07-16 16:48:16,334 - INFO - Role distribution:\n"]},{"output_type":"stream","name":"stdout","text":["+------------------------------+----------------+-----------------+\n|role                          |assignment_count|unique_principals|\n+------------------------------+----------------+-----------------+\n|Admin                         |91              |27               |\n|ConnectionCreator             |23              |15               |\n|ConnectionCreatorWithResharing|1               |1                |\n+------------------------------+----------------+-----------------+\n\n"]},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'\\nMAINTENANCE AND BEST PRACTICES:\\n\\n1. SCHEDULED UPDATES:\\n   - Schedule this notebook to run after the gateway extraction notebook\\n   - Configure dependencies in Fabric pipelines to ensure proper sequence\\n   - Consider daily/weekly runs to track role assignment changes over time\\n\\n2. DELTA LAKE MAINTENANCE:\\n   - Run VACUUM periodically to clean old files (if supported in your Fabric environment):\\n     spark.sql(f\"VACUUM {CONFIG[\\'ROLE_ASSIGNMENTS_TABLE_NAME\\']} RETAIN 168 HOURS\")\\n   - Monitor history retention and storage usage\\n   - Review table properties and statistics\\n\\n3. MONITORING AND ALERTING:\\n   - Set up alerts for role assignment changes\\n   - Monitor for unusual permission patterns\\n   - Track role distribution for security audit purposes\\n\\n4. POWER BI INTEGRATION:\\n   - Create dashboards showing gateway permission structures\\n   - Visualize principal to gateway relationships\\n   - Create security reports for compliance purposes\\n\\n5. DATA SECURITY:\\n   - Implement appropriate access controls on the Delta table\\n   - Consider sensitive information in role assignment data\\n   - Document security implications of role assignments\\n\\n6. PERFORMANCE OPTIMIZATION:\\n   - Consider partitioning strategies if data grows significantly\\n   - Create joined views with the gateways table for common analytics\\n   - Use caching for frequently accessed data\\n\\nExample analytics query - Find principals with elevated permissions across multiple gateways:\\n```sql\\nSELECT \\n  principalId,\\n  principalType,\\n  role,\\n  COUNT(DISTINCT gatewayId) as gateway_count,\\n  COLLECT_LIST(gatewayId) as gateway_list\\nFROM fabric_onprem_gateway_role_assignments\\nWHERE role IN (\\'Admin\\', \\'GatewayAdmin\\')\\nGROUP BY principalId, principalType, role\\nHAVING COUNT(DISTINCT gatewayId) > 1\\nORDER BY gateway_count DESC\\n'"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65f5f6a3-888c-404e-ba9e-bc67ee921cde"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE fabric_onprem_gateway_role_assignments\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"11ec3ce4-fd3e-4b9d-84ac-f70fa01ab54a","normalized_state":"finished","queued_time":"2025-07-16T16:46:16.6288518Z","session_start_time":null,"execution_start_time":"2025-07-16T16:48:21.1996259Z","execution_finish_time":"2025-07-16T16:48:22.0817935Z","parent_msg_id":"79b1eff3-3ce1-4025-b5e7-483490618f86"},"text/plain":"StatementMeta(, 11ec3ce4-fd3e-4b9d-84ac-f70fa01ab54a, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1572c70-1f1a-43fc-9269-89dc5ed3e17c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}