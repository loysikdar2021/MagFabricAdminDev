{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Workspaces - List Workspaces\n","# Command:  GET https://api.fabric.microsoft.com/v1/admin/workspaces\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/fabric/admin/workspaces/list-workspaces\n","\n","# Loads table: fabric_workspaces"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"4fda317f-525a-48f8-9434-77bfdb8ced77","normalized_state":"finished","queued_time":"2025-07-16T15:45:12.2862232Z","session_start_time":"2025-07-16T15:45:12.2871766Z","execution_start_time":"2025-07-16T15:45:26.8977698Z","execution_finish_time":"2025-07-16T15:45:27.297468Z","parent_msg_id":"1615f1d9-5731-4ea1-aa0c-d23172b83552"},"text/plain":"StatementMeta(, 4fda317f-525a-48f8-9434-77bfdb8ced77, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"327b78d9-b267-4167-a025-03704c944da1"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Workspaces to Delta Lake - PySpark Notebook\n","# This notebook retrieves Microsoft Fabric workspaces and loads them into a Delta Lake table\n","# with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"FabricWorkspacesToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"WORKSPACES_ENDPOINT\": \"/admin/workspaces\",  # Endpoint for listing workspaces\n","    \"MAX_RETRIES\": 5,  # Increased number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"WORKSPACES_TABLE_NAME\": \"fabric_workspaces\",  # Name of the target Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\",  # Default Tables folder in Fabric Lakehouse\n","    \"DEBUG_MODE\": True  # Set to True to enable extra debugging output\n","}\n","\n","# Do not use the \"top\" parameter in API calls as instructed\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/admin/workspaces\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            # Log the full URL with parameters for debugging\n","            logger.info(f\"Making API call to: {url} with params: {params} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Log the full response for debugging\n","            logger.info(f\"Response status: {response.status_code}\")\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # Log the response for debugging in case of errors\n","            if response.status_code >= 400:\n","                logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                logger.error(f\"Request URL: {response.request.url}\")\n","                logger.error(f\"Request headers: {response.request.headers}\")\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            # Log a sample of the response content for debugging\n","            try:\n","                response_json = response.json()\n","                if \"value\" in response_json and isinstance(response_json[\"value\"], list):\n","                    logger.info(f\"Response contains {len(response_json['value'])} items in 'value' array\")\n","                if \"continuationToken\" in response_json:\n","                    logger.info(f\"Response contains continuationToken: {response_json['continuationToken']}\")\n","                return response_json\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Failed to parse response as JSON: {str(e)}\")\n","                logger.error(f\"Response content: {response.text[:1000]}\")  # Log first 1000 chars of response\n","                raise\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Get Workspaces Function (CORRECTED)\n","# ==================================\n","def get_workspaces(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all workspaces from the Fabric API, handling pagination.\n","    \n","    This function makes requests to the List Workspaces API endpoint and\n","    handles pagination using the continuationToken to retrieve all workspaces.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all workspace objects\n","    \"\"\"\n","    all_workspaces = []\n","    continuation_token = None\n","    page_count = 0\n","    \n","    while True:\n","        page_count += 1\n","        \n","        # For pagination, we need to construct the URL manually because the Fabric API\n","        # has specific requirements for how the continuation token is formatted\n","        if continuation_token:\n","            # The continuation token must be passed in a specific way for the Fabric API\n","            # It needs to be in the URL but not URL-encoded again\n","            url = f\"{CONFIG['API_BASE_URL']}{CONFIG['WORKSPACES_ENDPOINT']}?continuationToken={continuation_token}\"\n","            \n","            if CONFIG['DEBUG_MODE']:\n","                logger.info(f\"Page {page_count}: Making request with continuation token to URL: {url}\")\n","            \n","            # Make direct API call with proper headers and retry logic\n","            headers = {\n","                \"Authorization\": f\"Bearer {access_token}\",\n","                \"Content-Type\": \"application/json\"\n","            }\n","            \n","            # Use the same retry logic as in call_fabric_api\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            for attempt in range(CONFIG['MAX_RETRIES']):\n","                try:\n","                    logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","                    \n","                    response = requests.get(\n","                        url,\n","                        headers=headers,\n","                        timeout=CONFIG['TIMEOUT']\n","                    )\n","                    \n","                    logger.info(f\"Response status: {response.status_code}\")\n","                    \n","                    # Handle rate limiting\n","                    if response.status_code == 429:\n","                        retry_after = response.headers.get('Retry-After')\n","                        if retry_after and retry_after.isdigit():\n","                            wait_time = int(retry_after)\n","                        else:\n","                            jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                            wait_time = backoff_time + jitter\n","                            backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                        \n","                        logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                        time.sleep(wait_time)\n","                        continue\n","                    \n","                    # Log errors\n","                    if response.status_code >= 400:\n","                        logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                        logger.error(f\"Request URL: {response.request.url}\")\n","                    \n","                    response.raise_for_status()\n","                    response_data = response.json()\n","                    break  # Success, exit retry loop\n","                    \n","                except requests.exceptions.RequestException as e:\n","                    if attempt == CONFIG['MAX_RETRIES'] - 1:\n","                        logger.error(f\"All retry attempts failed for page {page_count}\")\n","                        raise\n","                    \n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                    \n","                    logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                    logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                    time.sleep(wait_time)\n","        else:\n","            # First page - use the standard call_fabric_api function\n","            if CONFIG['DEBUG_MODE']:\n","                logger.info(f\"Page {page_count}: Making initial request\")\n","            \n","            try:\n","                response_data = call_fabric_api(CONFIG['WORKSPACES_ENDPOINT'], access_token)\n","            except requests.exceptions.RequestException as e:\n","                logger.error(f\"API call failed on page {page_count}: {str(e)}\")\n","                raise\n","        \n","        # Log the response structure for debugging\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(f\"Response keys: {list(response_data.keys())}\")\n","        \n","        # Extract workspaces from the response\n","        # Based on the log, the API returns \"workspaces\" not \"value\"\n","        workspaces = response_data.get(\"workspaces\", [])\n","        \n","        if workspaces:\n","            all_workspaces.extend(workspaces)\n","            logger.info(f\"Retrieved {len(workspaces)} workspaces on page {page_count}. Running total: {len(all_workspaces)}\")\n","            \n","            # Log first workspace for debugging\n","            if CONFIG['DEBUG_MODE'] and workspaces:\n","                logger.info(f\"Sample workspace: {json.dumps(workspaces[0], indent=2)}\")\n","        else:\n","            logger.warning(f\"No workspaces found on page {page_count}\")\n","        \n","        # Check if there are more pages\n","        continuation_token = response_data.get(\"continuationToken\")\n","        \n","        if continuation_token:\n","            logger.info(f\"Found continuation token: {continuation_token}\")\n","        else:\n","            logger.info(\"No continuation token found - this is the last page\")\n","            break\n","    \n","    logger.info(f\"Finished retrieving all workspaces. Total count: {len(all_workspaces)}\")\n","    return all_workspaces\n","# ==================================\n","\n","\n","# CELL 8 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_workspaces_dataframe(workspaces_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the workspaces data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the workspace data\n","    - Extracts only the required fields (capacityId, id, name, state, type)\n","    - Adds metadata columns for tracking\n","    \n","    Args:\n","        workspaces_data: List of workspace dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Extract only the fields we need from each workspace\n","    simplified_workspaces = []\n","    \n","    for workspace in workspaces_data:\n","        # Based on the log output, the API returns these exact field names:\n","        # id, name, state, type, capacityId\n","        simplified_workspace = {\n","            \"capacityId\": workspace.get(\"capacityId\"),\n","            \"id\": workspace.get(\"id\"),\n","            \"name\": workspace.get(\"name\"),  # The API returns \"name\" not \"displayName\"\n","            \"state\": workspace.get(\"state\"),\n","            \"type\": workspace.get(\"type\")\n","        }\n","        simplified_workspaces.append(simplified_workspace)\n","    \n","    # Define the schema with the specific fields we need\n","    schema = StructType([\n","        StructField(\"capacityId\", StringType(), True),  # True = nullable\n","        StructField(\"id\", StringType(), False),         # False = not nullable\n","        StructField(\"name\", StringType(), True),\n","        StructField(\"state\", StringType(), True),\n","        StructField(\"type\", StringType(), True),\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Convert the data to a DataFrame\n","    if not simplified_workspaces:\n","        logger.warning(\"No workspaces found. Creating empty DataFrame.\")\n","        # Create an empty DataFrame with the schema\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        enhanced_df = spark.createDataFrame(empty_rdd, schema)\n","        return enhanced_df\n","    \n","    # Convert to pandas DataFrame first\n","    pandas_df = pd.DataFrame(simplified_workspaces)\n","    \n","    # Create the initial Spark DataFrame\n","    # We don't include extraction_timestamp here as we'll add it next\n","    required_columns = [\"capacityId\", \"id\", \"name\", \"state\", \"type\"]\n","    \n","    # Ensure all columns exist in the pandas DataFrame\n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","    \n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Add metadata column for tracking when this data was extracted\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 9 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table (without partitioning)\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new workspace data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if workspace ID matches\n","    - Inserts new records if workspace ID doesn't exist\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"workspace_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    if spark.table(table_name).count() == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Perform the merge operation\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING workspace_updates AS source\n","    ON target.id = source.id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.capacityId = source.capacityId,\n","            target.name = source.name,\n","            target.state = source.state,\n","            target.type = source.type,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(\"Merge operation completed successfully\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Updates table statistics for query optimization\n","    - Uses a more compatible method for Microsoft Fabric\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # Note: In Microsoft Fabric, Delta table optimization may be handled automatically\n","        # or through different commands than traditional Delta Lake\n","        # The standard OPTIMIZE and ZORDER commands might not be available\n","        \n","        # Alternative approach: Use Delta table properties to hint at optimization\n","        delta_table = DeltaTable.forName(spark, table_name)\n","        delta_table_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n","        \n","        logger.info(\"Delta table optimization completed via statistics computation\")\n","        logger.info(\"Note: Microsoft Fabric may automatically optimize Delta tables\")\n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 10 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all workspaces from the API\n","    3. Creates an enhanced PySpark DataFrame with the workspace data\n","    4. Loads data into a Delta Lake table\n","    5. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric Workspaces to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve all workspaces\n","        logger.info(\"Retrieving workspaces from Fabric API...\")\n","        workspaces_data = get_workspaces(access_token)\n","        \n","        if not workspaces_data:\n","            logger.warning(\"No workspaces found. Please check your permissions and API access.\")\n","            # Create empty dataframe with schema for consistent table structure\n","            empty_schema = StructType([\n","                StructField(\"capacityId\", StringType(), True),\n","                StructField(\"id\", StringType(), False),\n","                StructField(\"name\", StringType(), True),\n","                StructField(\"state\", StringType(), True),\n","                StructField(\"type\", StringType(), True),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            workspaces_df = spark.createDataFrame([], empty_schema)\n","        else:\n","            # Step 3: Create enhanced DataFrame\n","            logger.info(f\"Creating DataFrame for {len(workspaces_data)} workspaces...\")\n","            workspaces_df = create_enhanced_workspaces_dataframe(workspaces_data)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of enhanced workspaces data:\")\n","        workspaces_df.show(5, truncate=False)\n","        \n","        # Step 4: Prepare Delta table\n","        table_name = CONFIG[\"WORKSPACES_TABLE_NAME\"]\n","        ensure_delta_table_exists(table_name, workspaces_df.schema)\n","        \n","        # Step 5: Merge data into Delta table (if we have data)\n","        if workspaces_data:\n","            merge_data_to_delta(workspaces_df, table_name)\n","            \n","            # Step 6: Optimize the Delta table\n","            optimize_delta_table(table_name)\n","        \n","        # Step 7: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        spark.sql(f\"DESCRIBE DETAIL {table_name}\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"Total rows in {table_name}: {row_count}\")\n","        \n","        # Show summary statistics\n","        summary_stats = spark.sql(f\"\"\"\n","            SELECT \n","                COUNT(*) as total_workspaces,\n","                COUNT(DISTINCT capacityId) as unique_capacities,\n","                COUNT(DISTINCT state) as workspace_states,\n","                COUNT(DISTINCT type) as workspace_types,\n","                MAX(extraction_timestamp) as last_updated\n","            FROM {table_name}\n","        \"\"\")\n","        \n","        logger.info(\"Summary statistics:\")\n","        summary_stats.show(truncate=False)\n","        \n","        # Optional: Show distribution by state\n","        state_distribution = spark.sql(f\"\"\"\n","            SELECT \n","                state,\n","                COUNT(*) as count\n","            FROM {table_name}\n","            GROUP BY state\n","            ORDER BY count DESC\n","        \"\"\")\n","        \n","        logger.info(\"Workspace distribution by state:\")\n","        state_distribution.show(truncate=False)\n","        \n","        return workspaces_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 11 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    workspaces_df = main()\n","# ==================================\n","\n","\n","# CELL 12 - Maintenance and Best Practices\n","# ==================================\n","\"\"\"\n","MAINTENANCE AND BEST PRACTICES:\n","\n","1. SCHEDULED UPDATES:\n","   - Schedule this notebook to run on a regular basis (daily/weekly)\n","   - Configure it as part of a Fabric pipeline\n","   - Consider capturing historical snapshots by using a timestamp partition\n","\n","2. DELTA LAKE MAINTENANCE:\n","   - Run VACUUM periodically to clean old files (if supported in your Fabric environment):\n","     spark.sql(f\"VACUUM {CONFIG['WORKSPACES_TABLE_NAME']} RETAIN 168 HOURS\")\n","   - Monitor history retention and storage usage\n","   - Review table properties and statistics\n","\n","3. MONITORING AND ALERTING:\n","   - Set up alerts for workspace state changes\n","   - Monitor for capacity changes\n","   - Track workspace count by type for governance\n","\n","4. POWER BI INTEGRATION:\n","   - Create dashboards showing workspace distribution by capacity\n","   - Monitor workspace states\n","   - Visualize workspace types across the organization\n","\n","5. DATA SECURITY:\n","   - Implement appropriate access controls on the Delta table\n","   - Consider who should have access to workspace metadata\n","   - Document security implications of workspace settings\n","\n","6. PERFORMANCE OPTIMIZATION:\n","   - Consider partitioning strategies if data grows significantly\n","   - Create joined views with other Fabric metadata tables for broader analytics\n","   - Use caching for frequently accessed data\n","\n","Example analysis query - Find distribution of workspaces by type and state:\n","```sql\n","SELECT \n","  type,\n","  state,\n","  COUNT(*) as workspace_count\n","FROM fabric_workspaces\n","GROUP BY type, state\n","ORDER BY type, state\n","```\n","\n","7. ERROR RECOVERY:\n","   - Use Delta time travel for recovery (if supported in your Fabric environment):\n","     spark.read.option(\"versionAsOf\", 1).table(CONFIG['WORKSPACES_TABLE_NAME'])\n","   - Implement logging for all workspace changes\n","   - Create snapshots before major tenant changes\n","\"\"\"\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"4fda317f-525a-48f8-9434-77bfdb8ced77","normalized_state":"finished","queued_time":"2025-07-16T15:45:12.3784178Z","session_start_time":null,"execution_start_time":"2025-07-16T15:45:27.3009218Z","execution_finish_time":"2025-07-16T15:46:13.3226722Z","parent_msg_id":"1abec405-78ce-45d5-81ca-99e4adee1b0e"},"text/plain":"StatementMeta(, 4fda317f-525a-48f8-9434-77bfdb8ced77, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-16 15:45:27,899 - INFO - Starting Fabric Workspaces to Delta Lake process\n2025-07-16 15:45:27,899 - INFO - Getting access token...\n2025-07-16 15:45:28,695 - INFO - Successfully obtained access token\n2025-07-16 15:45:28,696 - INFO - Retrieving workspaces from Fabric API...\n2025-07-16 15:45:28,696 - INFO - Page 1: Making initial request\n2025-07-16 15:45:28,697 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/workspaces with params: None (Attempt 1)\n2025-07-16 15:45:30,014 - INFO - Response status: 200\n2025-07-16 15:45:30,024 - INFO - Response contains continuationToken: MTAwMDAsMTAwMDA%3D\n2025-07-16 15:45:30,027 - INFO - Response keys: ['workspaces', 'continuationUri', 'continuationToken']\n2025-07-16 15:45:30,027 - INFO - Retrieved 10000 workspaces on page 1. Running total: 10000\n2025-07-16 15:45:30,028 - INFO - Sample workspace: {\n  \"id\": \"47298625-cc8c-4967-82bf-2c97da0254af\",\n  \"name\": \"IAI - BI Delivery Team\",\n  \"state\": \"Active\",\n  \"type\": \"Workspace\",\n  \"capacityId\": \"665D4010-E0FD-4821-B5B0-6C2760C1D498\"\n}\n2025-07-16 15:45:30,028 - INFO - Found continuation token: MTAwMDAsMTAwMDA%3D\n2025-07-16 15:45:30,029 - INFO - Page 2: Making request with continuation token to URL: https://api.fabric.microsoft.com/v1/admin/workspaces?continuationToken=MTAwMDAsMTAwMDA%3D\n2025-07-16 15:45:30,030 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/workspaces?continuationToken=MTAwMDAsMTAwMDA%3D (Attempt 1)\n2025-07-16 15:45:32,217 - INFO - Response status: 200\n2025-07-16 15:45:32,227 - INFO - Response keys: ['workspaces', 'continuationUri', 'continuationToken']\n2025-07-16 15:45:32,228 - INFO - Retrieved 10000 workspaces on page 2. Running total: 20000\n2025-07-16 15:45:32,228 - INFO - Sample workspace: {\n  \"id\": \"b2667553-b6e6-4022-9ce0-f169a48d1285\",\n  \"name\": \"Glenda Phillips\",\n  \"state\": \"Active\",\n  \"type\": \"Personal\",\n  \"capacityId\": \"12E5359C-6E4D-4B52-95BF-7EE96E80747B\"\n}\n2025-07-16 15:45:32,228 - INFO - Found continuation token: MTAwMDAsMjAwMDA%3D\n2025-07-16 15:45:32,229 - INFO - Page 3: Making request with continuation token to URL: https://api.fabric.microsoft.com/v1/admin/workspaces?continuationToken=MTAwMDAsMjAwMDA%3D\n2025-07-16 15:45:32,230 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/workspaces?continuationToken=MTAwMDAsMjAwMDA%3D (Attempt 1)\n2025-07-16 15:45:35,033 - INFO - Response status: 200\n2025-07-16 15:45:35,045 - INFO - Response keys: ['workspaces', 'continuationUri', 'continuationToken']\n2025-07-16 15:45:35,045 - INFO - Retrieved 9899 workspaces on page 3. Running total: 29899\n2025-07-16 15:45:35,046 - INFO - Sample workspace: {\n  \"id\": \"bffbab77-4b54-4a81-b74e-8e242fba8253\",\n  \"name\": \"william.mauro@bannerhealth.com\",\n  \"state\": \"Active\",\n  \"type\": \"Personal\",\n  \"capacityId\": \"665D4010-E0FD-4821-B5B0-6C2760C1D498\"\n}\n2025-07-16 15:45:35,046 - INFO - No continuation token found - this is the last page\n2025-07-16 15:45:35,047 - INFO - Finished retrieving all workspaces. Total count: 29899\n2025-07-16 15:45:35,051 - INFO - Creating DataFrame for 29899 workspaces...\n2025-07-16 15:45:49,638 - INFO - Delta table 'fabric_workspaces' already exists\n2025-07-16 15:45:49,639 - INFO - Starting merge operation for fabric_workspaces\n2025-07-16 15:46:06,336 - INFO - Merge operation completed successfully\n2025-07-16 15:46:06,337 - INFO - Optimizing Delta table 'fabric_workspaces'\n2025-07-16 15:46:10,848 - INFO - Total rows in fabric_workspaces: 30497\n"]},{"output_type":"stream","name":"stdout","text":["+------------------------------------+------------------------------------+---------------------------+------+---------+--------------------------+\n|capacityId                          |id                                  |name                       |state |type     |extraction_timestamp      |\n+------------------------------------+------------------------------------+---------------------------+------+---------+--------------------------+\n|665D4010-E0FD-4821-B5B0-6C2760C1D498|47298625-cc8c-4967-82bf-2c97da0254af|IAI - BI Delivery Team     |Active|Workspace|2025-07-16 15:45:37.232706|\n|6BA0A957-48B9-4AD6-B422-6D779CC7DFF3|2022d60a-0ea0-460a-b794-8ac87ef3a916|IAI Management             |Active|Workspace|2025-07-16 15:45:37.232706|\n|646712A9-B160-4E8C-922D-A32ACB6EA7AA|49ec4821-c19f-4a69-ab00-53ed07fdc830|EDI Analytics              |Active|Workspace|2025-07-16 15:45:37.232706|\n|6F2BAF05-9C63-4402-BE41-C719011112F2|aad173a4-46ac-4b68-8718-e1ebc008f238|Github 8/26/2024 1:59:22 PM|Active|Workspace|2025-07-16 15:45:37.232706|\n|646712A9-B160-4E8C-922D-A32ACB6EA7AA|63875610-1ed5-40cc-8313-935f75ee2753|Test                       |Active|Workspace|2025-07-16 15:45:37.232706|\n+------------------------------------+------------------------------------+---------------------------+------+---------+--------------------------+\nonly showing top 5 rows\n\n+----------------+-----------------+----------------+---------------+--------------------------+\n|total_workspaces|unique_capacities|workspace_states|workspace_types|last_updated              |\n+----------------+-----------------+----------------+---------------+--------------------------+\n|30497           |18               |2               |3              |2025-07-16 15:45:53.489116|\n+----------------+-----------------+----------------+---------------+--------------------------+\n\n+-------+-----+\n|state  |count|\n+-------+-----+\n|Active |29917|\n|Deleted|580  |\n+-------+-----+\n\n"]},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'\\nMAINTENANCE AND BEST PRACTICES:\\n\\n1. SCHEDULED UPDATES:\\n   - Schedule this notebook to run on a regular basis (daily/weekly)\\n   - Configure it as part of a Fabric pipeline\\n   - Consider capturing historical snapshots by using a timestamp partition\\n\\n2. DELTA LAKE MAINTENANCE:\\n   - Run VACUUM periodically to clean old files (if supported in your Fabric environment):\\n     spark.sql(f\"VACUUM {CONFIG[\\'WORKSPACES_TABLE_NAME\\']} RETAIN 168 HOURS\")\\n   - Monitor history retention and storage usage\\n   - Review table properties and statistics\\n\\n3. MONITORING AND ALERTING:\\n   - Set up alerts for workspace state changes\\n   - Monitor for capacity changes\\n   - Track workspace count by type for governance\\n\\n4. POWER BI INTEGRATION:\\n   - Create dashboards showing workspace distribution by capacity\\n   - Monitor workspace states\\n   - Visualize workspace types across the organization\\n\\n5. DATA SECURITY:\\n   - Implement appropriate access controls on the Delta table\\n   - Consider who should have access to workspace metadata\\n   - Document security implications of workspace settings\\n\\n6. PERFORMANCE OPTIMIZATION:\\n   - Consider partitioning strategies if data grows significantly\\n   - Create joined views with other Fabric metadata tables for broader analytics\\n   - Use caching for frequently accessed data\\n\\nExample analysis query - Find distribution of workspaces by type and state:\\n```sql\\nSELECT \\n  type,\\n  state,\\n  COUNT(*) as workspace_count\\nFROM fabric_workspaces\\nGROUP BY type, state\\nORDER BY type, state\\n```\\n\\n7. ERROR RECOVERY:\\n   - Use Delta time travel for recovery (if supported in your Fabric environment):\\n     spark.read.option(\"versionAsOf\", 1).table(CONFIG[\\'WORKSPACES_TABLE_NAME\\'])\\n   - Implement logging for all workspace changes\\n   - Create snapshots before major tenant changes\\n'"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd21f7f2-448b-4a66-aafa-18d178d34462"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE fabric_workspaces\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"4fda317f-525a-48f8-9434-77bfdb8ced77","normalized_state":"finished","queued_time":"2025-07-16T15:45:12.5112177Z","session_start_time":null,"execution_start_time":"2025-07-16T15:46:13.3249121Z","execution_finish_time":"2025-07-16T15:46:14.1673217Z","parent_msg_id":"34a38ec0-cc27-48db-8f60-38e1906ca35d"},"text/plain":"StatementMeta(, 4fda317f-525a-48f8-9434-77bfdb8ced77, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f401d4d-4c77-40a6-81c8-3f05fb63bdd2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}