{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Gateways - List Gateways\n","# Command:  GET https://api.fabric.microsoft.com/v1/gateways\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/fabric/core/gateways/list-gateways\n","\n","# Loads table: fabric_onprem_gateways"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"3e21ce52-3042-45fe-9271-a9741d2a5f91","normalized_state":"finished","queued_time":"2025-07-16T16:01:01.4106639Z","session_start_time":"2025-07-16T16:01:01.4116688Z","execution_start_time":"2025-07-16T16:01:12.9790614Z","execution_finish_time":"2025-07-16T16:01:13.3560497Z","parent_msg_id":"e56df34e-2d57-42bb-8388-49254e6cb5b0"},"text/plain":"StatementMeta(, 3e21ce52-3042-45fe-9271-a9741d2a5f91, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"774eb113-bb58-43a2-b5dc-695b984bf66d"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Gateways to Delta Lake - PySpark Notebook\n","# This notebook retrieves Microsoft Fabric on-premises gateways and loads them into a Delta Lake table\n","# with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType, IntegerType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"FabricGatewaysToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"MAX_RETRIES\": 3,\n","    \"PAGE_SIZE\": 100,  # Number of items per page for API calls\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"DELTA_TABLE_NAME\": \"fabric_onprem_gateways\",  # Name of the Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\"  # Default Tables folder in Fabric Lakehouse\n","}\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries if the request fails\n","    - Error handling\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/gateways\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Retry logic - sometimes API calls can fail temporarily\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Check if the request was successful\n","            response.raise_for_status()\n","            \n","            return response.json()\n","            \n","        except requests.exceptions.RequestException as e:\n","            logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","            \n","            if attempt == CONFIG['MAX_RETRIES'] - 1:\n","                logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                raise\n","            \n","            # Wait before retrying (exponential backoff)\n","            import time\n","            time.sleep(2 ** attempt)\n","# ==================================\n","\n","\n","# CELL 7 - Get All On-Premises Gateways Function\n","# ==================================\n","def get_all_onpremises_gateways(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all Microsoft Fabric on-premises gateways, handling pagination if necessary.\n","    \n","    The Fabric API may return results in pages if there are many gateways.\n","    This function handles the pagination automatically to get all on-premises gateways.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all on-premises gateway objects\n","    \"\"\"\n","    all_gateways = []\n","    continuation_token = None\n","    \n","    while True:\n","        # Set up parameters for the API call\n","        params = {\"top\": CONFIG['PAGE_SIZE']}\n","        if continuation_token:\n","            params[\"continuationToken\"] = continuation_token\n","        \n","        # Call the API\n","        response = call_fabric_api(\"/gateways\", access_token, params)\n","        \n","        # Extract gateways from the response\n","        gateways = response.get(\"value\", [])\n","        \n","        # Filter for only OnPremises type gateways\n","        onpremises_gateways = [gateway for gateway in gateways if gateway.get(\"type\") == \"OnPremises\"]\n","        all_gateways.extend(onpremises_gateways)\n","        \n","        logger.info(f\"Retrieved {len(onpremises_gateways)} on-premises gateways out of {len(gateways)} total. Running total: {len(all_gateways)}\")\n","        \n","        # Check if there are more pages\n","        continuation_token = response.get(\"continuationToken\")\n","        if not continuation_token:\n","            break\n","    \n","    logger.info(f\"Finished retrieving on-premises gateways. Total count: {len(all_gateways)}\")\n","    return all_gateways\n","# ==================================\n","\n","\n","# CELL 8 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_gateways_dataframe(gateways_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the gateways data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the gateway data\n","    - Extracts only the required fields as specified\n","    - Adds metadata columns for tracking\n","    - Prepares the data for optimal Delta Lake storage\n","    \n","    Args:\n","        gateways_data: List of gateway dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Extract only the fields we need from each gateway\n","    simplified_gateways = []\n","    \n","    for gateway in gateways_data:\n","        simplified_gateway = {\n","            \"id\": gateway.get(\"id\"),\n","            \"displayName\": gateway.get(\"displayName\"),\n","            \"type\": gateway.get(\"type\"),\n","            \"version\": gateway.get(\"version\"),\n","            \"allowCloudConnectionRefresh\": gateway.get(\"allowCloudConnectionRefresh\"),\n","            \"allowCustomConnectors\": gateway.get(\"allowCustomConnectors\"),\n","            \"numberOfMemberGateways\": gateway.get(\"numberOfMemberGateways\"),\n","            # Handle loadBalancingSetting as a string since it's an object type\n","            \"loadBalancingSetting\": json.dumps(gateway.get(\"loadBalancingSetting\")) if gateway.get(\"loadBalancingSetting\") else None\n","        }\n","        simplified_gateways.append(simplified_gateway)\n","    \n","    # Define the schema with the specific fields we need\n","    schema = StructType([\n","        StructField(\"id\", StringType(), False),  # False = not nullable\n","        StructField(\"displayName\", StringType(), True),\n","        StructField(\"type\", StringType(), True),\n","        StructField(\"version\", StringType(), True),\n","        StructField(\"allowCloudConnectionRefresh\", BooleanType(), True),\n","        StructField(\"allowCustomConnectors\", BooleanType(), True),\n","        StructField(\"numberOfMemberGateways\", IntegerType(), True),\n","        StructField(\"loadBalancingSetting\", StringType(), True),\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Convert the data to a DataFrame\n","    if not simplified_gateways:\n","        logger.warning(\"No on-premises gateways found. Creating empty DataFrame.\")\n","        # Create an empty DataFrame with the schema\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        enhanced_df = spark.createDataFrame(empty_rdd, schema)\n","        return enhanced_df\n","    \n","    # Convert to pandas DataFrame first\n","    pandas_df = pd.DataFrame(simplified_gateways)\n","    \n","    # Create the initial Spark DataFrame\n","    # We don't include extraction_timestamp here as we'll add it next\n","    required_columns = [\"id\", \"displayName\", \"type\", \"version\", \n","                        \"allowCloudConnectionRefresh\", \"allowCustomConnectors\", \n","                        \"numberOfMemberGateways\", \"loadBalancingSetting\"]\n","    \n","    # Ensure all columns exist in the pandas DataFrame\n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","    \n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Add metadata column for tracking when this data was extracted\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 9 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table (without partitioning)\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new gateway data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if gateway ID matches\n","    - Inserts new records if gateway ID doesn't exist\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"gateway_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    if spark.table(table_name).count() == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Perform the merge operation\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING gateway_updates AS source\n","    ON target.id = source.id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.displayName = source.displayName,\n","            target.type = source.type,\n","            target.version = source.version,\n","            target.allowCloudConnectionRefresh = source.allowCloudConnectionRefresh,\n","            target.allowCustomConnectors = source.allowCustomConnectors,\n","            target.numberOfMemberGateways = source.numberOfMemberGateways,\n","            target.loadBalancingSetting = source.loadBalancingSetting,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(\"Merge operation completed successfully\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Compacts small files (reduces file count)\n","    - Z-orders data by commonly queried columns\n","    - Updates table statistics\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    # Compact files and Z-order by commonly queried columns\n","    spark.sql(f\"\"\"\n","        OPTIMIZE {table_name}\n","        ZORDER BY (type, allowCloudConnectionRefresh, allowCustomConnectors)\n","    \"\"\")\n","    \n","    # Update table statistics for better query planning\n","    spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","    \n","    logger.info(\"Delta table optimization completed\")\n","# ==================================\n","\n","\n","# CELL 10 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all on-premises gateways from the API\n","    3. Creates an enhanced PySpark DataFrame\n","    4. Loads data into a Delta Lake table\n","    5. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric On-Premises Gateways to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve all on-premises gateways\n","        logger.info(\"Retrieving on-premises gateways from Fabric API...\")\n","        gateways_data = get_all_onpremises_gateways(access_token)\n","        \n","        if not gateways_data:\n","            logger.warning(\"No on-premises gateways found\")\n","            # Create empty dataframe with schema for consistent table structure\n","            empty_schema = StructType([\n","                StructField(\"id\", StringType(), False),\n","                StructField(\"displayName\", StringType(), True),\n","                StructField(\"type\", StringType(), True),\n","                StructField(\"version\", StringType(), True),\n","                StructField(\"allowCloudConnectionRefresh\", BooleanType(), True),\n","                StructField(\"allowCustomConnectors\", BooleanType(), True),\n","                StructField(\"numberOfMemberGateways\", IntegerType(), True),\n","                StructField(\"loadBalancingSetting\", StringType(), True),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            gateways_df = spark.createDataFrame([], empty_schema)\n","        else:\n","            # Step 3: Create enhanced PySpark DataFrame\n","            logger.info(\"Creating enhanced PySpark DataFrame...\")\n","            gateways_df = create_enhanced_gateways_dataframe(gateways_data)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of enhanced data:\")\n","        gateways_df.show(5, truncate=False)\n","        \n","        # Step 4: Prepare Delta table\n","        table_name = CONFIG[\"DELTA_TABLE_NAME\"]\n","        ensure_delta_table_exists(table_name, gateways_df.schema)\n","        \n","        # Step 5: Merge data into Delta table (if we have data)\n","        if gateways_data:\n","            merge_data_to_delta(gateways_df, table_name)\n","            \n","            # Step 6: Optimize the Delta table\n","            optimize_delta_table(table_name)\n","        \n","        # Step 7: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        spark.sql(f\"DESCRIBE DETAIL {table_name}\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"Total rows in {table_name}: {row_count}\")\n","        \n","        # Show summary statistics\n","        summary_stats = spark.sql(f\"\"\"\n","            SELECT \n","                COUNT(DISTINCT id) as unique_gateways,\n","                COUNT(DISTINCT version) as gateway_versions,\n","                SUM(numberOfMemberGateways) as total_member_gateways,\n","                MAX(extraction_timestamp) as last_updated\n","            FROM {table_name}\n","        \"\"\")\n","        \n","        logger.info(\"Summary statistics:\")\n","        summary_stats.show(truncate=False)\n","        \n","        return gateways_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 11 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    gateways_df = main()\n","# ==================================\n","\n","\n","# CELL 12 - Maintenance and Best Practices\n","# ==================================\n","\"\"\"\n","MAINTENANCE AND BEST PRACTICES:\n","\n","1. SCHEDULED UPDATES:\n","   - Schedule this notebook to run periodically (e.g., daily/weekly)\n","   - Use Fabric pipelines or scheduling features\n","   - Consider monitoring gateway changes for compliance/security\n","\n","2. DELTA LAKE MAINTENANCE:\n","   - Run VACUUM periodically to clean old files:\n","     spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS\")\n","   - Monitor history retention and storage usage\n","   - Review Z-ORDER columns based on query patterns\n","\n","3. MONITORING AND ALERTING:\n","   - Set up alerts for gateway changes or versions\n","   - Monitor for gateways with unusual settings\n","   - Track gateway counts and distributions\n","\n","4. POWER BI INTEGRATION:\n","   - Create dashboards showing gateway distributions\n","   - Monitor gateway versions for outdated installations\n","   - Visualize gateway member counts \n","\n","5. DATA SECURITY:\n","   - Implement appropriate access controls on the Delta table\n","   - Consider sensitive information in gateway metadata\n","   - Document security implications of gateway settings\n","\n","6. PERFORMANCE OPTIMIZATION:\n","   - Consider adding a date partition if data grows significantly\n","   - Create aggregated views for common analytics\n","   - Use databricks caching for frequently accessed data\n","\n","Example maintenance query - Find outdated gateways:\n","```sql\n","SELECT \n","  displayName, \n","  version, \n","  numberOfMemberGateways,\n","  extraction_timestamp\n","FROM Fabric_OnPrem_Gateways\n","WHERE version < '3.0.0'  -- Replace with current recommended version\n","ORDER BY numberOfMemberGateways DESC\n","```\n","\n","7. ERROR RECOVERY:\n","   - Use Delta time travel for recovery:\n","     spark.read.option(\"versionAsOf\", 1).table(table_name)\n","   - Implement logging for all gateway changes\n","   - Create snapshots before major gateway updates\n","\"\"\"\n","# ==================================\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"3e21ce52-3042-45fe-9271-a9741d2a5f91","normalized_state":"finished","queued_time":"2025-07-16T16:01:01.4839187Z","session_start_time":null,"execution_start_time":"2025-07-16T16:01:13.3589755Z","execution_finish_time":"2025-07-16T16:01:54.2576395Z","parent_msg_id":"8e848ff7-0f57-48cf-bb7a-b2a608824b97"},"text/plain":"StatementMeta(, 3e21ce52-3042-45fe-9271-a9741d2a5f91, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-16 16:01:13,939 - INFO - Starting Fabric On-Premises Gateways to Delta Lake process\n2025-07-16 16:01:13,940 - INFO - Getting access token...\n2025-07-16 16:01:14,644 - INFO - Successfully obtained access token\n2025-07-16 16:01:14,644 - INFO - Retrieving on-premises gateways from Fabric API...\n2025-07-16 16:01:14,645 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/gateways (Attempt 1)\n2025-07-16 16:01:16,196 - INFO - Retrieved 14 on-premises gateways out of 14 total. Running total: 14\n2025-07-16 16:01:16,197 - INFO - Finished retrieving on-premises gateways. Total count: 14\n2025-07-16 16:01:16,197 - INFO - Creating enhanced PySpark DataFrame...\n2025-07-16 16:01:17,372 - INFO - Sample of enhanced data:\n2025-07-16 16:01:27,442 - INFO - Delta table 'fabric_onprem_gateways' already exists\n2025-07-16 16:01:27,442 - INFO - Starting merge operation for fabric_onprem_gateways\n2025-07-16 16:01:42,698 - INFO - Merge operation completed successfully\n2025-07-16 16:01:42,699 - INFO - Optimizing Delta table 'fabric_onprem_gateways'\n"]},{"output_type":"stream","name":"stdout","text":["+------------------------------------+-------------------------------+----------+----------------------+---------------------------+---------------------+----------------------+--------------------+--------------------------+\n|id                                  |displayName                    |type      |version               |allowCloudConnectionRefresh|allowCustomConnectors|numberOfMemberGateways|loadBalancingSetting|extraction_timestamp      |\n+------------------------------------+-------------------------------+----------+----------------------+---------------------------+---------------------+----------------------+--------------------+--------------------------+\n|211a986c-1e4a-4029-a064-126e534f40bb|Academic Analytics & Technology|OnPremises|3000.6.204+g6523bf7046|false                      |false                |1                     |\"Failover\"          |2025-07-16 16:01:17.390736|\n|7505cd2d-8438-4fac-9b32-d43a1182d32e|mdaEnterprise                  |OnPremises|3000.45.7             |false                      |false                |1                     |\"Failover\"          |2025-07-16 16:01:17.390736|\n|4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c|MDA Analytics Gateway          |OnPremises|3000.45.7             |true                       |true                 |2                     |\"DistributeEvenly\"  |2025-07-16 16:01:17.390736|\n|ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194|PowerBI                        |OnPremises|3000.51.7             |true                       |true                 |1                     |\"DistributeEvenly\"  |2025-07-16 16:01:17.390736|\n|47d3ea9b-fb98-4e41-8516-2aa7c02b7eca|MDAENTDGWY01-DEV               |OnPremises|3000.63.6             |true                       |true                 |1                     |\"Failover\"          |2025-07-16 16:01:17.390736|\n+------------------------------------+-------------------------------+----------+----------------------+---------------------------+---------------------+----------------------+--------------------+--------------------------+\nonly showing top 5 rows\n\n+---------------+----------------+---------------------+--------------------------+\n|unique_gateways|gateway_versions|total_member_gateways|last_updated              |\n+---------------+----------------+---------------------+--------------------------+\n|14             |7               |22                   |2025-07-16 16:01:31.530674|\n+---------------+----------------+---------------------+--------------------------+\n\n"]},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'\\nMAINTENANCE AND BEST PRACTICES:\\n\\n1. SCHEDULED UPDATES:\\n   - Schedule this notebook to run periodically (e.g., daily/weekly)\\n   - Use Fabric pipelines or scheduling features\\n   - Consider monitoring gateway changes for compliance/security\\n\\n2. DELTA LAKE MAINTENANCE:\\n   - Run VACUUM periodically to clean old files:\\n     spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS\")\\n   - Monitor history retention and storage usage\\n   - Review Z-ORDER columns based on query patterns\\n\\n3. MONITORING AND ALERTING:\\n   - Set up alerts for gateway changes or versions\\n   - Monitor for gateways with unusual settings\\n   - Track gateway counts and distributions\\n\\n4. POWER BI INTEGRATION:\\n   - Create dashboards showing gateway distributions\\n   - Monitor gateway versions for outdated installations\\n   - Visualize gateway member counts \\n\\n5. DATA SECURITY:\\n   - Implement appropriate access controls on the Delta table\\n   - Consider sensitive information in gateway metadata\\n   - Document security implications of gateway settings\\n\\n6. PERFORMANCE OPTIMIZATION:\\n   - Consider adding a date partition if data grows significantly\\n   - Create aggregated views for common analytics\\n   - Use databricks caching for frequently accessed data\\n\\nExample maintenance query - Find outdated gateways:\\n```sql\\nSELECT \\n  displayName, \\n  version, \\n  numberOfMemberGateways,\\n  extraction_timestamp\\nFROM Fabric_OnPrem_Gateways\\nWHERE version < \\'3.0.0\\'  -- Replace with current recommended version\\nORDER BY numberOfMemberGateways DESC\\n```\\n\\n7. ERROR RECOVERY:\\n   - Use Delta time travel for recovery:\\n     spark.read.option(\"versionAsOf\", 1).table(table_name)\\n   - Implement logging for all gateway changes\\n   - Create snapshots before major gateway updates\\n'"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52e30780-1c34-4daa-ba87-f8cc094dd2d1"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE fabric_onprem_gateways\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"3e21ce52-3042-45fe-9271-a9741d2a5f91","normalized_state":"finished","queued_time":"2025-07-16T16:01:01.5812314Z","session_start_time":null,"execution_start_time":"2025-07-16T16:01:54.2598103Z","execution_finish_time":"2025-07-16T16:01:55.154652Z","parent_msg_id":"2c9b3172-7c75-4dcf-a3d3-2c8bdba70e7d"},"text/plain":"StatementMeta(, 3e21ce52-3042-45fe-9271-a9741d2a5f91, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a17fd2f1-bf48-437f-85ea-1825f1512af4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}