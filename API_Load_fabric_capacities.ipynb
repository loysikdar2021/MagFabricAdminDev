{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Capacities - List Capacities\n","# Command:  GET https://api.fabric.microsoft.com/v1/capacities\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/fabric/core/capacities/list-capacities\n","\n","# Loads table: fabric_capacities"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"aa104a35-c8a8-4a1c-916b-bc3ca5880acb","normalized_state":"finished","queued_time":"2025-07-16T13:18:06.9617203Z","session_start_time":"2025-07-16T13:18:06.9627641Z","execution_start_time":"2025-07-16T13:18:20.1119098Z","execution_finish_time":"2025-07-16T13:18:20.5371816Z","parent_msg_id":"8a981f0f-bf7c-4061-ac9f-4c569e66b893"},"text/plain":"StatementMeta(, aa104a35-c8a8-4a1c-916b-bc3ca5880acb, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61aa929e-4ee3-442d-9768-d7191b3cdac1"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Capacities to Delta Lake - PySpark Notebook\n","# This notebook retrieves Microsoft Fabric capacities and loads them into a Delta Lake table\n","# with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"FabricCapacitiesToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"MAX_RETRIES\": 3,\n","    \"PAGE_SIZE\": 100,  # Number of items per page for API calls\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"DELTA_TABLE_NAME\": \"fabric_capacities\",  # Name of the Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\"  # Default Tables folder in Fabric Lakehouse\n","}\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries if the request fails\n","    - Error handling\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/capacities\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Retry logic - sometimes API calls can fail temporarily\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Check if the request was successful\n","            response.raise_for_status()\n","            \n","            return response.json()\n","            \n","        except requests.exceptions.RequestException as e:\n","            logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","            \n","            if attempt == CONFIG['MAX_RETRIES'] - 1:\n","                logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                raise\n","            \n","            # Wait before retrying (exponential backoff)\n","            import time\n","            time.sleep(2 ** attempt)\n","# ==================================\n","\n","\n","# CELL 7 - Get All Capacities Function\n","# ==================================\n","def get_all_capacities(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all Fabric capacities, handling pagination if necessary.\n","    \n","    The Fabric API may return results in pages if there are many capacities.\n","    This function handles the pagination automatically to get all capacities.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all capacity objects\n","    \"\"\"\n","    all_capacities = []\n","    continuation_token = None\n","    \n","    while True:\n","        # Set up parameters for the API call\n","        params = {\"top\": CONFIG['PAGE_SIZE']}\n","        if continuation_token:\n","            params[\"continuationToken\"] = continuation_token\n","        \n","        # Call the API\n","        response = call_fabric_api(\"/capacities\", access_token, params)\n","        \n","        # Extract capacities from the response\n","        capacities = response.get(\"value\", [])\n","        all_capacities.extend(capacities)\n","        \n","        logger.info(f\"Retrieved {len(capacities)} capacities. Total so far: {len(all_capacities)}\")\n","        \n","        # Check if there are more pages\n","        continuation_token = response.get(\"continuationToken\")\n","        if not continuation_token:\n","            break\n","    \n","    logger.info(f\"Finished retrieving capacities. Total count: {len(all_capacities)}\")\n","    return all_capacities\n","# ==================================\n","\n","\n","# CELL 8 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_capacities_dataframe(capacities_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the capacities data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the capacity data\n","    - Adds metadata columns for tracking\n","    - Prepares the data for optimal Delta Lake storage\n","    \n","    Args:\n","        capacities_data: List of capacity dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Define the schema with additional metadata columns\n","    schema = StructType([\n","        StructField(\"id\", StringType(), False),  # False = not nullable\n","        StructField(\"displayName\", StringType(), True),\n","        StructField(\"sku\", StringType(), True),\n","        StructField(\"region\", StringType(), True),\n","        StructField(\"state\", StringType(), True),\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Convert the data to a DataFrame\n","    pandas_df = pd.DataFrame(capacities_data)\n","    \n","    # Ensure we have all required columns, filling with None if missing\n","    required_columns = [\"id\", \"displayName\", \"sku\", \"region\", \"state\"]\n","    for col in required_columns:\n","        if col not in pandas_df.columns:\n","            pandas_df[col] = None\n","    \n","    # Create the initial Spark DataFrame\n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Add metadata columns for tracking\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 9 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table (without partitioning)\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new capacity data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if capacity ID matches\n","    - Inserts new records if capacity ID doesn't exist\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"capacity_updates\")\n","    \n","    # Perform the merge operation\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING capacity_updates AS source\n","    ON target.id = source.id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.displayName = source.displayName,\n","            target.sku = source.sku,\n","            target.region = source.region,\n","            target.state = source.state,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(\"Merge operation completed successfully\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Compacts small files (reduces file count)\n","    - Z-orders data by commonly queried columns\n","    - Updates table statistics\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    # Compact files and Z-order by commonly queried columns\n","    spark.sql(f\"\"\"\n","        OPTIMIZE {table_name}\n","        ZORDER BY (region, sku, state)\n","    \"\"\")\n","    \n","    # Update table statistics for better query planning\n","    spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","    \n","    logger.info(\"Delta table optimization completed\")\n","# ==================================\n","\n","\n","# CELL 10 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all capacities from the API\n","    3. Creates an enhanced PySpark DataFrame\n","    4. Loads data into a Delta Lake table\n","    5. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric Capacities to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve all capacities\n","        logger.info(\"Retrieving capacities from Fabric API...\")\n","        capacities_data = get_all_capacities(access_token)\n","        \n","        if not capacities_data:\n","            logger.warning(\"No capacities found\")\n","            return\n","        \n","        # Step 3: Create enhanced PySpark DataFrame\n","        logger.info(\"Creating enhanced PySpark DataFrame...\")\n","        capacities_df = create_enhanced_capacities_dataframe(capacities_data)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of enhanced data:\")\n","        capacities_df.show(5, truncate=False)\n","        \n","        # Step 4: Prepare Delta table\n","        table_name = CONFIG[\"DELTA_TABLE_NAME\"]\n","        ensure_delta_table_exists(table_name, capacities_df.schema)\n","        \n","        # Step 5: Merge data into Delta table\n","        merge_data_to_delta(capacities_df, table_name)\n","        \n","        # Step 6: Optimize the Delta table\n","        optimize_delta_table(table_name)\n","        \n","        # Step 7: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        spark.sql(f\"DESCRIBE DETAIL {table_name}\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"Total rows in {table_name}: {row_count}\")\n","        \n","        # Show summary statistics\n","        summary_stats = spark.sql(f\"\"\"\n","            SELECT \n","                COUNT(DISTINCT id) as unique_capacities,\n","                COUNT(DISTINCT region) as regions,\n","                COUNT(DISTINCT sku) as skus,\n","                MAX(extraction_timestamp) as last_updated\n","            FROM {table_name}\n","        \"\"\")\n","        \n","        logger.info(\"Summary statistics:\")\n","        summary_stats.show(truncate=False)\n","        \n","        return capacities_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 11 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    capacities_df = main()\n","# ==================================\n","\n","\n","# CELL 12 - Maintenance and Best Practices\n","# ==================================\n","\"\"\"\n","MAINTENANCE AND BEST PRACTICES:\n","\n","1. SCHEDULED UPDATES:\n","   - Schedule this notebook to run periodically (e.g., daily/hourly)\n","   - Use Fabric pipelines or scheduling features\n","   - Consider incremental updates for large datasets\n","\n","2. DELTA LAKE MAINTENANCE:\n","   - Run VACUUM periodically to clean old files:\n","     spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS\")\n","   - Monitor file sizes and compaction needs\n","   - Review Z-ORDER columns based on query patterns\n","\n","3. MONITORING:\n","   - Set up alerts for data quality issues\n","   - Monitor table growth and storage usage\n","   - Track query performance in Power BI\n","\n","4. POWER BI INTEGRATION:\n","   - Use DirectQuery mode for real-time data\n","   - Create relationships in Power BI data model\n","   - Optimize DAX measures for performance\n","\n","5. ACCESS CONTROL:\n","   - Set appropriate permissions on the Delta table\n","   - Use row-level security if needed\n","   - Audit access to sensitive capacity data\n","\n","6. PERFORMANCE TIPS:\n","   - Use Z-ordering for frequently filtered columns\n","   - Leverage columnar format (Parquet) in Delta\n","   - Create aggregated tables for complex calculations\n","\n","Example maintenance script:\n","```python\n","# Run weekly maintenance\n","def weekly_maintenance():\n","    # Optimize table\n","    spark.sql(f\"OPTIMIZE {CONFIG['DELTA_TABLE_NAME']}\")\n","    \n","    # Clean up old files\n","    spark.sql(f\"VACUUM {CONFIG['DELTA_TABLE_NAME']} RETAIN 168 HOURS\")\n","    \n","    # Update statistics\n","    spark.sql(f\"ANALYZE TABLE {CONFIG['DELTA_TABLE_NAME']} COMPUTE STATISTICS\")\n","```\n","\n","7. ERROR RECOVERY:\n","   - Implement checkpointing for long processes\n","   - Use Delta time travel for recovery:\n","     spark.read.option(\"versionAsOf\", 1).table(table_name)\n","   - Keep logs for troubleshooting\n","\"\"\"\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"aa104a35-c8a8-4a1c-916b-bc3ca5880acb","normalized_state":"finished","queued_time":"2025-07-16T13:18:32.1874707Z","session_start_time":null,"execution_start_time":"2025-07-16T13:18:32.1894693Z","execution_finish_time":"2025-07-16T13:19:17.1541923Z","parent_msg_id":"927b8566-6fb9-4c04-abe0-4b732ef067fc"},"text/plain":"StatementMeta(, aa104a35-c8a8-4a1c-916b-bc3ca5880acb, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-16 13:18:32,713 - INFO - Starting Fabric Capacities to Delta Lake process\n2025-07-16 13:18:32,714 - INFO - Getting access token...\n2025-07-16 13:18:33,652 - INFO - Successfully obtained access token\n2025-07-16 13:18:33,653 - INFO - Retrieving capacities from Fabric API...\n2025-07-16 13:18:33,653 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/capacities (Attempt 1)\n2025-07-16 13:18:39,451 - INFO - Retrieved 9 capacities. Total so far: 9\n2025-07-16 13:18:39,451 - INFO - Finished retrieving capacities. Total count: 9\n2025-07-16 13:18:39,452 - INFO - Creating enhanced PySpark DataFrame...\n2025-07-16 13:18:51,511 - INFO - Delta table 'fabric_capacities' already exists\n2025-07-16 13:18:51,511 - INFO - Starting merge operation for fabric_capacities\n2025-07-16 13:19:05,186 - INFO - Merge operation completed successfully\n2025-07-16 13:19:05,187 - INFO - Optimizing Delta table 'fabric_capacities'\n2025-07-16 13:19:14,777 - INFO - Total rows in fabric_capacities: 9\n"]},{"output_type":"stream","name":"stdout","text":["+------------------------------------+---------------------------------+---+----------------+--------+--------------------------+\n|id                                  |displayName                      |sku|region          |state   |extraction_timestamp      |\n+------------------------------------+---------------------------------+---+----------------+--------+--------------------------+\n|c73a5223-9ef6-4514-83cc-3e70297ee377|MDA Institutional Capacity - PROD|P1 |West US         |Active  |2025-07-16 13:18:40.648411|\n|250aef2d-b24b-43a8-8564-8fefc5152522|f8nonprodsouthcentral001         |F8 |South Central US|Inactive|2025-07-16 13:18:40.648411|\n|ab3b62c5-cff1-4341-a584-4ef86a529e8a|f64nonprodsouthcentral001        |F64|South Central US|Inactive|2025-07-16 13:18:40.648411|\n|d94bc350-4bb9-4f24-9d89-fd633996eb28|f64x002                          |F64|South Central US|Active  |2025-07-16 13:18:40.648411|\n|56125c55-2f69-4fa3-bac0-e9407fc17374|f64x001                          |F64|South Central US|Active  |2025-07-16 13:18:40.648411|\n+------------------------------------+---------------------------------+---+----------------+--------+--------------------------+\nonly showing top 5 rows\n\n+-----------------+-------+----+--------------------------+\n|unique_capacities|regions|skus|last_updated              |\n+-----------------+-------+----+--------------------------+\n|9                |2      |7   |2025-07-16 13:18:51.807851|\n+-----------------+-------+----+--------------------------+\n\n"]},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'\\nMAINTENANCE AND BEST PRACTICES:\\n\\n1. SCHEDULED UPDATES:\\n   - Schedule this notebook to run periodically (e.g., daily/hourly)\\n   - Use Fabric pipelines or scheduling features\\n   - Consider incremental updates for large datasets\\n\\n2. DELTA LAKE MAINTENANCE:\\n   - Run VACUUM periodically to clean old files:\\n     spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS\")\\n   - Monitor file sizes and compaction needs\\n   - Review Z-ORDER columns based on query patterns\\n\\n3. MONITORING:\\n   - Set up alerts for data quality issues\\n   - Monitor table growth and storage usage\\n   - Track query performance in Power BI\\n\\n4. POWER BI INTEGRATION:\\n   - Use DirectQuery mode for real-time data\\n   - Create relationships in Power BI data model\\n   - Optimize DAX measures for performance\\n\\n5. ACCESS CONTROL:\\n   - Set appropriate permissions on the Delta table\\n   - Use row-level security if needed\\n   - Audit access to sensitive capacity data\\n\\n6. PERFORMANCE TIPS:\\n   - Use Z-ordering for frequently filtered columns\\n   - Leverage columnar format (Parquet) in Delta\\n   - Create aggregated tables for complex calculations\\n\\nExample maintenance script:\\n```python\\n# Run weekly maintenance\\ndef weekly_maintenance():\\n    # Optimize table\\n    spark.sql(f\"OPTIMIZE {CONFIG[\\'DELTA_TABLE_NAME\\']}\")\\n    \\n    # Clean up old files\\n    spark.sql(f\"VACUUM {CONFIG[\\'DELTA_TABLE_NAME\\']} RETAIN 168 HOURS\")\\n    \\n    # Update statistics\\n    spark.sql(f\"ANALYZE TABLE {CONFIG[\\'DELTA_TABLE_NAME\\']} COMPUTE STATISTICS\")\\n```\\n\\n7. ERROR RECOVERY:\\n   - Implement checkpointing for long processes\\n   - Use Delta time travel for recovery:\\n     spark.read.option(\"versionAsOf\", 1).table(table_name)\\n   - Keep logs for troubleshooting\\n'"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31dc53c0-59bd-41d1-b38b-6ff85420b22a"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE fabric_capacities\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"aa104a35-c8a8-4a1c-916b-bc3ca5880acb","normalized_state":"finished","queued_time":"2025-07-16T13:26:25.6452182Z","session_start_time":null,"execution_start_time":"2025-07-16T13:26:25.6463046Z","execution_finish_time":"2025-07-16T13:26:28.1415301Z","parent_msg_id":"0dfa97b5-615e-4edc-90ca-a07f0efcd122"},"text/plain":"StatementMeta(, aa104a35-c8a8-4a1c-916b-bc3ca5880acb, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb7bdaf2-6b9d-4814-85ca-5bebb8cfb234"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}