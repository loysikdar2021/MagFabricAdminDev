{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Items - List Items\n","# Command:  GET https://api.fabric.microsoft.com/v1/admin/items\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/fabric/admin/items/list-items\n","\n","# Loads table: fabric_items\n","# Loads table: fabric_items_creators"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"f516eaea-ae7d-4600-9404-46789e43c376","normalized_state":"finished","queued_time":"2025-07-16T15:52:04.8163386Z","session_start_time":"2025-07-16T15:52:04.8173873Z","execution_start_time":"2025-07-16T15:52:17.7474928Z","execution_finish_time":"2025-07-16T15:52:18.15175Z","parent_msg_id":"b1ef19ce-7032-4c22-9e50-0cec6fec9807"},"text/plain":"StatementMeta(, f516eaea-ae7d-4600-9404-46789e43c376, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3f01d3c6-f50d-4b13-9a26-d2216832d8ac"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Items to Delta Lake - PySpark Notebook\n","# This notebook retrieves Microsoft Fabric items using the List Items API and loads them\n","# into two normalized Delta Lake tables:\n","# 1. fabric_items - Core item information\n","# 2. fabric_items_creators - Normalized creator/principal information\n","# This design eliminates data duplication and supports flexible analytics queries\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json, when\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, TimestampType, \n","    ArrayType, BooleanType\n",")\n","import logging\n","from typing import Dict, List, Optional, Tuple\n","from delta.tables import DeltaTable\n","import random\n","import time\n","from datetime import datetime\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"FabricItemsToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","\n","# Configure timestamp handling for Parquet compatibility\n","# This fixes issues with ancient timestamps that may exist in the API data\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"ITEMS_ENDPOINT\": \"/admin/items\",  # Endpoint for listing all Fabric items\n","    \"MAX_RETRIES\": 5,  # Number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"ITEMS_TABLE_NAME\": \"fabric_items\",  # Name of the main items Delta table\n","    \"CREATORS_TABLE_NAME\": \"fabric_items_creators\",  # Name of the creators Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\",  # Default Tables folder in Fabric Lakehouse\n","    \"DEBUG_MODE\": True,  # Set to True to enable extra debugging output\n","    \"RATE_LIMIT_MAX_REQUESTS\": 200,  # API limit: 200 requests per hour\n","    \"PAGE_SIZE\": 10000  # Maximum records per API request (API limit)\n","}\n","\n","# Note: The API has a rate limit of 200 requests per hour\n","# With 10,000 records per request, we can theoretically retrieve up to 2M items per hour\n","logger.info(f\"API Rate Limit: {CONFIG['RATE_LIMIT_MAX_REQUESTS']} requests/hour\")\n","logger.info(f\"Max records per request: {CONFIG['PAGE_SIZE']}\")\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","        \n","        The caller must be a Fabric administrator or authenticate using a service principal.\n","        Required Delegated Scopes: Tenant.Read.All or Tenant.ReadWrite.All\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        logger.info(\"Successfully obtained access token for Fabric API\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        logger.error(\"Make sure you have Fabric administrator permissions or are using a service principal\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function with Rate Limiting\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    - Special handling for Fabric's 200 requests/hour limit\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/admin/items\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            # Log the full URL with parameters for debugging\n","            logger.info(f\"Making API call to: {url} with params: {params} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Log the full response for debugging\n","            logger.info(f\"Response status: {response.status_code}\")\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                    logger.warning(f\"Rate limit exceeded (429). Server requested wait time: {wait_time} seconds\")\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                    logger.warning(f\"Rate limit exceeded (429). Calculated wait time: {wait_time:.2f} seconds\")\n","                \n","                logger.warning(f\"Note: Fabric API has a limit of {CONFIG['RATE_LIMIT_MAX_REQUESTS']} requests per hour\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # Log the response for debugging in case of errors\n","            if response.status_code >= 400:\n","                logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                logger.error(f\"Request URL: {response.request.url}\")\n","                logger.error(f\"Request headers: {dict(response.request.headers)}\")\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            # Log a sample of the response content for debugging\n","            try:\n","                response_json = response.json()\n","                if \"itemEntities\" in response_json and isinstance(response_json[\"itemEntities\"], list):\n","                    logger.info(f\"Response contains {len(response_json['itemEntities'])} items in 'itemEntities' array\")\n","                if \"continuationToken\" in response_json:\n","                    logger.info(f\"Response contains continuationToken: {response_json['continuationToken']}\")\n","                return response_json\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Failed to parse response as JSON: {str(e)}\")\n","                logger.error(f\"Response content: {response.text[:1000]}\")  # Log first 1000 chars of response\n","                raise\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Get All Items Function with Pagination\n","# ==================================\n","def get_all_items(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all items from the Fabric API, handling pagination.\n","    \n","    This function makes requests to the List Items API endpoint and\n","    handles pagination using the continuationToken to retrieve all items.\n","    The API returns up to 10,000 items per request and supports filtering by:\n","    - workspaceId, capacityId, state, type\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all item objects from itemEntities\n","    \"\"\"\n","    all_items = []\n","    continuation_token = None\n","    page_count = 0\n","    total_items_processed = 0\n","    \n","    logger.info(\"Starting to retrieve all Fabric items...\")\n","    logger.info(\"Items will be retrieved in this order: Fabric items, Datamarts, Reports, Dashboards, SemanticModels, Apps, Dataflows\")\n","    \n","    while True:\n","        page_count += 1\n","        \n","        # For pagination, we need to construct the URL manually because the Fabric API\n","        # has specific requirements for how the continuation token is formatted\n","        if continuation_token:\n","            # The continuation token must be passed in a specific way for the Fabric API\n","            url = f\"{CONFIG['API_BASE_URL']}{CONFIG['ITEMS_ENDPOINT']}?continuationToken={continuation_token}\"\n","            \n","            if CONFIG['DEBUG_MODE']:\n","                logger.info(f\"Page {page_count}: Making request with continuation token\")\n","            \n","            # Make direct API call with proper headers and retry logic\n","            headers = {\n","                \"Authorization\": f\"Bearer {access_token}\",\n","                \"Content-Type\": \"application/json\"\n","            }\n","            \n","            # Use the same retry logic as in call_fabric_api\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            for attempt in range(CONFIG['MAX_RETRIES']):\n","                try:\n","                    logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","                    \n","                    response = requests.get(\n","                        url,\n","                        headers=headers,\n","                        timeout=CONFIG['TIMEOUT']\n","                    )\n","                    \n","                    logger.info(f\"Response status: {response.status_code}\")\n","                    \n","                    # Handle rate limiting\n","                    if response.status_code == 429:\n","                        retry_after = response.headers.get('Retry-After')\n","                        if retry_after and retry_after.isdigit():\n","                            wait_time = int(retry_after)\n","                        else:\n","                            jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                            wait_time = backoff_time + jitter\n","                            backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                        \n","                        logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                        time.sleep(wait_time)\n","                        continue\n","                    \n","                    # Log errors\n","                    if response.status_code >= 400:\n","                        logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                    \n","                    response.raise_for_status()\n","                    response_data = response.json()\n","                    break  # Success, exit retry loop\n","                    \n","                except requests.exceptions.RequestException as e:\n","                    if attempt == CONFIG['MAX_RETRIES'] - 1:\n","                        logger.error(f\"All retry attempts failed for page {page_count}\")\n","                        raise\n","                    \n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                    \n","                    logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                    logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                    time.sleep(wait_time)\n","        else:\n","            # First page - use the standard call_fabric_api function\n","            if CONFIG['DEBUG_MODE']:\n","                logger.info(f\"Page {page_count}: Making initial request\")\n","            \n","            try:\n","                response_data = call_fabric_api(CONFIG['ITEMS_ENDPOINT'], access_token)\n","            except requests.exceptions.RequestException as e:\n","                logger.error(f\"API call failed on page {page_count}: {str(e)}\")\n","                raise\n","        \n","        # Log the response structure for debugging\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(f\"Response keys: {list(response_data.keys())}\")\n","        \n","        # Extract items from the response\n","        # The API returns items in \"itemEntities\" array\n","        items = response_data.get(\"itemEntities\", [])\n","        \n","        if items:\n","            all_items.extend(items)\n","            total_items_processed += len(items)\n","            logger.info(f\"Retrieved {len(items)} items on page {page_count}. Running total: {total_items_processed}\")\n","            \n","            # Log first item for debugging\n","            if CONFIG['DEBUG_MODE'] and items:\n","                logger.info(f\"Sample item: {json.dumps(items[0], indent=2)}\")\n","                \n","            # Log item type distribution for this page\n","            if CONFIG['DEBUG_MODE']:\n","                item_types = {}\n","                for item in items:\n","                    item_type = item.get('type', 'Unknown')\n","                    item_types[item_type] = item_types.get(item_type, 0) + 1\n","                logger.info(f\"Page {page_count} item types: {item_types}\")\n","        else:\n","            logger.warning(f\"No items found on page {page_count}\")\n","        \n","        # Check if there are more pages\n","        continuation_token = response_data.get(\"continuationToken\")\n","        continuation_uri = response_data.get(\"continuationUri\")\n","        \n","        if continuation_token:\n","            logger.info(f\"Found continuation token, will retrieve next page\")\n","            if CONFIG['DEBUG_MODE']:\n","                logger.info(f\"Continuation URI: {continuation_uri}\")\n","        else:\n","            logger.info(\"No continuation token found - this is the last page\")\n","            break\n","    \n","    # Log final statistics\n","    logger.info(f\"Finished retrieving all items. Total pages: {page_count}, Total items: {len(all_items)}\")\n","    \n","    # Log overall item type distribution\n","    if all_items:\n","        item_types_summary = {}\n","        for item in all_items:\n","            item_type = item.get('type', 'Unknown')\n","            item_types_summary[item_type] = item_types_summary.get(item_type, 0) + 1\n","        \n","        logger.info(\"Final item type distribution:\")\n","        for item_type, count in sorted(item_types_summary.items()):\n","            logger.info(f\"  {item_type}: {count}\")\n","    \n","    return all_items\n","# ==================================\n","\n","\n","# CELL 8 - Data Processing Functions\n","# ==================================\n","def extract_items_and_creators(items_data: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n","    \"\"\"\n","    Extract and normalize items and creators from the API response.\n","    \n","    This function separates the nested creator information from items to avoid\n","    data duplication and creates normalized records for both entities.\n","    \n","    Args:\n","        items_data: List of item dictionaries from the API\n","    \n","    Returns:\n","        tuple: (simplified_items, unique_creators)\n","            - simplified_items: List of item records without nested creator details\n","            - unique_creators: List of unique creator/principal records\n","    \"\"\"\n","    simplified_items = []\n","    creators_dict = {}  # Use dict to automatically handle duplicates by principal ID\n","    \n","    logger.info(f\"Processing {len(items_data)} items to extract normalized data...\")\n","    \n","    for item in items_data:\n","        # Extract core item information\n","        simplified_item = {\n","            \"id\": item.get(\"id\"),\n","            \"type\": item.get(\"type\"),\n","            \"name\": item.get(\"name\"),\n","            \"description\": item.get(\"description\"),\n","            \"state\": item.get(\"state\"),\n","            \"last_updated_date\": item.get(\"lastUpdatedDate\"),  # API uses this exact field name\n","            \"workspace_id\": item.get(\"workspaceId\"),\n","            \"capacity_id\": item.get(\"capacityId\"),\n","            \"creator_principal_id\": None  # Will be populated from creatorPrincipal\n","        }\n","        \n","        # Extract creator principal information\n","        creator_principal = item.get(\"creatorPrincipal\")\n","        if creator_principal:\n","            principal_id = creator_principal.get(\"id\")\n","            simplified_item[\"creator_principal_id\"] = principal_id\n","            \n","            # Only add creator if we haven't seen this principal ID before\n","            if principal_id and principal_id not in creators_dict:\n","                creator_record = {\n","                    \"principal_id\": principal_id,\n","                    \"display_name\": creator_principal.get(\"displayName\"),\n","                    \"principal_type\": creator_principal.get(\"type\"),\n","                    \"user_principal_name\": None,\n","                    \"aad_app_id\": None,\n","                    \"group_type\": None,\n","                    \"parent_principal_id\": None\n","                }\n","                \n","                # Extract type-specific details\n","                principal_type = creator_principal.get(\"type\")\n","                \n","                if principal_type == \"User\":\n","                    user_details = creator_principal.get(\"userDetails\", {})\n","                    creator_record[\"user_principal_name\"] = user_details.get(\"userPrincipalName\")\n","                \n","                elif principal_type == \"ServicePrincipal\":\n","                    service_details = creator_principal.get(\"servicePrincipalDetails\", {})\n","                    creator_record[\"aad_app_id\"] = service_details.get(\"aadAppId\")\n","                \n","                elif principal_type == \"Group\":\n","                    group_details = creator_principal.get(\"groupDetails\", {})\n","                    creator_record[\"group_type\"] = group_details.get(\"groupType\")\n","                \n","                elif principal_type == \"ServicePrincipalProfile\":\n","                    profile_details = creator_principal.get(\"servicePrincipalProfileDetails\", {})\n","                    parent_principal = profile_details.get(\"parentPrincipal\", {})\n","                    creator_record[\"parent_principal_id\"] = parent_principal.get(\"id\")\n","                \n","                # Add to creators dictionary (this automatically handles duplicates)\n","                creators_dict[principal_id] = creator_record\n","        \n","        simplified_items.append(simplified_item)\n","    \n","    # Convert creators dict to list\n","    unique_creators = list(creators_dict.values())\n","    \n","    logger.info(f\"Extracted {len(simplified_items)} items and {len(unique_creators)} unique creators\")\n","    \n","    # Log creator type distribution\n","    if unique_creators:\n","        creator_types = {}\n","        for creator in unique_creators:\n","            creator_type = creator.get('principal_type', 'Unknown')\n","            creator_types[creator_type] = creator_types.get(creator_type, 0) + 1\n","        \n","        logger.info(\"Creator type distribution:\")\n","        for creator_type, count in sorted(creator_types.items()):\n","            logger.info(f\"  {creator_type}: {count}\")\n","    \n","    return simplified_items, unique_creators\n","\n","\n","def create_items_dataframe(items_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the items data into a PySpark DataFrame for Delta Lake.\n","    \n","    This function creates a structured DataFrame with the core item information,\n","    properly handling data types and adding metadata columns.\n","    \n","    Args:\n","        items_data: List of simplified item dictionaries\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame ready for Delta Lake storage\n","    \"\"\"\n","    # Define the schema for items table\n","    items_schema = StructType([\n","        StructField(\"id\", StringType(), False),                    # Primary key\n","        StructField(\"type\", StringType(), True),\n","        StructField(\"name\", StringType(), True),\n","        StructField(\"description\", StringType(), True),\n","        StructField(\"state\", StringType(), True),\n","        StructField(\"last_updated_date\", StringType(), True),      # Keep as string initially, will convert\n","        StructField(\"workspace_id\", StringType(), True),\n","        StructField(\"capacity_id\", StringType(), True),\n","        StructField(\"creator_principal_id\", StringType(), True),   # Foreign key to creators table\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Handle empty data\n","    if not items_data:\n","        logger.warning(\"No items found. Creating empty DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        return spark.createDataFrame(empty_rdd, items_schema)\n","    \n","    # Convert to pandas DataFrame first for easier manipulation\n","    pandas_df = pd.DataFrame(items_data)\n","    \n","    # Ensure all required columns exist with proper default values\n","    required_columns = [\"id\", \"type\", \"name\", \"description\", \"state\", \n","                       \"last_updated_date\", \"workspace_id\", \"capacity_id\", \"creator_principal_id\"]\n","    \n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","        # Convert any empty strings to None to avoid VOID type issues\n","        pandas_df[col_name] = pandas_df[col_name].replace('', None)\n","    \n","    # Create Spark DataFrame with explicit schema to avoid VOID type issues\n","    base_columns = [\"id\", \"type\", \"name\", \"description\", \"state\", \n","                   \"last_updated_date\", \"workspace_id\", \"capacity_id\", \"creator_principal_id\"]\n","    \n","    spark_df = spark.createDataFrame(\n","        pandas_df[base_columns],\n","        schema=StructType([\n","            StructField(\"id\", StringType(), False),\n","            StructField(\"type\", StringType(), True),\n","            StructField(\"name\", StringType(), True),\n","            StructField(\"description\", StringType(), True),\n","            StructField(\"state\", StringType(), True),\n","            StructField(\"last_updated_date\", StringType(), True),\n","            StructField(\"workspace_id\", StringType(), True),\n","            StructField(\"capacity_id\", StringType(), True),\n","            StructField(\"creator_principal_id\", StringType(), True)\n","        ])\n","    )\n","    \n","    # Add extraction timestamp\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    # Convert last_updated_date from string to timestamp if not null\n","    # Also handle potential invalid/ancient timestamps that cause Parquet write issues\n","    final_df = enhanced_df.withColumn(\n","        \"last_updated_date\",\n","        when(col(\"last_updated_date\").isNotNull(), \n","             # Only convert timestamps that are after 1900-01-01 to avoid Parquet issues\n","             when(col(\"last_updated_date\") >= \"1900-01-01T00:00:00.000Z\",\n","                  col(\"last_updated_date\").cast(TimestampType()))\n","             .otherwise(None))\n","        .otherwise(None)\n","    )\n","    \n","    return final_df\n","\n","\n","def create_creators_dataframe(creators_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the creators data into a PySpark DataFrame for Delta Lake.\n","    \n","    This function creates a structured DataFrame with the creator/principal information,\n","    properly handling the different principal types and their specific details.\n","    \n","    Args:\n","        creators_data: List of creator/principal dictionaries\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame ready for Delta Lake storage\n","    \"\"\"\n","    # Define the schema for creators table with explicit types\n","    creators_schema = StructType([\n","        StructField(\"principal_id\", StringType(), False),          # Primary key\n","        StructField(\"display_name\", StringType(), True),\n","        StructField(\"principal_type\", StringType(), True),\n","        StructField(\"user_principal_name\", StringType(), True),    # For User type\n","        StructField(\"aad_app_id\", StringType(), True),            # For ServicePrincipal type\n","        StructField(\"group_type\", StringType(), True),            # For Group type\n","        StructField(\"parent_principal_id\", StringType(), True),    # For ServicePrincipalProfile type\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Handle empty data\n","    if not creators_data:\n","        logger.warning(\"No creators found. Creating empty DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        return spark.createDataFrame(empty_rdd, creators_schema)\n","    \n","    # Convert to pandas DataFrame first for easier manipulation\n","    pandas_df = pd.DataFrame(creators_data)\n","    \n","    # Ensure all required columns exist with proper default values\n","    required_columns = [\"principal_id\", \"display_name\", \"principal_type\", \n","                       \"user_principal_name\", \"aad_app_id\", \"group_type\", \"parent_principal_id\"]\n","    \n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","        # Convert any empty strings to None to avoid VOID type issues\n","        pandas_df[col_name] = pandas_df[col_name].replace('', None)\n","    \n","    # Create Spark DataFrame with explicit schema to avoid VOID type issues\n","    # First create with the required columns only (excluding extraction_timestamp)\n","    base_columns = [\"principal_id\", \"display_name\", \"principal_type\", \n","                   \"user_principal_name\", \"aad_app_id\", \"group_type\", \"parent_principal_id\"]\n","    \n","    # Create the DataFrame with explicit schema\n","    spark_df = spark.createDataFrame(\n","        pandas_df[base_columns], \n","        schema=StructType([\n","            StructField(\"principal_id\", StringType(), False),\n","            StructField(\"display_name\", StringType(), True),\n","            StructField(\"principal_type\", StringType(), True),\n","            StructField(\"user_principal_name\", StringType(), True),\n","            StructField(\"aad_app_id\", StringType(), True),\n","            StructField(\"group_type\", StringType(), True),\n","            StructField(\"parent_principal_id\", StringType(), True)\n","        ])\n","    )\n","    \n","    # Add extraction timestamp\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 10 - Delta Lake Operations and Schema Fix Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists with the correct schema, creating or recreating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists and get its schema\n","        existing_table_schema = spark.table(table_name).schema\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","        \n","        # Check for VOID/NullType columns in existing schema\n","        has_void_columns = False\n","        for field in existing_table_schema.fields:\n","            if str(field.dataType) in ['VoidType', 'NullType']:\n","                logger.warning(f\"Found VOID/NullType column '{field.name}' in existing table\")\n","                has_void_columns = True\n","        \n","        if has_void_columns:\n","            logger.warning(f\"Table '{table_name}' has VOID/NullType columns. Recreating with correct schema...\")\n","            # Drop and recreate the table with correct schema\n","            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","            logger.info(f\"Dropped table '{table_name}' with invalid schema\")\n","            \n","            # Create new table with correct schema\n","            empty_df = spark.createDataFrame([], df_schema)\n","            empty_df.write \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .saveAsTable(table_name)\n","            \n","            logger.info(f\"Recreated Delta table '{table_name}' with correct schema\")\n","        else:\n","            # Check if schemas are compatible\n","            expected_fields = {field.name: field.dataType for field in df_schema.fields}\n","            existing_fields = {field.name: field.dataType for field in existing_table_schema.fields}\n","            \n","            schema_matches = True\n","            for field_name, expected_type in expected_fields.items():\n","                if field_name in existing_fields:\n","                    if str(existing_fields[field_name]) != str(expected_type):\n","                        logger.warning(f\"Schema mismatch for column '{field_name}': expected {expected_type}, found {existing_fields[field_name]}\")\n","                        schema_matches = False\n","                else:\n","                    logger.warning(f\"Missing column '{field_name}' in existing table\")\n","                    schema_matches = False\n","            \n","            if not schema_matches:\n","                logger.warning(f\"Schema mismatch detected. Recreating table '{table_name}' with correct schema...\")\n","                spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","                \n","                empty_df = spark.createDataFrame([], df_schema)\n","                empty_df.write \\\n","                    .mode(\"overwrite\") \\\n","                    .option(\"overwriteSchema\", \"true\") \\\n","                    .saveAsTable(table_name)\n","                \n","                logger.info(f\"Recreated Delta table '{table_name}' with correct schema\")\n","            else:\n","                logger.info(f\"Table '{table_name}' schema is compatible\")\n","                \n","    except Exception as e:\n","        # Table doesn't exist or other error, create it\n","        logger.info(f\"Creating Delta table '{table_name}' (reason: {str(e)})\")\n","        \n","        # Create an empty DataFrame with the correct schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def fix_void_columns_in_existing_tables():\n","    \"\"\"\n","    Fix VOID column issues in existing tables by recreating them with proper schemas.\n","    This is specifically needed for SQL Analytics Endpoint compatibility.\n","    \"\"\"\n","    logger.info(\"=\" * 60)\n","    logger.info(\"FIXING VOID COLUMNS FOR SQL ANALYTICS ENDPOINT COMPATIBILITY\")\n","    logger.info(\"=\" * 60)\n","    \n","    tables_to_fix = [\n","        CONFIG[\"ITEMS_TABLE_NAME\"],\n","        CONFIG[\"CREATORS_TABLE_NAME\"]\n","    ]\n","    \n","    for table_name in tables_to_fix:\n","        try:\n","            # Check if table exists\n","            existing_df = spark.table(table_name)\n","            logger.info(f\"Checking table '{table_name}' for VOID columns...\")\n","            \n","            # Get current schema\n","            current_schema = existing_df.schema\n","            has_void_columns = False\n","            \n","            for field in current_schema.fields:\n","                if str(field.dataType) in ['VoidType', 'NullType']:\n","                    logger.warning(f\"Found VOID/NullType column: {field.name} ({field.dataType})\")\n","                    has_void_columns = True\n","            \n","            if has_void_columns:\n","                logger.info(f\"Fixing VOID columns in table '{table_name}'...\")\n","                \n","                # Get the data from existing table\n","                existing_data = existing_df.collect()\n","                row_count = len(existing_data)\n","                logger.info(f\"Backing up {row_count} rows from '{table_name}'\")\n","                \n","                # Define correct schema based on table type\n","                if table_name == CONFIG[\"CREATORS_TABLE_NAME\"]:\n","                    correct_schema = StructType([\n","                        StructField(\"principal_id\", StringType(), False),\n","                        StructField(\"display_name\", StringType(), True),\n","                        StructField(\"principal_type\", StringType(), True),\n","                        StructField(\"user_principal_name\", StringType(), True),\n","                        StructField(\"aad_app_id\", StringType(), True),\n","                        StructField(\"group_type\", StringType(), True),\n","                        StructField(\"parent_principal_id\", StringType(), True),\n","                        StructField(\"extraction_timestamp\", TimestampType(), False)\n","                    ])\n","                else:  # items table\n","                    correct_schema = StructType([\n","                        StructField(\"id\", StringType(), False),\n","                        StructField(\"type\", StringType(), True),\n","                        StructField(\"name\", StringType(), True),\n","                        StructField(\"description\", StringType(), True),\n","                        StructField(\"state\", StringType(), True),\n","                        StructField(\"last_updated_date\", TimestampType(), True),\n","                        StructField(\"workspace_id\", StringType(), True),\n","                        StructField(\"capacity_id\", StringType(), True),\n","                        StructField(\"creator_principal_id\", StringType(), True),\n","                        StructField(\"extraction_timestamp\", TimestampType(), False)\n","                    ])\n","                \n","                # Create new DataFrame with correct schema and existing data\n","                if row_count > 0:\n","                    # Convert existing data to proper format\n","                    fixed_data = []\n","                    for row in existing_data:\n","                        row_dict = row.asDict()\n","                        # Convert any None values and ensure proper types\n","                        for field in correct_schema.fields:\n","                            if field.name not in row_dict:\n","                                row_dict[field.name] = None\n","                            elif row_dict[field.name] == '' or row_dict[field.name] == 'null':\n","                                row_dict[field.name] = None\n","                        fixed_data.append(row_dict)\n","                    \n","                    # Create pandas DataFrame and then Spark DataFrame\n","                    import pandas as pd\n","                    pandas_df = pd.DataFrame(fixed_data)\n","                    \n","                    # Create new DataFrame with correct schema\n","                    new_df = spark.createDataFrame(pandas_df, schema=correct_schema)\n","                else:\n","                    # Empty table case\n","                    new_df = spark.createDataFrame([], correct_schema)\n","                \n","                # Drop the old table and recreate\n","                spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","                logger.info(f\"Dropped table '{table_name}' with VOID columns\")\n","                \n","                # Create new table with correct schema\n","                new_df.write \\\n","                    .mode(\"overwrite\") \\\n","                    .option(\"overwriteSchema\", \"true\") \\\n","                    .saveAsTable(table_name)\n","                \n","                logger.info(f\"Recreated table '{table_name}' with proper schema\")\n","                logger.info(f\"Restored {row_count} rows to '{table_name}'\")\n","                \n","                # Verify the fix\n","                fixed_df = spark.table(table_name)\n","                logger.info(f\"Verification - New schema for '{table_name}':\")\n","                fixed_df.printSchema()\n","                \n","            else:\n","                logger.info(f\"Table '{table_name}' schema is already correct - no VOID columns found\")\n","                \n","        except Exception as e:\n","            logger.error(f\"Failed to check/fix table '{table_name}': {str(e)}\")\n","            # Continue with other tables\n","            continue\n","    \n","    logger.info(\"VOID column fix process completed!\")\n","    logger.info(\"Tables should now be compatible with SQL Analytics Endpoint\")\n","\n","\n","def merge_items_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new items data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if item ID matches\n","    - Inserts new records if item ID doesn't exist\n","    - Handles timestamp validation to prevent Parquet write issues\n","    \n","    Args:\n","        source_df: DataFrame with new items data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Validate and clean the source DataFrame before writing\n","    # Filter out any records with null IDs\n","    # Note: last_updated_date filtering is already handled in DataFrame creation\n","    cleaned_df = source_df.filter(col(\"id\").isNotNull())\n","    \n","    row_count_before = source_df.count()\n","    row_count_after = cleaned_df.count()\n","    \n","    if row_count_before != row_count_after:\n","        logger.warning(f\"Filtered out {row_count_before - row_count_after} records with null IDs\")\n","    \n","    # Validate source DataFrame schema before proceeding\n","    logger.info(\"Source DataFrame schema:\")\n","    cleaned_df.printSchema()\n","    \n","    # Validate target table schema\n","    try:\n","        target_df = spark.table(table_name)\n","        logger.info(\"Target table schema:\")\n","        target_df.printSchema()\n","        \n","        # Check for VOID/NullType columns in target\n","        for field in target_df.schema.fields:\n","            if str(field.dataType) in ['VoidType', 'NullType']:\n","                logger.error(f\"Target table has VOID/NullType column '{field.name}'. Cannot proceed with merge.\")\n","                logger.error(\"Please recreate the target table with correct schema.\")\n","                raise ValueError(f\"Invalid target table schema - column '{field.name}' has type {field.dataType}\")\n","                \n","    except Exception as e:\n","        logger.warning(f\"Cannot validate target table schema: {str(e)}. Proceeding with merge operation.\")\n","    \n","    # Create a temporary view for the merge operation\n","    cleaned_df.createOrReplaceTempView(\"items_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    try:\n","        if spark.table(table_name).count() == 0:\n","            logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","            cleaned_df.write.mode(\"append\").saveAsTable(table_name)\n","            return\n","    except Exception as e:\n","        logger.warning(f\"Could not check table row count: {str(e)}. Proceeding with merge operation.\")\n","    \n","    # Perform the merge operation\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING items_updates AS source\n","    ON target.id = source.id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.type = source.type,\n","            target.name = source.name,\n","            target.description = source.description,\n","            target.state = source.state,\n","            target.last_updated_date = source.last_updated_date,\n","            target.workspace_id = source.workspace_id,\n","            target.capacity_id = source.capacity_id,\n","            target.creator_principal_id = source.creator_principal_id,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    try:\n","        spark.sql(merge_query)\n","        logger.info(\"Items merge operation completed successfully\")\n","    except Exception as e:\n","        logger.error(f\"Merge operation failed: {str(e)}\")\n","        logger.info(\"Attempting fallback: direct insert with overwrite mode\")\n","        try:\n","            cleaned_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n","            logger.info(\"Fallback insert completed successfully\")\n","        except Exception as fallback_e:\n","            logger.error(f\"Fallback insert also failed: {str(fallback_e)}\")\n","            raise\n","\n","\n","def merge_creators_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new creators data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if principal ID matches\n","    - Inserts new records if principal ID doesn't exist\n","    - Handles schema validation to prevent type mismatch issues\n","    \n","    Args:\n","        source_df: DataFrame with new creators data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Validate source DataFrame schema before proceeding\n","    logger.info(\"Source DataFrame schema:\")\n","    source_df.printSchema()\n","    \n","    # Validate target table schema\n","    try:\n","        target_df = spark.table(table_name)\n","        logger.info(\"Target table schema:\")\n","        target_df.printSchema()\n","        \n","        # Check for VOID/NullType columns in target\n","        for field in target_df.schema.fields:\n","            if str(field.dataType) in ['VoidType', 'NullType']:\n","                logger.error(f\"Target table has VOID/NullType column '{field.name}'. Cannot proceed with merge.\")\n","                logger.error(\"Please recreate the target table with correct schema.\")\n","                raise ValueError(f\"Invalid target table schema - column '{field.name}' has type {field.dataType}\")\n","                \n","    except Exception as e:\n","        logger.error(f\"Cannot validate target table schema: {str(e)}\")\n","        raise\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"creators_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    try:\n","        if spark.table(table_name).count() == 0:\n","            logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","            source_df.write.mode(\"append\").saveAsTable(table_name)\n","            return\n","    except Exception as e:\n","        logger.warning(f\"Could not check target table row count: {str(e)}. Proceeding with merge operation.\")\n","    \n","    # Perform the merge operation\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING creators_updates AS source\n","    ON target.principal_id = source.principal_id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.display_name = source.display_name,\n","            target.principal_type = source.principal_type,\n","            target.user_principal_name = source.user_principal_name,\n","            target.aad_app_id = source.aad_app_id,\n","            target.group_type = source.group_type,\n","            target.parent_principal_id = source.parent_principal_id,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    try:\n","        spark.sql(merge_query)\n","        logger.info(\"Creators merge operation completed successfully\")\n","    except Exception as e:\n","        logger.error(f\"Merge operation failed: {str(e)}\")\n","        logger.info(\"Attempting fallback: direct insert with overwrite mode\")\n","        try:\n","            source_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n","            logger.info(\"Fallback insert completed successfully\")\n","        except Exception as fallback_e:\n","            logger.error(f\"Fallback insert also failed: {str(fallback_e)}\")\n","            raise\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Updates table statistics for query optimization\n","    - Uses a compatible method for Microsoft Fabric\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # Note: In Microsoft Fabric, Delta table optimization may be handled automatically\n","        # or through different commands than traditional Delta Lake\n","        # The standard OPTIMIZE and ZORDER commands might not be available\n","        \n","        # Alternative approach: Use Delta table properties to hint at optimization\n","        try:\n","            delta_table = DeltaTable.forName(spark, table_name)\n","            logger.info(\"Delta table optimization completed via statistics computation\")\n","            logger.info(\"Note: Microsoft Fabric may automatically optimize Delta tables\")\n","        except Exception as delta_e:\n","            logger.info(f\"Delta table reference not available: {str(delta_e)}\")\n","            logger.info(\"Continuing with standard statistics optimization\")\n","        \n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 10 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all items from the Fabric List Items API\n","    3. Extracts and normalizes items and creators data\n","    4. Creates two enhanced PySpark DataFrames\n","    5. Loads data into two normalized Delta Lake tables\n","    6. Optimizes the tables for analytics\n","    7. Provides comprehensive reporting and statistics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric Items to Delta Lake process\")\n","        logger.info(\"This process will create two normalized tables:\")\n","        logger.info(f\"  1. {CONFIG['ITEMS_TABLE_NAME']} - Core item information\")\n","        logger.info(f\"  2. {CONFIG['CREATORS_TABLE_NAME']} - Normalized creator/principal information\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve all items from API\n","        logger.info(\"Retrieving items from Fabric List Items API...\")\n","        logger.info(\"Note: This API supports up to 10,000 items per request with pagination\")\n","        items_data = get_all_items(access_token)\n","        \n","        if not items_data:\n","            logger.warning(\"No items found. Please check your permissions and API access.\")\n","            logger.warning(\"Required permissions: Fabric administrator or service principal\")\n","            logger.warning(\"Required scopes: Tenant.Read.All or Tenant.ReadWrite.All\")\n","            \n","            # Create empty dataframes with proper schemas for consistent table structure\n","            items_schema = StructType([\n","                StructField(\"id\", StringType(), False),\n","                StructField(\"type\", StringType(), True),\n","                StructField(\"name\", StringType(), True),\n","                StructField(\"description\", StringType(), True),\n","                StructField(\"state\", StringType(), True),\n","                StructField(\"last_updated_date\", TimestampType(), True),\n","                StructField(\"workspace_id\", StringType(), True),\n","                StructField(\"capacity_id\", StringType(), True),\n","                StructField(\"creator_principal_id\", StringType(), True),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            \n","            creators_schema = StructType([\n","                StructField(\"principal_id\", StringType(), False),\n","                StructField(\"display_name\", StringType(), True),\n","                StructField(\"principal_type\", StringType(), True),\n","                StructField(\"user_principal_name\", StringType(), True),\n","                StructField(\"aad_app_id\", StringType(), True),\n","                StructField(\"group_type\", StringType(), True),\n","                StructField(\"parent_principal_id\", StringType(), True),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            \n","            items_df = spark.createDataFrame([], items_schema)\n","            creators_df = spark.createDataFrame([], creators_schema)\n","            \n","        else:\n","            # Step 3: Extract and normalize data\n","            logger.info(f\"Extracting and normalizing data from {len(items_data)} items...\")\n","            simplified_items, unique_creators = extract_items_and_creators(items_data)\n","            \n","            # Step 4: Create DataFrames\n","            logger.info(\"Creating optimized DataFrames for Delta Lake storage...\")\n","            items_df = create_items_dataframe(simplified_items)\n","            creators_df = create_creators_dataframe(unique_creators)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of items data:\")\n","        items_df.show(5, truncate=False)\n","        \n","        logger.info(\"Sample of creators data:\")\n","        creators_df.show(5, truncate=False)\n","        \n","        # Step 5: Prepare and populate Items Delta table\n","        items_table_name = CONFIG[\"ITEMS_TABLE_NAME\"]\n","        creators_table_name = CONFIG[\"CREATORS_TABLE_NAME\"]\n","        \n","        logger.info(\"Setting up Delta tables...\")\n","        ensure_delta_table_exists(items_table_name, items_df.schema)\n","        ensure_delta_table_exists(creators_table_name, creators_df.schema)\n","        \n","        # Step 6: Merge data into Delta tables (if we have data)\n","        if items_data:\n","            logger.info(\"Merging data into Delta tables...\")\n","            \n","            # Merge creators first (since items reference creators)\n","            merge_creators_to_delta(creators_df, creators_table_name)\n","            \n","            # Then merge items\n","            merge_items_to_delta(items_df, items_table_name)\n","            \n","            # Step 7: Optimize the Delta tables\n","            logger.info(\"Optimizing Delta tables for query performance...\")\n","            optimize_delta_table(items_table_name)\n","            optimize_delta_table(creators_table_name)\n","        \n","        # Step 8: Fix VOID columns for SQL Analytics Endpoint compatibility\n","        logger.info(\"Fixing any VOID column issues for SQL Analytics Endpoint compatibility...\")\n","        fix_void_columns_in_existing_tables()\n","        \n","        # Step 9: Display comprehensive statistics and information\n","        logger.info(\"=\" * 60)\n","        logger.info(\"FABRIC ITEMS ETL PROCESS COMPLETED SUCCESSFULLY!\")\n","        logger.info(\"=\" * 60)\n","        \n","        # Show table details\n","        logger.info(\"Items table details:\")\n","        spark.sql(f\"DESCRIBE DETAIL {items_table_name}\").show(truncate=False)\n","        \n","        logger.info(\"Creators table details:\")\n","        spark.sql(f\"DESCRIBE DETAIL {creators_table_name}\").show(truncate=False)\n","        \n","        # Show row counts\n","        items_count = spark.table(items_table_name).count()\n","        creators_count = spark.table(creators_table_name).count()\n","        \n","        logger.info(f\"Total rows in {items_table_name}: {items_count}\")\n","        logger.info(f\"Total rows in {creators_table_name}: {creators_count}\")\n","        \n","        # Show comprehensive summary statistics\n","        if items_count > 0:\n","            logger.info(\"=\" * 40)\n","            logger.info(\"ITEMS SUMMARY STATISTICS\")\n","            logger.info(\"=\" * 40)\n","            \n","            summary_stats = spark.sql(f\"\"\"\n","                SELECT \n","                    COUNT(*) as total_items,\n","                    COUNT(DISTINCT workspace_id) as unique_workspaces,\n","                    COUNT(DISTINCT capacity_id) as unique_capacities,\n","                    COUNT(DISTINCT type) as unique_item_types,\n","                    COUNT(DISTINCT state) as unique_states,\n","                    COUNT(DISTINCT creator_principal_id) as unique_creators,\n","                    MAX(extraction_timestamp) as last_updated\n","                FROM {items_table_name}\n","            \"\"\")\n","            \n","            summary_stats.show(truncate=False)\n","            \n","            # Show item type distribution\n","            logger.info(\"Item Type Distribution:\")\n","            type_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    type,\n","                    COUNT(*) as count,\n","                    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n","                FROM {items_table_name}\n","                GROUP BY type\n","                ORDER BY count DESC\n","            \"\"\")\n","            type_distribution.show(20, truncate=False)\n","            \n","            # Show state distribution\n","            logger.info(\"Item State Distribution:\")\n","            state_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    state,\n","                    COUNT(*) as count\n","                FROM {items_table_name}\n","                GROUP BY state\n","                ORDER BY count DESC\n","            \"\"\")\n","            state_distribution.show(truncate=False)\n","            \n","            # Show top workspaces by item count\n","            logger.info(\"Top 10 Workspaces by Item Count:\")\n","            workspace_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    workspace_id,\n","                    COUNT(*) as item_count,\n","                    COUNT(DISTINCT type) as unique_types\n","                FROM {items_table_name}\n","                WHERE workspace_id IS NOT NULL\n","                GROUP BY workspace_id\n","                ORDER BY item_count DESC\n","                LIMIT 10\n","            \"\"\")\n","            workspace_distribution.show(truncate=False)\n","        \n","        if creators_count > 0:\n","            logger.info(\"=\" * 40)\n","            logger.info(\"CREATORS SUMMARY STATISTICS\")\n","            logger.info(\"=\" * 40)\n","            \n","            # Show creator type distribution\n","            creator_type_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    principal_type,\n","                    COUNT(*) as count,\n","                    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n","                FROM {creators_table_name}\n","                GROUP BY principal_type\n","                ORDER BY count DESC\n","            \"\"\")\n","            creator_type_distribution.show(truncate=False)\n","            \n","            # Show top creators by item count (join with items table)\n","            logger.info(\"Top 10 Most Active Creators:\")\n","            top_creators = spark.sql(f\"\"\"\n","                SELECT \n","                    c.display_name,\n","                    c.principal_type,\n","                    c.user_principal_name,\n","                    COUNT(i.id) as items_created\n","                FROM {creators_table_name} c\n","                JOIN {items_table_name} i ON c.principal_id = i.creator_principal_id\n","                GROUP BY c.principal_id, c.display_name, c.principal_type, c.user_principal_name\n","                ORDER BY items_created DESC\n","                LIMIT 10\n","            \"\"\")\n","            top_creators.show(truncate=False)\n","        \n","        # Show sample join query demonstrating the two-table relationship\n","        if items_count > 0 and creators_count > 0:\n","            logger.info(\"=\" * 40)\n","            logger.info(\"SAMPLE JOIN QUERY - Items with Creator Details\")\n","            logger.info(\"=\" * 40)\n","            \n","            sample_join = spark.sql(f\"\"\"\n","                SELECT \n","                    i.name as item_name,\n","                    i.type as item_type,\n","                    i.state,\n","                    i.workspace_id,\n","                    c.display_name as creator_name,\n","                    c.principal_type as creator_type,\n","                    c.user_principal_name,\n","                    i.last_updated_date\n","                FROM {items_table_name} i\n","                LEFT JOIN {creators_table_name} c ON i.creator_principal_id = c.principal_id\n","                ORDER BY i.last_updated_date DESC\n","                LIMIT 10\n","            \"\"\")\n","            sample_join.show(truncate=False)\n","        \n","        logger.info(\"=\" * 60)\n","        logger.info(\"PROCESS COMPLETED - Data is ready for analytics!\")\n","        logger.info(\"=\" * 60)\n","        logger.info(f\"Tables created:\")\n","        logger.info(f\"   {items_table_name} - {items_count} records\")\n","        logger.info(f\"   {creators_table_name} - {creators_count} records\")\n","        logger.info(\"Use these tables for Fabric governance, compliance, and analytics!\")\n","        \n","        return items_df, creators_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        logger.error(\"Please check:\")\n","        logger.error(\"  1. You have Fabric administrator permissions\")\n","        logger.error(\"  2. Required scopes: Tenant.Read.All or Tenant.ReadWrite.All\")\n","        logger.error(\"  3. API rate limits (200 requests/hour)\")\n","        logger.error(\"  4. Network connectivity to api.fabric.microsoft.com\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 11 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    items_df, creators_df = main()\n","    \n","    # Optional: Store DataFrames in variables for further analysis\n","    print(\"\\nDataFrames are available as:\")\n","    print(\"  - items_df: Main items DataFrame\")\n","    print(\"  - creators_df: Creators DataFrame\")\n","    print(\"\\nYou can now perform additional analysis or create custom queries!\")\n","# ==================================\n","\n","\n","# CELL 12 - Optional: Additional Analysis Examples\n","# ==================================\n","# OPTIONAL CELL - Uncomment and run for additional analysis examples\n","# This cell provides examples of how to use the data for common analytics scenarios\n","\n","\"\"\"\n","# Example 1: Find all items created in the last 30 days\n","recent_items = spark.sql(f'''\n","    SELECT \n","        i.name,\n","        i.type,\n","        c.display_name as creator,\n","        i.last_updated_date\n","    FROM {CONFIG[\"ITEMS_TABLE_NAME\"]} i\n","    LEFT JOIN {CONFIG[\"CREATORS_TABLE_NAME\"]} c ON i.creator_principal_id = c.principal_id\n","    WHERE i.last_updated_date >= date_sub(current_date(), 30)\n","    ORDER BY i.last_updated_date DESC\n","''')\n","\n","print(\"Items created/updated in the last 30 days:\")\n","recent_items.show(10, truncate=False)\n","\n","\n","# Example 2: Workspace utilization analysis\n","workspace_analysis = spark.sql(f'''\n","    SELECT \n","        workspace_id,\n","        COUNT(*) as total_items,\n","        COUNT(DISTINCT type) as item_types,\n","        COUNT(DISTINCT creator_principal_id) as unique_creators,\n","        MAX(last_updated_date) as most_recent_activity\n","    FROM {CONFIG[\"ITEMS_TABLE_NAME\"]}\n","    WHERE workspace_id IS NOT NULL\n","    GROUP BY workspace_id\n","    ORDER BY total_items DESC\n","''')\n","\n","print(\"Workspace utilization analysis:\")\n","workspace_analysis.show(20, truncate=False)\n","\n","\n","# Example 3: Creator productivity analysis\n","creator_productivity = spark.sql(f'''\n","    SELECT \n","        c.display_name,\n","        c.principal_type,\n","        COUNT(i.id) as items_created,\n","        COUNT(DISTINCT i.type) as item_types_created,\n","        COUNT(DISTINCT i.workspace_id) as workspaces_contributed,\n","        MAX(i.last_updated_date) as most_recent_item\n","    FROM {CONFIG[\"CREATORS_TABLE_NAME\"]} c\n","    JOIN {CONFIG[\"ITEMS_TABLE_NAME\"]} i ON c.principal_id = i.creator_principal_id\n","    GROUP BY c.principal_id, c.display_name, c.principal_type\n","    HAVING items_created >= 5\n","    ORDER BY items_created DESC\n","''')\n","\n","print(\"Most productive creators (5+ items):\")\n","creator_productivity.show(15, truncate=False)\n","\n","\n","# Example 4: Item type distribution by workspace\n","type_by_workspace = spark.sql(f'''\n","    SELECT \n","        workspace_id,\n","        type as item_type,\n","        COUNT(*) as count\n","    FROM {CONFIG[\"ITEMS_TABLE_NAME\"]}\n","    WHERE workspace_id IS NOT NULL\n","    GROUP BY workspace_id, type\n","    ORDER BY workspace_id, count DESC\n","''')\n","\n","print(\"Item type distribution by workspace:\")\n","type_by_workspace.show(50, truncate=False)\n","\"\"\"\n","\n","print(\"Additional analysis examples are available in Cell 12\")\n","print(\"Uncomment the code in Cell 12 to run example analytics queries\")\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"f516eaea-ae7d-4600-9404-46789e43c376","normalized_state":"finished","queued_time":"2025-07-16T15:52:04.9018449Z","session_start_time":null,"execution_start_time":"2025-07-16T15:52:18.1571906Z","execution_finish_time":"2025-07-16T15:53:38.8377556Z","parent_msg_id":"a971288e-a326-47af-bb5e-41e412b7aa74"},"text/plain":"StatementMeta(, f516eaea-ae7d-4600-9404-46789e43c376, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-16 15:52:18,813 - INFO - API Rate Limit: 200 requests/hour\n2025-07-16 15:52:18,813 - INFO - Max records per request: 10000\n2025-07-16 15:52:18,819 - INFO - Starting Fabric Items to Delta Lake process\n2025-07-16 15:52:18,820 - INFO - This process will create two normalized tables:\n2025-07-16 15:52:18,821 - INFO -   1. fabric_items - Core item information\n2025-07-16 15:52:18,822 - INFO -   2. fabric_items_creators - Normalized creator/principal information\n2025-07-16 15:52:18,822 - INFO - Getting access token...\n2025-07-16 15:52:20,227 - INFO - Successfully obtained access token for Fabric API\n2025-07-16 15:52:20,227 - INFO - Successfully obtained access token\n2025-07-16 15:52:20,228 - INFO - Retrieving items from Fabric List Items API...\n2025-07-16 15:52:20,228 - INFO - Note: This API supports up to 10,000 items per request with pagination\n2025-07-16 15:52:20,229 - INFO - Starting to retrieve all Fabric items...\n2025-07-16 15:52:20,229 - INFO - Items will be retrieved in this order: Fabric items, Datamarts, Reports, Dashboards, SemanticModels, Apps, Dataflows\n2025-07-16 15:52:20,230 - INFO - Page 1: Making initial request\n2025-07-16 15:52:20,231 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items with params: None (Attempt 1)\n2025-07-16 15:52:20,586 - INFO - Response status: 200\n2025-07-16 15:52:20,589 - INFO - Response contains 219 items in 'itemEntities' array\n2025-07-16 15:52:20,589 - INFO - Response contains continuationToken: MSwxMDAwMCww\n2025-07-16 15:52:20,594 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:20,595 - INFO - Retrieved 219 items on page 1. Running total: 219\n2025-07-16 15:52:20,595 - INFO - Sample item: {\n  \"id\": \"5c3b932b-b27d-456e-8a40-2b699c3952ab\",\n  \"type\": \"Lakehouse\",\n  \"name\": \"LH_Bronze\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"2024-09-12T13:53:13.3821849\",\n  \"creatorPrincipal\": {\n    \"id\": \"d87d15b5-62cd-41e0-87fd-92329908fadb\",\n    \"displayName\": \"Rey Allen Gatilao\",\n    \"type\": \"User\",\n    \"userDetails\": {\n      \"userPrincipalName\": \"rgatilao@mdanderson.org\"\n    }\n  },\n  \"workspaceId\": \"1a95e3bb-a11d-45bc-84d4-5c2c1f1aa990\",\n  \"capacityId\": \"c73a5223-9ef6-4514-83cc-3e70297ee377\"\n}\n2025-07-16 15:52:20,597 - INFO - Page 1 item types: {'Lakehouse': 48, 'DataPipeline': 38, 'Notebook': 115, 'Reflex': 5, 'Environment': 6, 'Eventstream': 1, 'Eventhouse': 1, 'KQLDatabase': 1, 'OrgApp': 1, 'Exploration': 1, 'CopyJob': 2}\n2025-07-16 15:52:20,598 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:20,598 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MSwxMDAwMCww\n2025-07-16 15:52:20,600 - INFO - Page 2: Making request with continuation token\n2025-07-16 15:52:20,602 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MSwxMDAwMCww (Attempt 1)\n2025-07-16 15:52:21,140 - INFO - Response status: 200\n2025-07-16 15:52:21,141 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:21,142 - INFO - Retrieved 93 items on page 2. Running total: 312\n2025-07-16 15:52:21,142 - INFO - Sample item: {\n  \"id\": \"0214d478-2f5a-40b5-8788-026a207ddc2b\",\n  \"type\": \"Warehouse\",\n  \"name\": \"DataflowsStagingWarehouse\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"2025-07-09T17:27:43.37\",\n  \"creatorPrincipal\": {\n    \"id\": \"f12096e4-4ee8-4adc-8f64-1b25cc463fc4\",\n    \"displayName\": \"Brent Hand\",\n    \"type\": \"User\",\n    \"userDetails\": {\n      \"userPrincipalName\": \"bwhand@mdanderson.org\"\n    }\n  },\n  \"workspaceId\": \"667bdbc5-6a2b-427e-ac17-c5a2a7bb0960\",\n  \"capacityId\": \"4dc39e58-c232-494c-b629-45298de2fa27\"\n}\n2025-07-16 15:52:21,143 - INFO - Page 2 item types: {'Warehouse': 42, 'Datamart': 3, 'SQLEndpoint': 48}\n2025-07-16 15:52:21,143 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:21,144 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MiwxMDAwMCww\n2025-07-16 15:52:21,145 - INFO - Page 3: Making request with continuation token\n2025-07-16 15:52:21,145 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MiwxMDAwMCww (Attempt 1)\n2025-07-16 15:52:27,834 - INFO - Response status: 200\n2025-07-16 15:52:27,862 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:27,862 - INFO - Retrieved 10000 items on page 3. Running total: 10312\n2025-07-16 15:52:27,863 - INFO - Sample item: {\n  \"id\": \"6d760708-b3ca-4da3-90d4-31d94885bc81\",\n  \"type\": \"Report\",\n  \"name\": \"Shipping Data\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"0001-01-01T00:00:00\",\n  \"workspaceId\": \"21c0515f-4846-4031-8f7f-fa710636f7ec\",\n  \"capacityId\": \"335562e4-1800-43e9-b53f-e7947e4a98b7\"\n}\n2025-07-16 15:52:27,865 - INFO - Page 3 item types: {'Report': 10000}\n2025-07-16 15:52:27,866 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:27,866 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MiwxMDAwMCwxMDAwMA%3d%3d\n2025-07-16 15:52:27,866 - INFO - Page 4: Making request with continuation token\n2025-07-16 15:52:27,867 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MiwxMDAwMCwxMDAwMA%3D%3D (Attempt 1)\n2025-07-16 15:52:33,495 - INFO - Response status: 200\n2025-07-16 15:52:33,496 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:33,496 - INFO - Retrieved 105 items on page 4. Running total: 10417\n2025-07-16 15:52:33,497 - INFO - Sample item: {\n  \"id\": \"cdea47df-c802-489b-8c0a-7926cb2ea7e3\",\n  \"type\": \"Report\",\n  \"name\": \"M. Von-Maszewski, MD- Faculty \",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"2020-02-25T21:27:22.3\",\n  \"creatorPrincipal\": {\n    \"id\": \"ed7a2b87-6676-4e3b-8275-54e7a2439c05\",\n    \"displayName\": \"Jazmin Owens\",\n    \"type\": \"User\",\n    \"userDetails\": {\n      \"userPrincipalName\": \"JCOwens@mdanderson.org\"\n    }\n  },\n  \"workspaceId\": \"0ad9567d-3ea3-4a04-9b01-2728f5bd5a30\",\n  \"capacityId\": \"4b2fd937-be2d-4ade-abf7-529bc655d981\"\n}\n2025-07-16 15:52:33,497 - INFO - Page 4 item types: {'Report': 105}\n2025-07-16 15:52:33,498 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:33,498 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MywxMDAwMCww\n2025-07-16 15:52:33,499 - INFO - Page 5: Making request with continuation token\n2025-07-16 15:52:33,499 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=MywxMDAwMCww (Attempt 1)\n2025-07-16 15:52:34,453 - INFO - Response status: 200\n2025-07-16 15:52:34,456 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:34,457 - INFO - Retrieved 819 items on page 5. Running total: 11236\n2025-07-16 15:52:34,457 - INFO - Sample item: {\n  \"id\": \"fa0291b9-abaf-4c73-8afa-c18c66567695\",\n  \"type\": \"Dashboard\",\n  \"name\": \"Supplier Quality Analysis Sample\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"0001-01-01T00:00:00\",\n  \"workspaceId\": \"512b837e-b824-42f0-893d-ba83037425c5\",\n  \"capacityId\": \"335562e4-1800-43e9-b53f-e7947e4a98b7\"\n}\n2025-07-16 15:52:34,458 - INFO - Page 5 item types: {'Dashboard': 819}\n2025-07-16 15:52:34,458 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:34,459 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NCwxMDAwMCww\n2025-07-16 15:52:34,459 - INFO - Page 6: Making request with continuation token\n2025-07-16 15:52:34,460 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NCwxMDAwMCww (Attempt 1)\n2025-07-16 15:52:39,199 - INFO - Response status: 200\n2025-07-16 15:52:39,324 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:39,325 - INFO - Retrieved 10000 items on page 6. Running total: 21236\n2025-07-16 15:52:39,325 - INFO - Sample item: {\n  \"id\": \"d657676d-237b-40a0-bf5b-8a1c62cdf773\",\n  \"type\": \"SemanticModel\",\n  \"name\": \"CE TRB Responses\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"2022-06-01T13:09:50.16\",\n  \"creatorPrincipal\": {\n    \"id\": \"56c30960-cfb1-4dc6-97d1-ea3c19122da5\",\n    \"displayName\": \"Jessica Pettigrew\",\n    \"type\": \"User\",\n    \"userDetails\": {\n      \"userPrincipalName\": \"JVPettigrew@mdanderson.org\"\n    }\n  },\n  \"workspaceId\": \"c0734ebf-8221-4623-80cb-35e58851c580\",\n  \"capacityId\": \"6f2baf05-9c63-4402-be41-c719011112f2\"\n}\n2025-07-16 15:52:39,327 - INFO - Page 6 item types: {'SemanticModel': 10000}\n2025-07-16 15:52:39,328 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:39,328 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NCwxMDAwMCwxMDAwMA%3d%3d\n2025-07-16 15:52:39,328 - INFO - Page 7: Making request with continuation token\n2025-07-16 15:52:39,329 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NCwxMDAwMCwxMDAwMA%3D%3D (Attempt 1)\n2025-07-16 15:52:44,685 - INFO - Response status: 200\n2025-07-16 15:52:44,690 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:44,691 - INFO - Retrieved 1345 items on page 7. Running total: 22581\n2025-07-16 15:52:44,691 - INFO - Sample item: {\n  \"id\": \"47dd38f6-6508-4e61-a48d-9268392f2cef\",\n  \"type\": \"SemanticModel\",\n  \"name\": \"Human Resources Sample\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"2019-02-15T17:37:44.937\",\n  \"creatorPrincipal\": {\n    \"id\": \"07b6db72-67ec-4533-b441-2f73536fdadd\",\n    \"displayName\": \"Roberto Hernandez\",\n    \"type\": \"User\",\n    \"userDetails\": {\n      \"userPrincipalName\": \"RAHernandez2@mdanderson.org\"\n    }\n  },\n  \"workspaceId\": \"68d0e8f6-1c85-4a10-a25c-853fe99d998b\",\n  \"capacityId\": \"b9750da0-6ee7-4457-bd3e-ac6f21fba083\"\n}\n2025-07-16 15:52:44,692 - INFO - Page 7 item types: {'SemanticModel': 1345}\n2025-07-16 15:52:44,692 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:44,693 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NSwxMDAwMCww\n2025-07-16 15:52:44,693 - INFO - Page 8: Making request with continuation token\n2025-07-16 15:52:44,693 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NSwxMDAwMCww (Attempt 1)\n2025-07-16 15:52:45,353 - INFO - Response status: 200\n2025-07-16 15:52:45,355 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:45,355 - INFO - Retrieved 415 items on page 8. Running total: 22996\n2025-07-16 15:52:45,355 - INFO - Sample item: {\n  \"id\": \"0dcbcb62-445f-48bb-bd20-d65d8e1b1685\",\n  \"type\": \"App\",\n  \"name\": \"Github 8/26/2024 1:59:22 PM\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"2024-08-26T18:59:36.877\",\n  \"workspaceId\": \"aad173a4-46ac-4b68-8718-e1ebc008f238\",\n  \"capacityId\": \"6f2baf05-9c63-4402-be41-c719011112f2\"\n}\n2025-07-16 15:52:45,356 - INFO - Page 8 item types: {'App': 415}\n2025-07-16 15:52:45,357 - INFO - Found continuation token, will retrieve next page\n2025-07-16 15:52:45,357 - INFO - Continuation URI: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NiwxMDAwMCww\n2025-07-16 15:52:45,357 - INFO - Page 9: Making request with continuation token\n2025-07-16 15:52:45,358 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/items?continuationToken=NiwxMDAwMCww (Attempt 1)\n2025-07-16 15:52:45,660 - INFO - Response status: 200\n2025-07-16 15:52:45,662 - INFO - Response keys: ['itemEntities', 'continuationUri', 'continuationToken']\n2025-07-16 15:52:45,662 - INFO - Retrieved 298 items on page 9. Running total: 23294\n2025-07-16 15:52:45,663 - INFO - Sample item: {\n  \"id\": \"aa0029b7-4f7e-47a2-9310-3434c3c7d41f\",\n  \"type\": \"Dataflow\",\n  \"name\": \"Master Nova Notes\",\n  \"description\": \"This dataflow is connecting to the Master Nova Notes list.\",\n  \"state\": \"Active\",\n  \"lastUpdatedDate\": \"2020-01-16T14:05:44.063\",\n  \"creatorPrincipal\": {\n    \"id\": \"9eed0af7-c696-4778-9395-b028abc1b256\",\n    \"displayName\": \"Eric Pena\",\n    \"type\": \"User\",\n    \"userDetails\": {\n      \"userPrincipalName\": \"eepena@mdanderson.org\"\n    }\n  },\n  \"workspaceId\": \"9beed1e6-1368-4a64-9cb9-1a2a9630eb92\",\n  \"capacityId\": \"335562e4-1800-43e9-b53f-e7947e4a98b7\"\n}\n2025-07-16 15:52:45,663 - INFO - Page 9 item types: {'Dataflow': 298}\n2025-07-16 15:52:45,664 - INFO - No continuation token found - this is the last page\n2025-07-16 15:52:45,664 - INFO - Finished retrieving all items. Total pages: 9, Total items: 23294\n2025-07-16 15:52:45,669 - INFO - Final item type distribution:\n2025-07-16 15:52:45,669 - INFO -   App: 415\n2025-07-16 15:52:45,669 - INFO -   CopyJob: 2\n2025-07-16 15:52:45,670 - INFO -   Dashboard: 819\n2025-07-16 15:52:45,670 - INFO -   DataPipeline: 38\n2025-07-16 15:52:45,671 - INFO -   Dataflow: 298\n2025-07-16 15:52:45,673 - INFO -   Datamart: 3\n2025-07-16 15:52:45,673 - INFO -   Environment: 6\n2025-07-16 15:52:45,674 - INFO -   Eventhouse: 1\n2025-07-16 15:52:45,674 - INFO -   Eventstream: 1\n2025-07-16 15:52:45,675 - INFO -   Exploration: 1\n2025-07-16 15:52:45,675 - INFO -   KQLDatabase: 1\n2025-07-16 15:52:45,676 - INFO -   Lakehouse: 48\n2025-07-16 15:52:45,676 - INFO -   Notebook: 115\n2025-07-16 15:52:45,676 - INFO -   OrgApp: 1\n2025-07-16 15:52:45,677 - INFO -   Reflex: 5\n2025-07-16 15:52:45,677 - INFO -   Report: 10105\n2025-07-16 15:52:45,678 - INFO -   SQLEndpoint: 48\n2025-07-16 15:52:45,679 - INFO -   SemanticModel: 11345\n2025-07-16 15:52:45,680 - INFO -   Warehouse: 42\n2025-07-16 15:52:45,682 - INFO - Extracting and normalizing data from 23294 items...\n2025-07-16 15:52:45,684 - INFO - Processing 23294 items to extract normalized data...\n2025-07-16 15:52:45,708 - INFO - Extracted 23294 items and 5720 unique creators\n2025-07-16 15:52:45,710 - INFO - Creator type distribution:\n2025-07-16 15:52:45,711 - INFO -   User: 5720\n2025-07-16 15:52:45,711 - INFO - Creating optimized DataFrames for Delta Lake storage...\n2025-07-16 15:53:24,096 - INFO - Items merge operation completed successfully\n2025-07-16 15:53:24,097 - INFO - Optimizing Delta tables for query performance...\n2025-07-16 15:53:24,097 - INFO - Optimizing Delta table 'fabric_items'\n2025-07-16 15:53:26,965 - INFO - Table statistics updated successfully\n2025-07-16 15:53:27,235 - INFO - Delta table optimization completed via statistics computation\n2025-07-16 15:53:27,236 - INFO - Note: Microsoft Fabric may automatically optimize Delta tables\n2025-07-16 15:53:27,237 - INFO - Optimizing Delta table 'fabric_items_creators'\n"]},{"output_type":"stream","name":"stdout","text":["+------------------------------------+------------+-------------------------+------------+------+--------------------------+------------------------------------+------------------------------------+------------------------------------+--------------------------+\n|id                                  |type        |name                     |description |state |last_updated_date         |workspace_id                        |capacity_id                         |creator_principal_id                |extraction_timestamp      |\n+------------------------------------+------------+-------------------------+------------+------+--------------------------+------------------------------------+------------------------------------+------------------------------------+--------------------------+\n|5c3b932b-b27d-456e-8a40-2b699c3952ab|Lakehouse   |LH_Bronze                |NULL        |Active|2024-09-12 13:53:13.382184|1a95e3bb-a11d-45bc-84d4-5c2c1f1aa990|c73a5223-9ef6-4514-83cc-3e70297ee377|d87d15b5-62cd-41e0-87fd-92329908fadb|2025-07-16 15:52:47.505373|\n|56cb05a1-5211-4ac3-9d25-5edffdffd75a|DataPipeline|Pipeline_Bronze          |NULL        |Active|2024-09-12 13:53:14.116562|1a95e3bb-a11d-45bc-84d4-5c2c1f1aa990|c73a5223-9ef6-4514-83cc-3e70297ee377|d87d15b5-62cd-41e0-87fd-92329908fadb|2025-07-16 15:52:47.505373|\n|9e39eec2-77b5-44b7-b2ac-8bc0c25a6f85|Notebook    |Notebook 1               |New notebook|Active|2025-02-12 13:44:36.731107|1a95e3bb-a11d-45bc-84d4-5c2c1f1aa990|c73a5223-9ef6-4514-83cc-3e70297ee377|d87d15b5-62cd-41e0-87fd-92329908fadb|2025-07-16 15:52:47.505373|\n|1a9ab837-e868-4d04-ae3d-8e8ca58b3f77|Lakehouse   |DataflowsStagingLakehouse|NULL        |Active|2024-09-12 13:53:14.804064|1a95e3bb-a11d-45bc-84d4-5c2c1f1aa990|c73a5223-9ef6-4514-83cc-3e70297ee377|f12096e4-4ee8-4adc-8f64-1b25cc463fc4|2025-07-16 15:52:47.505373|\n|5841c510-0314-414f-908f-4ca8ea49daa7|Notebook    |Pull Airport Data        |New notebook|Active|2024-11-07 20:56:03.510383|1a95e3bb-a11d-45bc-84d4-5c2c1f1aa990|c73a5223-9ef6-4514-83cc-3e70297ee377|f12096e4-4ee8-4adc-8f64-1b25cc463fc4|2025-07-16 15:52:47.505373|\n+------------------------------------+------------+-------------------------+------------+------+--------------------------+------------------------------------+------------------------------------+------------------------------------+--------------------------+\nonly showing top 5 rows\n\n+------------------------------------+-----------------+--------------+------------------------+----------+----------+-------------------+-------------------------+\n|principal_id                        |display_name     |principal_type|user_principal_name     |aad_app_id|group_type|parent_principal_id|extraction_timestamp     |\n+------------------------------------+-----------------+--------------+------------------------+----------+----------+-------------------+-------------------------+\n|d87d15b5-62cd-41e0-87fd-92329908fadb|Rey Allen Gatilao|User          |rgatilao@mdanderson.org |NULL      |NULL      |NULL               |2025-07-16 15:52:47.94578|\n|f12096e4-4ee8-4adc-8f64-1b25cc463fc4|Brent Hand       |User          |bwhand@mdanderson.org   |NULL      |NULL      |NULL               |2025-07-16 15:52:47.94578|\n|0b536f4a-3e41-4411-8c0a-806c1935121e|Morgan Jolley    |User          |MMJolley@mdanderson.org |NULL      |NULL      |NULL               |2025-07-16 15:52:47.94578|\n|12e387e0-e8cd-493f-84ce-73669e6e00d3|Omer Piperdi     |User          |OAPiperdi@mdanderson.org|NULL      |NULL      |NULL               |2025-07-16 15:52:47.94578|\n|f41d2a5d-5892-41b2-8c1f-30e10f603f22|Yogesh Keshetty  |User          |YKeshetty@mdanderson.org|NULL      |NULL      |NULL               |2025-07-16 15:52:47.94578|\n+------------------------------------+-----------------+--------------+------------------------+----------+----------+-------------------+-------------------------+\nonly showing top 5 rows\n\nroot\n |-- principal_id: string (nullable = false)\n |-- display_name: string (nullable = true)\n |-- principal_type: string (nullable = true)\n |-- user_principal_name: string (nullable = true)\n |-- aad_app_id: string (nullable = true)\n |-- group_type: string (nullable = true)\n |-- parent_principal_id: string (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = false)\n\nroot\n |-- principal_id: string (nullable = true)\n |-- display_name: string (nullable = true)\n |-- principal_type: string (nullable = true)\n |-- user_principal_name: string (nullable = true)\n |-- aad_app_id: string (nullable = true)\n |-- group_type: string (nullable = true)\n |-- parent_principal_id: string (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = true)\n\nroot\n |-- id: string (nullable = false)\n |-- type: string (nullable = true)\n |-- name: string (nullable = true)\n |-- description: string (nullable = true)\n |-- state: string (nullable = true)\n |-- last_updated_date: timestamp (nullable = true)\n |-- workspace_id: string (nullable = true)\n |-- capacity_id: string (nullable = true)\n |-- creator_principal_id: string (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = false)\n\nroot\n |-- id: string (nullable = true)\n |-- type: string (nullable = true)\n |-- name: string (nullable = true)\n |-- description: string (nullable = true)\n |-- state: string (nullable = true)\n |-- last_updated_date: timestamp (nullable = true)\n |-- workspace_id: string (nullable = true)\n |-- capacity_id: string (nullable = true)\n |-- creator_principal_id: string (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = true)\n\n+------+------------------------------------+-----------------------------------------------------------------------------------------------------------+-----------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+-----------------------------------------------------------------------------------------------------------------+----------------+----------------+------------------------+\n|format|id                                  |name                                                                                                       |description|location                                                                                                                                  |createdAt              |lastModified           |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                                                                                                       |minReaderVersion|minWriterVersion|tableFeatures           |\n+------+------------------------------------+-----------------------------------------------------------------------------------------------------------+-----------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+-----------------------------------------------------------------------------------------------------------------+----------------+----------------+------------------------+\n|delta |611f44a6-6780-44f8-a494-f6da22accfff|spark_catalog.chimcobldhq2ahj1c9p6iop085i6qqbe4126ath58pgm4sj9cd0m8rb9dpfkoobbclk6utbjckim8ojf.fabric_items|NULL       |abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Tables/dbo/fabric_items|2025-05-28 18:08:07.067|2025-07-16 15:53:22.226|[]              |[]               |2       |1784098    |{delta.stats.extended.collect -> true, delta.stats.extended.inject -> true, delta.parquet.vorder.enabled -> true}|1               |2               |[appendOnly, invariants]|\n+------+------------------------------------+-----------------------------------------------------------------------------------------------------------+-----------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+-----------------------------------------------------------------------------------------------------------------+----------------+----------------+------------------------+\n\n+------+------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+-----------------------------------------------------------------------------------------------------------------+----------------+----------------+------------------------+\n|format|id                                  |name                                                                                                                |description|location                                                                                                                                           |createdAt              |lastModified           |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                                                                                                       |minReaderVersion|minWriterVersion|tableFeatures           |\n+------+------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+-----------------------------------------------------------------------------------------------------------------+----------------+----------------+------------------------+\n|delta |20b5fec3-918f-4b2c-8df2-c7ff28a0de3b|spark_catalog.chimcobldhq2ahj1c9p6iop085i6qqbe4126ath58pgm4sj9cd0m8rb9dpfkoobbclk6utbjckim8ojf.fabric_items_creators|NULL       |abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Tables/dbo/fabric_items_creators|2025-05-28 18:19:04.497|2025-07-16 15:53:10.781|[]              |[]               |2       |411923     |{delta.stats.extended.collect -> true, delta.stats.extended.inject -> true, delta.parquet.vorder.enabled -> true}|1               |2               |[appendOnly, invariants]|\n+------+------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+-----------------------------------------------------------------------------------------------------------------+----------------+----------------+------------------------+\n\n+-----------+-----------------+-----------------+-----------------+-------------+---------------+--------------------------+\n|total_items|unique_workspaces|unique_capacities|unique_item_types|unique_states|unique_creators|last_updated              |\n+-----------+-----------------+-----------------+-----------------+-------------+---------------+--------------------------+\n|23775      |6667             |18               |19               |1            |5804           |2025-07-16 15:53:17.308102|\n+-----------+-----------------+-----------------+-----------------+-------------+---------------+--------------------------+\n\n+------------------------------------+----------+------------+\n|workspace_id                        |item_count|unique_types|\n+------------------------------------+----------+------------+\n|21c0515f-4846-4031-8f7f-fa710636f7ec|135       |4           |\n|836ff1eb-5c2e-4da0-b8a6-d94f0cbdc033|132       |4           |\n|d4a6c88e-8544-49a3-bc4b-cbbb66379ded|109       |3           |\n|9c5ab23c-9ac4-4bb1-bc96-fb1dfb34112a|92        |4           |\n|fb56589c-1f81-4f37-9eed-bec2759ffd67|90        |2           |\n|187daf3a-a500-4476-8c09-221d676b08fa|90        |3           |\n|122d675e-1ec9-497d-9709-7084a5bcf3ae|85        |2           |\n|b7f50f80-9e4d-459b-9a71-8ced241b0f49|85        |4           |\n|3054fcd6-a5b5-4549-be30-4e28eb009059|82        |3           |\n|fedbe62f-5ef1-479f-85e0-56e77d211973|82        |3           |\n+------------------------------------+----------+------------+\n\n+-------------------------------+-------------+------+------------------------------------+---------------------------------------------+------------+---------------------------------------------+--------------------------+\n|item_name                      |item_type    |state |workspace_id                        |creator_name                                 |creator_type|user_principal_name                          |last_updated_date         |\n+-------------------------------+-------------+------+------------------------------------+---------------------------------------------+------------+---------------------------------------------+--------------------------+\n|INCSTMT_DF                     |Dataflow     |Active|146d2ddc-24db-45ad-8b41-9eb623ca5b91|Spiwe Mazunga                                |User        |SMazunga@mdanderson.org                      |2025-07-16 15:52:26.563   |\n|API_Load_fabric_items          |Notebook     |Active|7a21dc44-c8b8-446e-9e80-59458a88ece8|Brent Hand                                   |User        |bwhand@mdanderson.org                        |2025-07-16 15:52:16.402931|\n|Service Excellence - Statistics|SemanticModel|Active|e2f198e8-0263-4588-8734-003c3607e639|b7d990e3-a6ca-4281-b9b1-e3878c5693db OneDrive|User        |OneDrive-b7d990e3-a6ca-4281-b9b1-e3878c5693db|2025-07-16 15:52:09.277   |\n|Service Excellence - Statistics|Report       |Active|e2f198e8-0263-4588-8734-003c3607e639|b7d990e3-a6ca-4281-b9b1-e3878c5693db OneDrive|User        |OneDrive-b7d990e3-a6ca-4281-b9b1-e3878c5693db|2025-07-16 15:52:07.933   |\n|INC_LH                         |Lakehouse    |Active|146d2ddc-24db-45ad-8b41-9eb623ca5b91|Spiwe Mazunga                                |User        |SMazunga@mdanderson.org                      |2025-07-16 15:47:23.433025|\n|INC_LH                         |SQLEndpoint  |Active|146d2ddc-24db-45ad-8b41-9eb623ca5b91|Spiwe Mazunga                                |User        |SMazunga@mdanderson.org                      |2025-07-16 15:47:22.511142|\n|INC_LH                         |SemanticModel|Active|146d2ddc-24db-45ad-8b41-9eb623ca5b91|Spiwe Mazunga                                |User        |SMazunga@mdanderson.org                      |2025-07-16 15:47:13.183   |\n|DL Program Status Report       |SemanticModel|Active|a6fbc44b-f4c6-44b7-8240-f4076f749566|Lashelle Inman                               |User        |LMInman@mdanderson.org                       |2025-07-16 15:47:00.96    |\n|API_Load_fabric_workspaces     |Notebook     |Active|7a21dc44-c8b8-446e-9e80-59458a88ece8|Brent Hand                                   |User        |bwhand@mdanderson.org                        |2025-07-16 15:46:19.010824|\n|BA_SILVER_LH                   |SQLEndpoint  |Active|146d2ddc-24db-45ad-8b41-9eb623ca5b91|Omer Piperdi                                 |User        |OAPiperdi@mdanderson.org                     |2025-07-16 15:45:15.494949|\n+-------------------------------+-------------+------+------------------------------------+---------------------------------------------+------------+---------------------------------------------+--------------------------+\n\n\nDataFrames are available as:\n  - items_df: Main items DataFrame\n  - creators_df: Creators DataFrame\n\nYou can now perform additional analysis or create custom queries!\nAdditional analysis examples are available in Cell 12\nUncomment the code in Cell 12 to run example analytics queries\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7d9aa36e-c1a4-40a2-82df-3eb47b970f93"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE fabric_items\")\n","print(\"Metadata refresh triggered successfully.\")\n","\n","spark.sql(\"REFRESH TABLE fabric_items_creators\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"f516eaea-ae7d-4600-9404-46789e43c376","normalized_state":"finished","queued_time":"2025-07-16T15:55:08.6210111Z","session_start_time":null,"execution_start_time":"2025-07-16T15:55:08.6220533Z","execution_finish_time":"2025-07-16T15:55:10.1849051Z","parent_msg_id":"ccd75053-9cba-4c35-a59f-b8f50b71920a"},"text/plain":"StatementMeta(, f516eaea-ae7d-4600-9404-46789e43c376, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\nMetadata refresh triggered successfully.\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"547f09b4-8e52-4783-af9f-c40b0d92751b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}