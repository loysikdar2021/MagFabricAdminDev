{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: \n","# Command:  \n","# Doc:      \n","\n","# Loads table: api_discovery\n","# Loads table: api_documentation"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"e2c2929e-68d0-4eb9-bb28-93801e523339","normalized_state":"finished","queued_time":"2025-06-11T20:54:45.0423636Z","session_start_time":"2025-06-11T20:54:45.0433123Z","execution_start_time":"2025-06-11T20:54:59.7579858Z","execution_finish_time":"2025-06-11T20:55:00.2044085Z","parent_msg_id":"cefab143-346d-4937-be02-aef0fc8e9033"},"text/plain":"StatementMeta(, e2c2929e-68d0-4eb9-bb28-93801e523339, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6614f9b8-d55d-40ae-a202-32139bc84c8f"},{"cell_type":"code","source":["# =============================================================================\n","# MICROSOFT FABRIC REST API DOCUMENTATION SCRAPER - CLEAN VERSION\n","# =============================================================================\n","# This notebook scrapes Microsoft Fabric REST API documentation and stores\n","# the results in Delta tables in your Microsoft Fabric Lakehouse.\n","#\n","# What this does:\n","# 1. Discovers API endpoints from overview pages\n","# 2. Scrapes detailed documentation for each endpoint\n","# 3. Stores everything in structured Delta tables for analysis\n","# =============================================================================\n","\n","# =============================================================================\n","# BLOCK 1: IMPORTS AND SETUP\n","# =============================================================================\n","import requests\n","import time\n","import json\n","import re\n","import logging\n","import pytz\n","from datetime import datetime\n","from typing import List, Dict, Optional, Tuple\n","from urllib.parse import urljoin, urlparse\n","from bs4 import BeautifulSoup\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Get local timezone (Houston, Texas - Central Time)\n","LOCAL_TIMEZONE = pytz.timezone('America/Chicago')\n","\n","def get_local_timestamp():\n","    \"\"\"Get current timestamp in local timezone.\"\"\"\n","    utc_now = datetime.utcnow().replace(tzinfo=pytz.UTC)\n","    local_time = utc_now.astimezone(LOCAL_TIMEZONE)\n","    return local_time.replace(tzinfo=None)\n","\n","print(\"‚úÖ All libraries imported successfully!\")\n","\n","# =============================================================================\n","# BLOCK 2: CONFIGURATION - SUPPLY YOUR OVERVIEW URLS HERE\n","# =============================================================================\n","# üéØ THIS IS WHERE YOU SUPPLY YOUR OVERVIEW URLS!\n","\n","OVERVIEW_URLS = [\n","    # Admin APIs\n","    \"https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains\",\n","    \"https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider\",\n","\n","]\n","\n","print(f\"üìã Configured {len(OVERVIEW_URLS)} overview URLs to process\")\n","for i, url in enumerate(OVERVIEW_URLS, 1):\n","    print(f\"   {i}. {url}\")\n","\n","# =============================================================================\n","# BLOCK 3: HELPER FUNCTIONS\n","# =============================================================================\n","\n","def make_safe_request(session, url: str, max_retries: int = 3) -> Optional[requests.Response]:\n","    \"\"\"Safely make an HTTP request with retry logic.\"\"\"\n","    for attempt in range(max_retries):\n","        try:\n","            print(f\"   üì° Fetching: {url} (attempt {attempt + 1})\")\n","            response = session.get(url, timeout=30)\n","            \n","            if response.status_code == 200:\n","                print(f\"   ‚úÖ Success!\")\n","                return response\n","            elif response.status_code == 429:\n","                wait_time = 2 ** attempt * 3\n","                print(f\"   ‚è≥ Rate limited. Waiting {wait_time} seconds...\")\n","                time.sleep(wait_time)\n","            else:\n","                print(f\"   ‚ùå HTTP {response.status_code}\")\n","                \n","        except Exception as e:\n","            print(f\"   ‚ùå Error: {e}\")\n","            if attempt < max_retries - 1:\n","                time.sleep(2 ** attempt)\n","    \n","    return None\n","\n","def clean_description_text(description: str) -> str:\n","    \"\"\"Clean description text by removing unwanted content.\"\"\"\n","    if not description:\n","        return \"\"\n","    \n","    # Remove common preview notices and note prefixes\n","    unwanted_patterns = [\n","        r'Note\\s*This API is in preview\\.?',\n","        r'NoteThis API is in preview\\.?',\n","        r'Note\\s*',\n","        r'This API is in preview\\.?',\n","        r'Preview\\s*',\n","        r'^\\s*Note\\s*',\n","    ]\n","    \n","    cleaned = description\n","    for pattern in unwanted_patterns:\n","        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)\n","    \n","    # Clean up extra whitespace\n","    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n","    \n","    # If the description is now empty or too short, return empty\n","    if len(cleaned) < 10:\n","        return \"\"\n","    \n","    return cleaned\n","\n","def extract_api_operations_from_page(html_content: str, overview_url: str) -> List[Dict]:\n","    \"\"\"Extract individual API operations from an overview page.\"\"\"\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","    operations = []\n","    \n","    # Look for the operations table\n","    tables = soup.find_all('table')\n","    \n","    for table in tables:\n","        rows = table.find_all('tr')\n","        \n","        for row in rows:\n","            cells = row.find_all('td')\n","            if len(cells) >= 2:\n","                \n","                # First cell should contain the operation link\n","                link_cell = cells[0]\n","                operation_link = link_cell.find('a', href=lambda x: x and not x.startswith('http'))\n","                \n","                if not operation_link:\n","                    continue\n","                \n","                href = operation_link.get('href', '')\n","                operation_name = operation_link.get_text(strip=True)\n","                \n","                # Filter for actual API operation links\n","                if not (href and '/' in href and not href.startswith('#') and \n","                       not 'articles' in href and not 'quickstart' in href):\n","                    continue\n","                \n","                if not operation_name:\n","                    continue\n","                \n","                # Second cell contains description - extract from <p> tag, ignore NOTE divs\n","                desc_cell = cells[1]\n","                \n","                # Remove all NOTE divs from the cell before extracting description\n","                for note_div in desc_cell.find_all('div', class_='NOTE'):\n","                    note_div.decompose()\n","                \n","                # Now find the <p> tag with the actual description\n","                description = \"\"\n","                p_tag = desc_cell.find('p')\n","                if p_tag:\n","                    description = clean_description_text(p_tag.get_text(strip=True))\n","                \n","                # Build the full URL for this operation\n","                operation_url = urljoin(overview_url, href)\n","                \n","                # Extract category and service from the URL\n","                url_parts = overview_url.split('/')\n","                category = url_parts[-2] if len(url_parts) >= 2 else \"unknown\"\n","                service = url_parts[-1] if len(url_parts) >= 1 else \"unknown\"\n","                \n","                operation = {\n","                    'category': category,\n","                    'overview_url': overview_url,\n","                    'service': service,\n","                    'operation_name': operation_name,\n","                    'operation_desc': description,\n","                    'operation_url': operation_url,\n","                    'discovered_timestamp': get_local_timestamp()\n","                }\n","                operations.append(operation)\n","    \n","    # Fallback: if no table found, try alternative method\n","    if not operations:\n","        print(\"   ‚ö†Ô∏è No table found, trying alternative extraction method...\")\n","        operations = extract_operations_fallback_method(soup, overview_url)\n","    \n","    return operations\n","\n","def extract_operations_fallback_method(soup: BeautifulSoup, overview_url: str) -> List[Dict]:\n","    \"\"\"Fallback method for pages that don't use the standard table format.\"\"\"\n","    operations = []\n","    \n","    # Find all links that point to API operations\n","    operation_links = soup.find_all('a', href=lambda x: x and not x.startswith('http'))\n","    \n","    for link in operation_links:\n","        href = link.get('href', '')\n","        \n","        # Filter for actual API operation links\n","        if (href and '/' in href and not href.startswith('#') and \n","            not 'articles' in href and not 'quickstart' in href):\n","            \n","            operation_name = link.get_text(strip=True)\n","            if not operation_name:\n","                continue\n","            \n","            # Try to find description in nearby content\n","            description = \"\"\n","            \n","            # Look for description in parent row or container\n","            parent_row = link.find_parent('tr')\n","            if parent_row:\n","                cells = parent_row.find_all('td')\n","                for cell in cells:\n","                    # Remove NOTE divs\n","                    for note_div in cell.find_all('div', class_='NOTE'):\n","                        note_div.decompose()\n","                    \n","                    # Find <p> tag\n","                    p_tag = cell.find('p')\n","                    if p_tag:\n","                        desc_text = p_tag.get_text(strip=True)\n","                        if desc_text and len(desc_text) > 10:\n","                            description = clean_description_text(desc_text)\n","                            break\n","            \n","            # Build the full URL for this operation\n","            operation_url = urljoin(overview_url, href)\n","            \n","            # Extract category and service from the URL\n","            url_parts = overview_url.split('/')\n","            category = url_parts[-2] if len(url_parts) >= 2 else \"unknown\"\n","            service = url_parts[-1] if len(url_parts) >= 1 else \"unknown\"\n","            \n","            operation = {\n","                'category': category,\n","                'overview_url': overview_url,\n","                'service': service,\n","                'operation_name': operation_name,\n","                'operation_desc': description,\n","                'operation_url': operation_url,\n","                'discovered_timestamp': get_local_timestamp()\n","            }\n","            operations.append(operation)\n","    \n","    return operations\n","\n","print(\"‚úÖ Helper functions defined!\")\n","\n","# =============================================================================\n","# BLOCK 4: DELTA TABLE SETUP\n","# =============================================================================\n","\n","def initialize_delta_tables():\n","    \"\"\"Create the Delta tables if they don't exist.\"\"\"\n","    print(\"üóÉÔ∏è Setting up Delta tables...\")\n","    \n","    # Schema for the API discovery table\n","    discovery_schema = StructType([\n","        StructField(\"category\", StringType(), True),\n","        StructField(\"overview_url\", StringType(), True),\n","        StructField(\"service\", StringType(), True),\n","        StructField(\"operation_name\", StringType(), True),\n","        StructField(\"operation_desc\", StringType(), True),\n","        StructField(\"operation_url\", StringType(), True),\n","        StructField(\"discovered_timestamp\", TimestampType(), True)\n","    ])\n","    \n","    # Schema for parameter information\n","    param_schema = StructType([\n","        StructField(\"name\", StringType(), True),\n","        StructField(\"type\", StringType(), True),\n","        StructField(\"required\", BooleanType(), True),\n","        StructField(\"description\", StringType(), True)\n","    ])\n","    \n","    # Schema for the API documentation table (with discovered_timestamp added)\n","    documentation_schema = StructType([\n","        StructField(\"source_url\", StringType(), True),\n","        StructField(\"api_category\", StringType(), True),\n","        StructField(\"operation_name\", StringType(), True),\n","        StructField(\"description\", StringType(), True),\n","        StructField(\"http_method\", StringType(), True),\n","        StructField(\"endpoint_url\", StringType(), True),\n","        StructField(\"parameters\", ArrayType(param_schema), True),\n","        StructField(\"discovered_timestamp\", TimestampType(), True)  # Added this column\n","    ])\n","    \n","    # Check if api_documentation table exists and recreate if needed\n","    try:\n","        # Try to query the table to see if it exists\n","        existing_count = spark.sql(\"SELECT COUNT(*) as count FROM api_documentation\").collect()[0]['count']\n","        print(f\"üìä api_documentation table exists with {existing_count} records\")\n","        \n","        # Check if the table has the correct schema by trying to select the new column\n","        try:\n","            spark.sql(\"SELECT discovered_timestamp FROM api_documentation LIMIT 1\").collect()\n","            print(\"‚úÖ Table has correct schema with discovered_timestamp column\")\n","            table_needs_recreation = False\n","        except:\n","            print(\"‚ö†Ô∏è Table exists but missing discovered_timestamp column - will recreate\")\n","            table_needs_recreation = True\n","            \n","    except:\n","        print(\"üìä api_documentation table doesn't exist - will create new one\")\n","        table_needs_recreation = True\n","    \n","    # Recreate api_documentation table if needed\n","    if table_needs_recreation:\n","        print(\"üóëÔ∏è Dropping existing api_documentation table (if exists)...\")\n","        spark.sql(\"DROP TABLE IF EXISTS api_documentation\")\n","        \n","        print(\"üèóÔ∏è Creating new api_documentation table with discovered_timestamp column...\")\n","        empty_documentation = spark.createDataFrame([], documentation_schema)\n","        empty_documentation.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"api_documentation\")\n","        print(\"‚úÖ New api_documentation table created!\")\n","    \n","    # Create discovery table (only if it doesn't exist)\n","    empty_discovery = spark.createDataFrame([], discovery_schema)\n","    empty_discovery.write.format(\"delta\").mode(\"ignore\").saveAsTable(\"api_discovery\")\n","    \n","    print(\"‚úÖ Delta tables ready!\")\n","    \n","    # Show the current schema\n","    print(\"\\nüìã api_documentation table schema:\")\n","    spark.sql(\"DESCRIBE api_documentation\").show()\n","\n","# Run the table setup\n","initialize_delta_tables()\n","\n","# =============================================================================\n","# BLOCK 5: PHASE 1 - API DISCOVERY\n","# =============================================================================\n","\n","def discover_all_api_endpoints(overview_urls: List[str]) -> List[Dict]:\n","    \"\"\"Phase 1: Discover all API endpoints from overview pages.\"\"\"\n","    print(\"\\nüîç PHASE 1: DISCOVERING API ENDPOINTS\")\n","    print(\"=\"*50)\n","    \n","    # Set up a web session for making requests\n","    session = requests.Session()\n","    session.headers.update({\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n","        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n","    })\n","    \n","    all_discovered_endpoints = []\n","    \n","    for i, overview_url in enumerate(overview_urls, 1):\n","        print(f\"\\nüìã Processing overview page {i}/{len(overview_urls)}\")\n","        print(f\"   URL: {overview_url}\")\n","        \n","        # Fetch the overview page\n","        response = make_safe_request(session, overview_url)\n","        if not response:\n","            print(f\"   ‚ùå Failed to fetch overview page\")\n","            continue\n","        \n","        try:\n","            # Extract API operations from this page\n","            endpoints = extract_api_operations_from_page(response.text, overview_url)\n","            all_discovered_endpoints.extend(endpoints)\n","            print(f\"   ‚úÖ Found {len(endpoints)} API operations\")\n","            \n","            # Show what we found\n","            for endpoint in endpoints[:3]:\n","                print(f\"      - {endpoint['operation_name']}\")\n","            if len(endpoints) > 3:\n","                print(f\"      - ... and {len(endpoints) - 3} more\")\n","                \n","        except Exception as e:\n","            print(f\"   ‚ùå Error processing page: {e}\")\n","        \n","        # Be polite - wait between requests\n","        time.sleep(1.5)\n","    \n","    print(f\"\\nüéâ Discovery complete! Found {len(all_discovered_endpoints)} total API endpoints\")\n","    return all_discovered_endpoints\n","\n","# Run the discovery\n","discovered_endpoints = discover_all_api_endpoints(OVERVIEW_URLS)\n","\n","# =============================================================================\n","# BLOCK 6: SAVE DISCOVERED ENDPOINTS\n","# =============================================================================\n","\n","def save_discovered_endpoints_to_table(endpoints: List[Dict]):\n","    \"\"\"Save discovered endpoints to the api_discovery Delta table.\"\"\"\n","    if not endpoints:\n","        print(\"‚ùå No endpoints to save\")\n","        return\n","    \n","    print(f\"\\nüíæ Saving {len(endpoints)} discovered endpoints to Delta table...\")\n","    \n","    # Clean the data before saving\n","    cleaned_endpoints = []\n","    for endpoint in endpoints:\n","        cleaned_endpoint = endpoint.copy()\n","        if 'operation_desc' in cleaned_endpoint:\n","            cleaned_endpoint['operation_desc'] = clean_description_text(cleaned_endpoint['operation_desc'])\n","        cleaned_endpoints.append(cleaned_endpoint)\n","    \n","    # Convert to DataFrame and save\n","    endpoints_df = spark.createDataFrame(cleaned_endpoints)\n","    endpoints_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"api_discovery\")\n","    \n","    print(\"‚úÖ Endpoints saved to 'api_discovery' table\")\n","    \n","    # Show a preview\n","    print(\"\\nüìä Sample of discovered endpoints (with cleaned descriptions):\")\n","    spark.sql(\"\"\"\n","        SELECT category, service, operation_name, \n","               CASE \n","                   WHEN LENGTH(operation_desc) > 50 \n","                   THEN CONCAT(SUBSTRING(operation_desc, 1, 50), '...') \n","                   ELSE operation_desc \n","               END as description_preview\n","        FROM api_discovery \n","        WHERE operation_desc IS NOT NULL AND operation_desc != ''\n","        ORDER BY category, service, operation_name \n","        LIMIT 10\n","    \"\"\").show(truncate=False)\n","\n","# Save the discovered endpoints\n","save_discovered_endpoints_to_table(discovered_endpoints)\n","\n","# =============================================================================\n","# BLOCK 7: DETAILED CONTENT EXTRACTION FUNCTIONS\n","# =============================================================================\n","\n","def extract_description_from_api_page(soup: BeautifulSoup) -> str:\n","    \"\"\"Extract the main description of what this API does.\"\"\"\n","    description_selectors = [\n","        'p:first-of-type',\n","        '.description',\n","        'div[class*=\"summary\"] p'\n","    ]\n","    \n","    for selector in description_selectors:\n","        elements = soup.select(selector)\n","        for elem in elements:\n","            text = elem.get_text(strip=True)\n","            if text and len(text) > 20:\n","                return text[:1000]\n","    return \"\"\n","\n","def extract_http_method_and_endpoint(soup: BeautifulSoup) -> Tuple[str, str]:\n","    \"\"\"Extract the HTTP method and API endpoint URL.\"\"\"\n","    http_method = \"\"\n","    endpoint_url = \"\"\n","    \n","    # Look in code blocks for HTTP method patterns\n","    code_blocks = soup.find_all(['code', 'pre'])\n","    \n","    for block in code_blocks:\n","        text = block.get_text(strip=True)\n","        \n","        # Look for HTTP method\n","        method_match = re.search(r'\\b(GET|POST|PUT|DELETE|PATCH)\\s+', text)\n","        if method_match:\n","            http_method = method_match.group(1)\n","            \n","            # Look for the API URL in the same block\n","            url_match = re.search(r'https://api\\.fabric\\.microsoft\\.com[^\\s\\n]+', text)\n","            if url_match:\n","                endpoint_url = url_match.group(0)\n","                break\n","    \n","    return http_method, endpoint_url\n","\n","def extract_parameters_from_tables(soup: BeautifulSoup) -> List[Dict]:\n","    \"\"\"Extract parameter information from documentation tables.\"\"\"\n","    parameters = []\n","    tables = soup.find_all('table')\n","    \n","    for table in tables:\n","        headers = table.find_all('th')\n","        if not headers:\n","            continue\n","        \n","        header_texts = [h.get_text(strip=True).lower() for h in headers]\n","        \n","        # Check if this looks like a parameters table\n","        if 'name' in header_texts and ('type' in header_texts or 'description' in header_texts):\n","            rows = table.find_all('tr')[1:]  # Skip header row\n","            \n","            for row in rows:\n","                cells = row.find_all(['td', 'th'])\n","                if len(cells) >= 2:\n","                    param_name = cells[0].get_text(strip=True) if len(cells) > 0 else \"\"\n","                    param_type = cells[1].get_text(strip=True) if len(cells) > 1 else \"\"\n","                    \n","                    # Determine if required\n","                    required_text = row.get_text().lower()\n","                    is_required = ('required' in required_text or 'true' in required_text)\n","                    \n","                    param_desc = cells[-1].get_text(strip=True)[:500] if len(cells) > 2 else \"\"\n","                    \n","                    if param_name:\n","                        parameters.append({\n","                            'name': param_name,\n","                            'type': param_type,\n","                            'required': is_required,\n","                            'description': param_desc\n","                        })\n","    \n","    return parameters\n","\n","print(\"‚úÖ Content extraction functions ready!\")\n","\n","# =============================================================================\n","# BLOCK 8: PHASE 2 - DETAILED API DOCUMENTATION SCRAPING\n","# =============================================================================\n","\n","def scrape_single_api_documentation(url: str, session: requests.Session) -> Optional[Dict]:\n","    \"\"\"Scrape essential documentation from a single API page.\"\"\"\n","    response = make_safe_request(session, url)\n","    if not response:\n","        return None\n","    \n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    \n","    # Extract essential information only\n","    doc_data = {\n","        'source_url': url\n","    }\n","    \n","    # Get the operation name from URL\n","    url_parts = url.split('/')\n","    doc_data['operation_name'] = url_parts[-1].replace('-', ' ').title()\n","    doc_data['api_category'] = url_parts[-3] if len(url_parts) >= 3 else \"\"\n","    \n","    # Extract essential information\n","    doc_data['description'] = extract_description_from_api_page(soup)\n","    \n","    http_method, endpoint_url = extract_http_method_and_endpoint(soup)\n","    doc_data['http_method'] = http_method\n","    doc_data['endpoint_url'] = endpoint_url\n","    \n","    # Extract parameters\n","    doc_data['parameters'] = extract_parameters_from_tables(soup)\n","    \n","    return doc_data\n","\n","def clean_document_data(doc_data: Dict) -> Dict:\n","    \"\"\"Clean document data to ensure schema compliance.\"\"\"\n","    cleaned = {\n","        'source_url': str(doc_data.get('source_url', '')),\n","        'api_category': str(doc_data.get('api_category', '')),\n","        'operation_name': str(doc_data.get('operation_name', '')),\n","        'description': str(doc_data.get('description', '')),\n","        'http_method': str(doc_data.get('http_method', '')),\n","        'endpoint_url': str(doc_data.get('endpoint_url', '')),\n","        'discovered_timestamp': get_local_timestamp()  # Add current local timestamp\n","    }\n","    \n","    # Handle parameters carefully\n","    params = doc_data.get('parameters', [])\n","    if not isinstance(params, list):\n","        params = []\n","    \n","    clean_params = []\n","    for param in params:\n","        if isinstance(param, dict):\n","            clean_param = {\n","                'name': str(param.get('name', '')),\n","                'type': str(param.get('type', '')),\n","                'required': bool(param.get('required', False)),\n","                'description': str(param.get('description', ''))\n","            }\n","            clean_params.append(clean_param)\n","    \n","    cleaned['parameters'] = clean_params\n","    return cleaned\n","\n","def save_clean_batch(batch_data: List[Dict]) -> bool:\n","    \"\"\"Save batch with the updated schema including discovered_timestamp.\"\"\"\n","    try:\n","        # Define schema with discovered_timestamp\n","        param_schema = StructType([\n","            StructField(\"name\", StringType(), True),\n","            StructField(\"type\", StringType(), True),\n","            StructField(\"required\", BooleanType(), True),\n","            StructField(\"description\", StringType(), True)\n","        ])\n","        \n","        documentation_schema = StructType([\n","            StructField(\"source_url\", StringType(), True),\n","            StructField(\"api_category\", StringType(), True),\n","            StructField(\"operation_name\", StringType(), True),\n","            StructField(\"description\", StringType(), True),\n","            StructField(\"http_method\", StringType(), True),\n","            StructField(\"endpoint_url\", StringType(), True),\n","            StructField(\"parameters\", ArrayType(param_schema), True),\n","            StructField(\"discovered_timestamp\", TimestampType(), True)  # Added column\n","        ])\n","        \n","        # Create DataFrame with explicit schema\n","        df = spark.createDataFrame(batch_data, schema=documentation_schema)\n","        \n","        # Save to table\n","        df.write.format(\"delta\").mode(\"append\").saveAsTable(\"api_documentation\")\n","        \n","        return True\n","        \n","    except Exception as e:\n","        print(f\"‚ùå Save error: {e}\")\n","        return False\n","\n","def scrape_all_discovered_apis(batch_size: int = 5) -> int:\n","    \"\"\"Phase 2: Scrape detailed documentation for all discovered APIs.\"\"\"\n","    print(\"\\nüìö PHASE 2: SCRAPING DETAILED API DOCUMENTATION\")\n","    print(\"=\"*50)\n","    \n","    # Get URLs that we haven't scraped yet\n","    unprocessed_df = spark.sql(\"\"\"\n","        SELECT DISTINCT d.operation_url, d.category, d.service, d.operation_name\n","        FROM api_discovery d\n","        LEFT JOIN api_documentation doc ON d.operation_url = doc.source_url\n","        WHERE doc.source_url IS NULL\n","        ORDER BY d.category, d.service, d.operation_name\n","    \"\"\")\n","    \n","    unprocessed_urls = unprocessed_df.collect()\n","    total_urls = len(unprocessed_urls)\n","    \n","    if total_urls == 0:\n","        print(\"‚úÖ All discovered APIs have already been scraped!\")\n","        return 0\n","    \n","    print(f\"üìã Found {total_urls} APIs to scrape\")\n","    \n","    # Set up web session\n","    session = requests.Session()\n","    session.headers.update({\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n","    })\n","    \n","    scraped_count = 0\n","    batch_data = []\n","    \n","    for i, row in enumerate(unprocessed_urls):\n","        url = row['operation_url']\n","        operation_name = row['operation_name']\n","        \n","        print(f\"\\nüìÑ Scraping {i+1}/{total_urls}: {operation_name}\")\n","        print(f\"   URL: {url}\")\n","        \n","        try:\n","            doc_data = scrape_single_api_documentation(url, session)\n","            if doc_data:\n","                cleaned_data = clean_document_data(doc_data)\n","                batch_data.append(cleaned_data)\n","                scraped_count += 1\n","                print(f\"   ‚úÖ Successfully scraped!\")\n","                \n","                # Show what we extracted\n","                if cleaned_data['http_method'] and cleaned_data['endpoint_url']:\n","                    print(f\"      Method: {cleaned_data['http_method']}\")\n","                    print(f\"      Endpoint: {cleaned_data['endpoint_url'][:80]}...\")\n","            else:\n","                print(f\"   ‚ùå Failed to scrape\")\n","            \n","            # Save batch when it's full\n","            if len(batch_data) >= batch_size:\n","                success = save_clean_batch(batch_data)\n","                if success:\n","                    print(f\"   üíæ Batch of {len(batch_data)} saved!\")\n","                batch_data = []\n","                \n","        except Exception as e:\n","            print(f\"   ‚ùå Error: {e}\")\n","        \n","        # Be polite - wait between requests\n","        time.sleep(2.0)\n","    \n","    # Save any remaining data\n","    if batch_data:\n","        success = save_clean_batch(batch_data)\n","        if success:\n","            print(f\"   üíæ Final batch of {len(batch_data)} saved!\")\n","    \n","    print(f\"\\nüéâ Scraping complete! Successfully scraped {scraped_count}/{total_urls} APIs\")\n","    return scraped_count\n","\n","# Run the detailed scraping\n","scraped_count = scrape_all_discovered_apis(batch_size=5)\n","\n","# =============================================================================\n","# BLOCK 9: RESULTS AND SUMMARY\n","# =============================================================================\n","\n","def show_scraping_results():\n","    \"\"\"Display a summary of what we scraped.\"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üéâ SCRAPING COMPLETE - RESULTS SUMMARY\")\n","    print(\"=\"*60)\n","    \n","    # Count discovered endpoints\n","    discovery_count = spark.sql(\"SELECT COUNT(*) as count FROM api_discovery\").collect()[0]['count']\n","    print(f\"üìã Total discovered API endpoints: {discovery_count}\")\n","    \n","    # Count scraped documentation\n","    doc_count = spark.sql(\"SELECT COUNT(*) as count FROM api_documentation\").collect()[0]['count']\n","    print(f\"üìö Total scraped API documents: {doc_count}\")\n","    \n","    # Calculate success rate\n","    if discovery_count > 0:\n","        success_rate = (doc_count / discovery_count) * 100\n","        print(f\"‚úÖ Success rate: {success_rate:.1f}%\")\n","    \n","    # Show breakdown by category\n","    print(\"\\nüìä APIs by category:\")\n","    spark.sql(\"\"\"\n","        SELECT api_category, COUNT(*) as count \n","        FROM api_documentation \n","        GROUP BY api_category \n","        ORDER BY count DESC\n","    \"\"\").show()\n","    \n","    # Show some examples of what we scraped (including discovered_timestamp)\n","    print(\"\\nüìÑ Sample of scraped APIs:\")\n","    spark.sql(\"\"\"\n","        SELECT operation_name, http_method, \n","               CASE \n","                   WHEN LENGTH(endpoint_url) > 50 \n","                   THEN CONCAT(SUBSTRING(endpoint_url, 1, 50), '...') \n","                   ELSE endpoint_url \n","               END as endpoint_preview,\n","               discovered_timestamp\n","        FROM api_documentation \n","        WHERE http_method IS NOT NULL AND http_method != ''\n","        ORDER BY api_category, operation_name\n","        LIMIT 10\n","    \"\"\").show(truncate=False)\n","\n","def show_useful_queries():\n","    \"\"\"Show some useful SQL queries for analyzing the scraped data.\"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üìù USEFUL QUERIES FOR YOUR SIMPLIFIED DATA\")\n","    print(\"=\"*60)\n","    \n","    print(\"\\n1Ô∏è‚É£ See all GET endpoints:\")\n","    print(\"   SELECT operation_name, endpoint_url FROM api_documentation WHERE http_method = 'GET'\")\n","    \n","    print(\"\\n2Ô∏è‚É£ Find APIs with parameters:\")\n","    print(\"   SELECT operation_name, SIZE(parameters) as param_count FROM api_documentation WHERE SIZE(parameters) > 0\")\n","    \n","    print(\"\\n3Ô∏è‚É£ See all discovered but not yet scraped APIs:\")\n","    print(\"\"\"   SELECT d.operation_name, d.operation_url \n","   FROM api_discovery d \n","   LEFT JOIN api_documentation doc ON d.operation_url = doc.source_url \n","   WHERE doc.source_url IS NULL\"\"\")\n","    \n","    print(\"\\n4Ô∏è‚É£ Count APIs by HTTP method:\")\n","    print(\"   SELECT http_method, COUNT(*) FROM api_documentation GROUP BY http_method\")\n","    \n","    print(\"\\n5Ô∏è‚É£ Find APIs with descriptions:\")\n","    print(\"   SELECT operation_name FROM api_documentation WHERE description != '' AND description IS NOT NULL\")\n","    \n","    print(\"\\n6Ô∏è‚É£ View parameter details for specific API:\")\n","    print(\"   SELECT operation_name, EXPLODE(parameters) as param FROM api_documentation WHERE operation_name LIKE '%Create%'\")\n","\n","# Show the results\n","show_scraping_results()\n","show_useful_queries()\n","\n","# =============================================================================\n","# FINAL MESSAGE\n","# =============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"üîß UPDATED SCHEMA:\")\n","print(\"‚úÖ source_url\")\n","print(\"‚úÖ api_category\") \n","print(\"‚úÖ operation_name\")\n","print(\"‚úÖ description\")\n","print(\"‚úÖ http_method\")\n","print(\"‚úÖ endpoint_url\")\n","print(\"‚úÖ parameters (array)\")\n","print(\"‚úÖ discovered_timestamp (NEW - local time when API was scraped)\")\n","print(\"\\nüöÄ ALL DONE!\")\n","print(\"=\"*60)\n","print(\"Your Microsoft Fabric REST API documentation has been scraped and stored in:\")\n","print(\"üìã Table 'api_discovery' - All discovered API endpoints\")\n","print(\"üìö Table 'api_documentation' - Essential API specifications with timestamps\")\n","print(\"\\nYou can now query these tables to analyze the APIs!\")\n","print(\"üìÖ The discovered_timestamp column shows when each API was scraped (local time)\")\n","print(\"=\"*60)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"e2c2929e-68d0-4eb9-bb28-93801e523339","normalized_state":"finished","queued_time":"2025-06-11T20:54:45.0782078Z","session_start_time":null,"execution_start_time":"2025-06-11T20:55:00.207828Z","execution_finish_time":"2025-06-11T20:56:28.3394368Z","parent_msg_id":"449d4086-b723-44e0-b0de-25ccbe0d0588"},"text/plain":"StatementMeta(, e2c2929e-68d0-4eb9-bb28-93801e523339, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ All libraries imported successfully!\nüìã Configured 2 overview URLs to process\n   1. https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains\n   2. https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider\n‚úÖ Helper functions defined!\nüóÉÔ∏è Setting up Delta tables...\nüìä api_documentation table doesn't exist - will create new one\nüóëÔ∏è Dropping existing api_documentation table (if exists)...\nüèóÔ∏è Creating new api_documentation table with discovered_timestamp column...\n‚úÖ New api_documentation table created!\n‚úÖ Delta tables ready!\n\nüìã api_documentation table schema:\n+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|          source_url|              string|   NULL|\n|        api_category|              string|   NULL|\n|      operation_name|              string|   NULL|\n|         description|              string|   NULL|\n|         http_method|              string|   NULL|\n|        endpoint_url|              string|   NULL|\n|          parameters|array<struct<name...|   NULL|\n|discovered_timestamp|           timestamp|   NULL|\n+--------------------+--------------------+-------+\n\n\nüîç PHASE 1: DISCOVERING API ENDPOINTS\n==================================================\n\nüìã Processing overview page 1/2\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Found 13 API operations\n      - Assign Domain Workspaces By Capacities\n      - Assign Domain Workspaces By Ids\n      - Assign Domain Workspaces By Principals\n      - ... and 10 more\n\nüìã Processing overview page 2/2\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Found 2 API operations\n      - List External Data Shares\n      - Revoke External Data Share\n\nüéâ Discovery complete! Found 15 total API endpoints\n\nüíæ Saving 15 discovered endpoints to Delta table...\n‚úÖ Endpoints saved to 'api_discovery' table\n\nüìä Sample of discovered endpoints (with cleaned descriptions):\n+--------+-------+--------------------------------------+-----------------------------------------------------+\n|category|service|operation_name                        |description_preview                                  |\n+--------+-------+--------------------------------------+-----------------------------------------------------+\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Capacities|Assign all workspaces that reside on the specified...|\n|admin   |domains|Assign Domain Workspaces By Ids       |Assign workspaces to the specified domain by works...|\n|admin   |domains|Assign Domain Workspaces By Ids       |Assign workspaces to the specified domain by works...|\n+--------+-------+--------------------------------------+-----------------------------------------------------+\n\n‚úÖ Content extraction functions ready!\n\nüìö PHASE 2: SCRAPING DETAILED API DOCUMENTATION\n==================================================\nüìã Found 15 APIs to scrape\n\nüìÑ Scraping 1/15: Assign Domain Workspaces By Capacities\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/assign-domain-workspaces-by-capacities\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/assign-domain-workspaces-by-capacities (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/assignWorkspacesByC...\n\nüìÑ Scraping 2/15: Assign Domain Workspaces By Ids\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/assign-domain-workspaces-by-ids\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/assign-domain-workspaces-by-ids (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/assignWorkspaces...\n\nüìÑ Scraping 3/15: Assign Domain Workspaces By Principals\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/assign-domain-workspaces-by-principals\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/assign-domain-workspaces-by-principals (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/assignWorkspacesByP...\n\nüìÑ Scraping 4/15: Create Domain\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/create-domain\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/create-domain (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains...\n\nüìÑ Scraping 5/15: Delete Domain\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/delete-domain\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/delete-domain (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: DELETE\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}...\n   üíæ Batch of 5 saved!\n\nüìÑ Scraping 6/15: Get Domain\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/get-domain\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/get-domain (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: GET\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}...\n\nüìÑ Scraping 7/15: List Domain Workspaces\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/list-domain-workspaces\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/list-domain-workspaces (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: GET\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/workspaces...\n\nüìÑ Scraping 8/15: List Domains\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/list-domains\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/list-domains (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: GET\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains...\n\nüìÑ Scraping 9/15: Role Assignments Bulk Assign\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/role-assignments-bulk-assign\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/role-assignments-bulk-assign (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/roleAssignments/bul...\n\nüìÑ Scraping 10/15: Role Assignments Bulk Unassign\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/role-assignments-bulk-unassign\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/role-assignments-bulk-unassign (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/roleAssignments/bul...\n   üíæ Batch of 5 saved!\n\nüìÑ Scraping 11/15: Unassign All Domain Workspaces\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/unassign-all-domain-workspaces\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/unassign-all-domain-workspaces (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/unassignAllWorkspac...\n\nüìÑ Scraping 12/15: Unassign Domain Workspaces By Ids\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/unassign-domain-workspaces-by-ids\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/unassign-domain-workspaces-by-ids (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/unassignWorkspaces...\n\nüìÑ Scraping 13/15: Update Domain\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/update-domain\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/domains/update-domain (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: PATCH\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/domains/{domainId}...\n\nüìÑ Scraping 14/15: List External Data Shares\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider/list-external-data-shares\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider/list-external-data-shares (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: GET\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/items/externalDataShares...\n\nüìÑ Scraping 15/15: Revoke External Data Share\n   URL: https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider/revoke-external-data-share\n   üì° Fetching: https://learn.microsoft.com/en-us/rest/api/fabric/admin/external-data-shares-provider/revoke-external-data-share (attempt 1)\n   ‚úÖ Success!\n   ‚úÖ Successfully scraped!\n      Method: POST\n      Endpoint: https://api.fabric.microsoft.com/v1/admin/workspaces/{workspaceId}/items/{itemId...\n   üíæ Batch of 5 saved!\n\nüéâ Scraping complete! Successfully scraped 15/15 APIs\n\n============================================================\nüéâ SCRAPING COMPLETE - RESULTS SUMMARY\n============================================================\nüìã Total discovered API endpoints: 120\nüìö Total scraped API documents: 15\n‚úÖ Success rate: 12.5%\n\nüìä APIs by category:\n+------------+-----+\n|api_category|count|\n+------------+-----+\n|       admin|   15|\n+------------+-----+\n\n\nüìÑ Sample of scraped APIs:\n+--------------------------------------+-----------+-----------------------------------------------------+--------------------------+\n|operation_name                        |http_method|endpoint_preview                                     |discovered_timestamp      |\n+--------------------------------------+-----------+-----------------------------------------------------+--------------------------+\n|Assign Domain Workspaces By Capacities|POST       |https://api.fabric.microsoft.com/v1/admin/domains/...|2025-06-11 15:55:39.231189|\n|Assign Domain Workspaces By Ids       |POST       |https://api.fabric.microsoft.com/v1/admin/domains/...|2025-06-11 15:55:41.591618|\n|Assign Domain Workspaces By Principals|POST       |https://api.fabric.microsoft.com/v1/admin/domains/...|2025-06-11 15:55:44.060237|\n|Create Domain                         |POST       |https://api.fabric.microsoft.com/v1/admin/domains    |2025-06-11 15:55:46.466769|\n|Delete Domain                         |DELETE     |https://api.fabric.microsoft.com/v1/admin/domains/...|2025-06-11 15:55:48.645496|\n|Get Domain                            |GET        |https://api.fabric.microsoft.com/v1/admin/domains/...|2025-06-11 15:55:53.024337|\n|List Domain Workspaces                |GET        |https://api.fabric.microsoft.com/v1/admin/domains/...|2025-06-11 15:55:55.779882|\n|List Domains                          |GET        |https://api.fabric.microsoft.com/v1/admin/domains    |2025-06-11 15:55:58.143419|\n|List External Data Shares             |GET        |https://api.fabric.microsoft.com/v1/admin/items/ex...|2025-06-11 15:56:14.083297|\n|Revoke External Data Share            |POST       |https://api.fabric.microsoft.com/v1/admin/workspac...|2025-06-11 15:56:16.498323|\n+--------------------------------------+-----------+-----------------------------------------------------+--------------------------+\n\n\n============================================================\nüìù USEFUL QUERIES FOR YOUR SIMPLIFIED DATA\n============================================================\n\n1Ô∏è‚É£ See all GET endpoints:\n   SELECT operation_name, endpoint_url FROM api_documentation WHERE http_method = 'GET'\n\n2Ô∏è‚É£ Find APIs with parameters:\n   SELECT operation_name, SIZE(parameters) as param_count FROM api_documentation WHERE SIZE(parameters) > 0\n\n3Ô∏è‚É£ See all discovered but not yet scraped APIs:\n   SELECT d.operation_name, d.operation_url \n   FROM api_discovery d \n   LEFT JOIN api_documentation doc ON d.operation_url = doc.source_url \n   WHERE doc.source_url IS NULL\n\n4Ô∏è‚É£ Count APIs by HTTP method:\n   SELECT http_method, COUNT(*) FROM api_documentation GROUP BY http_method\n\n5Ô∏è‚É£ Find APIs with descriptions:\n   SELECT operation_name FROM api_documentation WHERE description != '' AND description IS NOT NULL\n\n6Ô∏è‚É£ View parameter details for specific API:\n   SELECT operation_name, EXPLODE(parameters) as param FROM api_documentation WHERE operation_name LIKE '%Create%'\n\n============================================================\nüîß UPDATED SCHEMA:\n‚úÖ source_url\n‚úÖ api_category\n‚úÖ operation_name\n‚úÖ description\n‚úÖ http_method\n‚úÖ endpoint_url\n‚úÖ parameters (array)\n‚úÖ discovered_timestamp (NEW - local time when API was scraped)\n\nüöÄ ALL DONE!\n============================================================\nYour Microsoft Fabric REST API documentation has been scraped and stored in:\nüìã Table 'api_discovery' - All discovered API endpoints\nüìö Table 'api_documentation' - Essential API specifications with timestamps\n\nYou can now query these tables to analyze the APIs!\nüìÖ The discovered_timestamp column shows when each API was scraped (local time)\n============================================================\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8ba3d93-1600-49ec-ad2e-3e05eed98ccb"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}