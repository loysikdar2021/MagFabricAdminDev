{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Gateways - Get Gateways\n","# Command:  GET https://api.powerbi.com/v1.0/myorg/gateways\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/power-bi/gateways/get-gateways\n","\n","# Loads table: pbi_gateways"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"afe84ec1-147d-4374-909d-9e60157c61a7","normalized_state":"finished","queued_time":"2025-07-16T19:01:52.366716Z","session_start_time":"2025-07-16T19:01:52.3676456Z","execution_start_time":"2025-07-16T19:02:05.3350477Z","execution_finish_time":"2025-07-16T19:02:05.762299Z","parent_msg_id":"8ed87209-a161-4fdc-a43c-03820719b43b"},"text/plain":"StatementMeta(, afe84ec1-147d-4374-909d-9e60157c61a7, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b637b982-e6bf-46d2-aa2d-4ff5d46498d9"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Power BI Gateways to Delta Lake - PySpark Notebook\n","# This notebook retrieves Power BI gateways and loads them into a Delta Lake table\n","# with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json, when, isnotnull\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"PowerBIGatewaysToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.powerbi.com/v1.0/myorg\",\n","    \"GATEWAYS_ENDPOINT\": \"/gateways\",  # Endpoint for listing gateways\n","    \"MAX_RETRIES\": 5,  # Number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"GATEWAYS_TABLE_NAME\": \"pbi_gateways\",  # Name of the target Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\",  # Default Tables folder in Fabric Lakehouse\n","    \"DEBUG_MODE\": True  # Set to True to enable extra debugging output\n","}\n","\n","# Power BI API uses standard OData pagination, not continuation tokens like Fabric API\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Power BI API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Power BI REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","        For Power BI API, we need the Power BI service scope.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        # Power BI API requires the Power BI service scope\n","        token_response = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_powerbi_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Power BI with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Power BI API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/gateways\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            # Log the full URL with parameters for debugging\n","            logger.info(f\"Making API call to: {url} with params: {params} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Log the response status for debugging\n","            logger.info(f\"Response status: {response.status_code}\")\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # Log the response for debugging in case of errors\n","            if response.status_code >= 400:\n","                logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                logger.error(f\"Request URL: {response.request.url}\")\n","                logger.error(f\"Request headers: {response.request.headers}\")\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            # Log a sample of the response content for debugging\n","            try:\n","                response_json = response.json()\n","                if \"value\" in response_json and isinstance(response_json[\"value\"], list):\n","                    logger.info(f\"Response contains {len(response_json['value'])} items in 'value' array\")\n","                return response_json\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Failed to parse response as JSON: {str(e)}\")\n","                logger.error(f\"Response content: {response.text[:1000]}\")  # Log first 1000 chars of response\n","                raise\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Get Gateways Function\n","# ==================================\n","def get_gateways(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all gateways from the Power BI API.\n","    \n","    This function makes a request to the Get Gateways API endpoint.\n","    Unlike some APIs, Power BI gateways API typically returns all results in one call\n","    since gateway counts are usually manageable (not requiring pagination).\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all gateway objects\n","    \"\"\"\n","    try:\n","        logger.info(\"Retrieving gateways from Power BI API...\")\n","        \n","        # Make the API call to get gateways\n","        # Power BI API returns gateways that the user is an admin for\n","        response_data = call_powerbi_api(CONFIG['GATEWAYS_ENDPOINT'], access_token)\n","        \n","        # Log the response structure for debugging\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(f\"Response keys: {list(response_data.keys())}\")\n","        \n","        # Extract gateways from the response\n","        # Power BI API returns data in \"value\" array (OData format)\n","        gateways = response_data.get(\"value\", [])\n","        \n","        if gateways:\n","            logger.info(f\"Retrieved {len(gateways)} gateways\")\n","            \n","            # Log first gateway for debugging (with sensitive data masked)\n","            if CONFIG['DEBUG_MODE'] and gateways:\n","                sample_gateway = gateways[0].copy()\n","                # Mask sensitive data in logs\n","                if 'publicKey' in sample_gateway:\n","                    if isinstance(sample_gateway['publicKey'], dict):\n","                        if 'modulus' in sample_gateway['publicKey']:\n","                            sample_gateway['publicKey']['modulus'] = sample_gateway['publicKey']['modulus'][:20] + \"...\"\n","                logger.info(f\"Sample gateway structure: {json.dumps(sample_gateway, indent=2)}\")\n","        else:\n","            logger.warning(\"No gateways found - this could mean:\")\n","            logger.warning(\"1. No gateways are configured in your Power BI tenant\")\n","            logger.warning(\"2. You don't have admin permissions on any gateways\")\n","            logger.warning(\"3. All gateways are VNet gateways (not supported by this API)\")\n","        \n","        return gateways\n","        \n","    except requests.exceptions.RequestException as e:\n","        logger.error(f\"Failed to retrieve gateways: {str(e)}\")\n","        raise\n","    except Exception as e:\n","        logger.error(f\"Unexpected error while retrieving gateways: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 8 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_gateways_dataframe(gateways_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the gateways data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the gateway data\n","    - Flattens the nested publicKey object into separate columns\n","    - Adds metadata columns for tracking\n","    - Derives useful fields like is_active status\n","    \n","    Args:\n","        gateways_data: List of gateway dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Extract and flatten the gateway data with proper null handling\n","    flattened_gateways = []\n","    \n","    for gateway in gateways_data:\n","        # Extract the main gateway fields with explicit null handling\n","        flattened_gateway = {\n","            \"id\": gateway.get(\"id\", \"\"),  # ID should never be null\n","            \"name\": gateway.get(\"name\", \"\"),\n","            \"type\": gateway.get(\"type\", \"Unknown\"),\n","            \"gateway_status\": gateway.get(\"gatewayStatus\", \"Unknown\"),  # Provide default\n","            \"gateway_annotation\": gateway.get(\"gatewayAnnotation\", \"\")\n","        }\n","        \n","        # Flatten the nested publicKey object\n","        public_key = gateway.get(\"publicKey\", {})\n","        if isinstance(public_key, dict):\n","            flattened_gateway[\"public_key_exponent\"] = public_key.get(\"exponent\", \"\")\n","            flattened_gateway[\"public_key_modulus\"] = public_key.get(\"modulus\", \"\")\n","        else:\n","            flattened_gateway[\"public_key_exponent\"] = \"\"\n","            flattened_gateway[\"public_key_modulus\"] = \"\"\n","        \n","        flattened_gateways.append(flattened_gateway)\n","    \n","    # Define the schema for our Delta table\n","    schema = StructType([\n","        StructField(\"id\", StringType(), False),                    # Primary key - not nullable\n","        StructField(\"name\", StringType(), True),                  # Gateway name\n","        StructField(\"type\", StringType(), True),                  # Gateway type\n","        StructField(\"gateway_status\", StringType(), True),        # Current status\n","        StructField(\"gateway_annotation\", StringType(), True),    # JSON metadata\n","        StructField(\"public_key_exponent\", StringType(), True),   # Flattened public key\n","        StructField(\"public_key_modulus\", StringType(), True),    # Flattened public key\n","        StructField(\"is_active\", BooleanType(), False),           # Derived status field\n","        StructField(\"extraction_timestamp\", TimestampType(), False) # ETL timestamp\n","    ])\n","    \n","    # Handle empty data case\n","    if not flattened_gateways:\n","        logger.warning(\"No gateways found. Creating empty DataFrame with proper schema.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        enhanced_df = spark.createDataFrame(empty_rdd, schema)\n","        return enhanced_df\n","    \n","    # Convert to pandas DataFrame first for easier handling\n","    pandas_df = pd.DataFrame(flattened_gateways)\n","    \n","    # Ensure all required columns exist with proper defaults\n","    required_columns = [\n","        \"id\", \"name\", \"type\", \"gateway_status\", \"gateway_annotation\",\n","        \"public_key_exponent\", \"public_key_modulus\"\n","    ]\n","    \n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = \"\"  # Use empty string instead of None\n","    \n","    # Fill any remaining NaN values with empty strings\n","    pandas_df = pandas_df.fillna(\"\")\n","    \n","    # Validate that ID column is not empty\n","    if pandas_df['id'].isna().any() or (pandas_df['id'] == \"\").any():\n","        logger.error(\"Found gateways with missing or empty IDs\")\n","        # Remove rows with empty IDs\n","        pandas_df = pandas_df[pandas_df['id'] != \"\"]\n","        pandas_df = pandas_df.dropna(subset=['id'])\n","        logger.warning(f\"Removed gateways with missing IDs. Remaining: {len(pandas_df)}\")\n","    \n","    # Create the initial Spark DataFrame with explicit type casting\n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Cast all columns to proper string types and handle nulls\n","    typed_df = spark_df \\\n","        .withColumn(\"id\", col(\"id\").cast(StringType())) \\\n","        .withColumn(\"name\", col(\"name\").cast(StringType())) \\\n","        .withColumn(\"type\", col(\"type\").cast(StringType())) \\\n","        .withColumn(\"gateway_status\", col(\"gateway_status\").cast(StringType())) \\\n","        .withColumn(\"gateway_annotation\", col(\"gateway_annotation\").cast(StringType())) \\\n","        .withColumn(\"public_key_exponent\", col(\"public_key_exponent\").cast(StringType())) \\\n","        .withColumn(\"public_key_modulus\", col(\"public_key_modulus\").cast(StringType()))\n","    \n","    # Add derived and metadata columns\n","    enhanced_df = typed_df \\\n","        .withColumn(\"is_active\", \n","                   when(col(\"gateway_status\").isin([\"Live\", \"Online\"]), True)\n","                   .otherwise(False)) \\\n","        .withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    # Ensure column order matches our schema exactly\n","    final_df = enhanced_df.select(\n","        \"id\", \n","        \"name\", \n","        \"type\", \n","        \"gateway_status\", \n","        \"gateway_annotation\", \n","        \"public_key_exponent\", \n","        \"public_key_modulus\", \n","        \"is_active\", \n","        \"extraction_timestamp\"\n","    )\n","    \n","    # Log DataFrame info for debugging\n","    if CONFIG['DEBUG_MODE']:\n","        logger.info(f\"Created DataFrame with {final_df.count()} rows\")\n","        logger.info(\"DataFrame schema:\")\n","        final_df.printSchema()\n","    \n","    return final_df\n","# ==================================\n","\n","\n","# CELL 9 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    This function checks if our target Delta table exists in the Lakehouse.\n","    If it doesn't exist, it creates a new table with the proper schema.\n","    \n","    Args:\n","        table_name: Name of the Delta table (e.g., \"pbi_gateways\")\n","        df_schema: Schema of the DataFrame to match table structure\n","    \"\"\"\n","    try:\n","        # Check if table exists by trying to describe it\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating new Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the required schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new gateway data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if gateway ID matches (gateway info might change)\n","    - Inserts new records if gateway ID doesn't exist (new gateways added)\n","    - Handles schema evolution when column types need to change\n","    \n","    Args:\n","        source_df: DataFrame with new gateway data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Validate the source DataFrame before merge\n","    logger.info(\"Validating source DataFrame...\")\n","    source_df.printSchema()\n","    \n","    # Show sample data for debugging\n","    logger.info(\"Sample source data:\")\n","    source_df.show(3, truncate=False)\n","    \n","    # Check existing table schema\n","    existing_count = spark.table(table_name).count()\n","    if existing_count == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        logger.info(f\"Inserted {source_df.count()} new gateway records\")\n","        return\n","    \n","    # Check for schema compatibility issues\n","    logger.info(\"Checking existing table schema...\")\n","    existing_schema = spark.table(table_name).schema\n","    logger.info(\"Existing table schema:\")\n","    for field in existing_schema.fields:\n","        logger.info(f\"  {field.name}: {field.dataType}\")\n","    \n","    # Check if we have NullType columns that need schema evolution\n","    null_type_columns = []\n","    for field in existing_schema.fields:\n","        if str(field.dataType) == \"NullType\" or \"NullType\" in str(field.dataType):\n","            null_type_columns.append(field.name)\n","    \n","    # Also check for incompatible types by comparing schemas\n","    schema_mismatch = False\n","    source_schema = source_df.schema\n","    \n","    for source_field in source_schema.fields:\n","        existing_field = None\n","        for field in existing_schema.fields:\n","            if field.name == source_field.name:\n","                existing_field = field\n","                break\n","        \n","        if existing_field and str(existing_field.dataType) != str(source_field.dataType):\n","            logger.warning(f\"Schema mismatch for column '{source_field.name}': \"\n","                         f\"existing={existing_field.dataType}, source={source_field.dataType}\")\n","            schema_mismatch = True\n","    \n","    if null_type_columns or schema_mismatch:\n","        logger.warning(f\"Found incompatible schema that needs evolution.\")\n","        logger.warning(f\"NullType columns: {null_type_columns}\")\n","        logger.info(\"Performing schema evolution by recreating the table...\")\n","        \n","        # Read and preserve existing data BEFORE dropping the table\n","        existing_data = spark.table(table_name)\n","        existing_count = existing_data.count()\n","        \n","        # Convert existing data to list BEFORE dropping table\n","        preserved_data_list = []\n","        if existing_count > 0:\n","            logger.info(f\"Preserving {existing_count} existing records during schema evolution...\")\n","            \n","            # Get the source DataFrame column names and types for mapping\n","            source_columns = {field.name: field.dataType for field in source_df.schema.fields}\n","            \n","            # Collect existing data before dropping table\n","            for row in existing_data.collect():\n","                preserved_row = {}\n","                for col_name in source_columns.keys():\n","                    if col_name in existing_data.columns:\n","                        # Get the value from existing data\n","                        value = row[col_name]\n","                        # Handle null values properly\n","                        if value is None or (isinstance(value, str) and value.strip() == \"\"):\n","                            if str(source_columns[col_name]) == \"StringType()\":\n","                                preserved_row[col_name] = \"\"\n","                            elif str(source_columns[col_name]) == \"BooleanType()\":\n","                                preserved_row[col_name] = False\n","                            else:\n","                                preserved_row[col_name] = None\n","                        else:\n","                            preserved_row[col_name] = value\n","                    else:\n","                        # Column doesn't exist in old data, set default\n","                        if str(source_columns[col_name]) == \"StringType()\":\n","                            preserved_row[col_name] = \"\"\n","                        elif str(source_columns[col_name]) == \"BooleanType()\":\n","                            preserved_row[col_name] = False\n","                        else:\n","                            preserved_row[col_name] = None\n","                \n","                preserved_data_list.append(preserved_row)\n","        \n","        # Now drop the existing table\n","        logger.info(f\"Dropping existing table {table_name} for schema evolution...\")\n","        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","        \n","        # Create the combined data\n","        if preserved_data_list:\n","            logger.info(f\"Creating DataFrame from {len(preserved_data_list)} preserved records...\")\n","            preserved_df = spark.createDataFrame(preserved_data_list, source_df.schema)\n","            # Union preserved data with new data\n","            combined_data = preserved_df.union(source_df)\n","        else:\n","            logger.info(\"No existing data to preserve, using only new data...\")\n","            combined_data = source_df\n","        \n","        # Create the new table with the correct schema\n","        logger.info(\"Creating new table with correct schema...\")\n","        combined_data.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(\"Schema evolution completed successfully\")\n","        \n","    else:\n","        # Normal merge operation when schemas are compatible\n","        logger.info(\"Schemas are compatible. Proceeding with normal merge...\")\n","        \n","        # Create a temporary view for the merge operation\n","        source_df.createOrReplaceTempView(\"gateway_updates\")\n","        \n","        # Perform the merge operation\n","        merge_query = f\"\"\"\n","        MERGE INTO {table_name} AS target\n","        USING gateway_updates AS source\n","        ON target.id = source.id\n","        WHEN MATCHED THEN\n","            UPDATE SET \n","                target.name = source.name,\n","                target.type = source.type,\n","                target.gateway_status = source.gateway_status,\n","                target.gateway_annotation = source.gateway_annotation,\n","                target.public_key_exponent = source.public_key_exponent,\n","                target.public_key_modulus = source.public_key_modulus,\n","                target.is_active = source.is_active,\n","                target.extraction_timestamp = source.extraction_timestamp\n","        WHEN NOT MATCHED THEN\n","            INSERT (id, name, type, gateway_status, gateway_annotation, \n","                    public_key_exponent, public_key_modulus, is_active, extraction_timestamp)\n","            VALUES (source.id, source.name, source.type, source.gateway_status, \n","                    source.gateway_annotation, source.public_key_exponent, \n","                    source.public_key_modulus, source.is_active, source.extraction_timestamp)\n","        \"\"\"\n","        \n","        # Execute the merge\n","        spark.sql(merge_query)\n","    \n","    # Log merge statistics\n","    updated_count = spark.table(table_name).count()\n","    logger.info(f\"Merge operation completed successfully\")\n","    logger.info(f\"Total records in table after merge: {updated_count}\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function performs optimization tasks suitable for Microsoft Fabric:\n","    - Updates table statistics for better query planning\n","    - Uses Fabric-compatible optimization methods\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for the query optimizer\n","        # This helps Spark make better decisions about query execution plans\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # In Microsoft Fabric, many Delta optimizations are handled automatically\n","        # We'll focus on what we can control directly\n","        \n","        # Get table details for monitoring\n","        delta_table = DeltaTable.forName(spark, table_name)\n","        table_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n","        \n","        logger.info(\"Delta table optimization completed\")\n","        logger.info(\"Note: Microsoft Fabric handles many optimizations automatically\")\n","        \n","        # Show some optimization info\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(\"Table details after optimization:\")\n","            table_detail.select(\"format\", \"numFiles\", \"sizeInBytes\", \"partitionColumns\").show(truncate=False)\n","            \n","    except Exception as e:\n","        logger.warning(f\"Table optimization encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing - optimization issues don't affect core functionality\")\n","# ==================================\n","\n","\n","# CELL 10 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire Power BI gateways ETL process.\n","    \n","    This function:\n","    1. Authenticates with Azure AD to get Power BI API access\n","    2. Retrieves all gateways that the user is an admin for\n","    3. Creates an enhanced PySpark DataFrame with flattened gateway data\n","    4. Loads data into a Delta Lake table with upsert logic\n","    5. Optimizes the table for analytics performance\n","    6. Provides summary statistics and data quality insights\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Power BI Gateways to Delta Lake ETL process\")\n","        \n","        # Step 1: Authentication\n","        logger.info(\"Step 1: Getting Power BI API access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"✓ Successfully obtained access token\")\n","        \n","        # Step 2: Data Extraction\n","        logger.info(\"Step 2: Retrieving gateways from Power BI API...\")\n","        gateways_data = get_gateways(access_token)\n","        \n","        # Handle the case where no gateways are found\n","        if not gateways_data:\n","            logger.warning(\"⚠ No gateways found. This might be expected if:\")\n","            logger.warning(\"  - No gateways are configured in your organization\")\n","            logger.warning(\"  - You don't have admin permissions on any gateways\")\n","            logger.warning(\"  - Only VNet gateways exist (not supported by this API)\")\n","            \n","            # Still create the table structure for consistency\n","            empty_schema = StructType([\n","                StructField(\"id\", StringType(), False),\n","                StructField(\"name\", StringType(), True),\n","                StructField(\"type\", StringType(), True),\n","                StructField(\"gateway_status\", StringType(), True),\n","                StructField(\"gateway_annotation\", StringType(), True),\n","                StructField(\"public_key_exponent\", StringType(), True),\n","                StructField(\"public_key_modulus\", StringType(), True),\n","                StructField(\"is_active\", BooleanType(), False),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            gateways_df = spark.createDataFrame([], empty_schema)\n","        else:\n","            logger.info(f\"✓ Successfully retrieved {len(gateways_data)} gateways\")\n","            \n","            # Step 3: Data Transformation\n","            logger.info(\"Step 3: Creating enhanced DataFrame with flattened structure...\")\n","            gateways_df = create_enhanced_gateways_dataframe(gateways_data)\n","            logger.info(\"✓ DataFrame created successfully\")\n","        \n","        # Show sample of the processed data\n","        logger.info(\"Sample of processed gateway data:\")\n","        gateways_df.show(5, truncate=False)\n","        \n","        # Step 4: Data Loading - Prepare Delta table\n","        table_name = CONFIG[\"GATEWAYS_TABLE_NAME\"]\n","        logger.info(f\"Step 4: Preparing Delta table '{table_name}'...\")\n","        ensure_delta_table_exists(table_name, gateways_df.schema)\n","        \n","        # Step 5: Data Loading - Merge data (if we have data)\n","        if gateways_data:\n","            logger.info(\"Step 5: Merging data into Delta table...\")\n","            merge_data_to_delta(gateways_df, table_name)\n","            logger.info(\"✓ Data merge completed successfully\")\n","            \n","            # Step 6: Optimization\n","            logger.info(\"Step 6: Optimizing Delta table...\")\n","            optimize_delta_table(table_name)\n","            logger.info(\"✓ Table optimization completed\")\n","        else:\n","            logger.info(\"Step 5-6: Skipping merge and optimization (no data to process)\")\n","        \n","        # Step 7: Results and Analytics\n","        logger.info(\"Step 7: Generating summary report...\")\n","        \n","        # Show table information\n","        logger.info(\"=== TABLE INFORMATION ===\")\n","        table_details = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n","        table_details.select(\"format\", \"numFiles\", \"sizeInBytes\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"📊 Total gateways in table: {row_count}\")\n","        \n","        # Show summary statistics (if we have data)\n","        if row_count > 0:\n","            logger.info(\"=== GATEWAY ANALYTICS ===\")\n","            \n","            summary_stats = spark.sql(f\"\"\"\n","                SELECT \n","                    COUNT(*) as total_gateways,\n","                    COUNT(DISTINCT type) as gateway_types,\n","                    COUNT(DISTINCT gateway_status) as status_types,\n","                    SUM(CASE WHEN is_active = true THEN 1 ELSE 0 END) as active_gateways,\n","                    MAX(extraction_timestamp) as last_updated\n","                FROM {table_name}\n","            \"\"\")\n","            \n","            logger.info(\"📈 Summary Statistics:\")\n","            summary_stats.show(truncate=False)\n","            \n","            # Gateway distribution by type\n","            type_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    COALESCE(type, 'Unknown') as gateway_type,\n","                    COUNT(*) as count,\n","                    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n","                FROM {table_name}\n","                GROUP BY type\n","                ORDER BY count DESC\n","            \"\"\")\n","            \n","            logger.info(\"🔧 Gateway Distribution by Type:\")\n","            type_distribution.show(truncate=False)\n","            \n","            # Gateway status distribution\n","            status_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    COALESCE(gateway_status, 'Unknown') as status,\n","                    COUNT(*) as count,\n","                    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n","                FROM {table_name}\n","                GROUP BY gateway_status\n","                ORDER BY count DESC\n","            \"\"\")\n","            \n","            logger.info(\"📡 Gateway Status Distribution:\")\n","            status_distribution.show(truncate=False)\n","            \n","            # Activity summary with better labeling\n","            activity_summary = spark.sql(f\"\"\"\n","                SELECT \n","                    CASE \n","                        WHEN is_active = true THEN 'Active'\n","                        ELSE 'Inactive'\n","                    END as status,\n","                    COUNT(*) as count\n","                FROM {table_name}\n","                GROUP BY is_active\n","                ORDER BY is_active DESC\n","            \"\"\")\n","            \n","            logger.info(\"⚡ Gateway Activity Summary:\")\n","            activity_summary.show(truncate=False)\n","        \n","        logger.info(\"🎉 ETL process completed successfully!\")\n","        logger.info(f\"✅ Gateway data is now available in the '{table_name}' table\")\n","        logger.info(\"💡 You can now use this data for Power BI reports and analytics\")\n","        \n","        return gateways_df\n","        \n","    except Exception as e:\n","        logger.error(f\"❌ Error in main execution: {str(e)}\")\n","        logger.error(\"🔍 Check the logs above for detailed error information\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 11 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","# This is the entry point that starts the entire ETL process\n","if __name__ == \"__main__\":\n","    try:\n","        logger.info(\"🚀 Starting Power BI Gateways ETL Pipeline\")\n","        gateways_df = main()\n","        logger.info(\"✨ Pipeline completed successfully!\")\n","    except Exception as e:\n","        logger.error(f\"💥 Pipeline failed: {str(e)}\")\n","        raise\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"afe84ec1-147d-4374-909d-9e60157c61a7","normalized_state":"finished","queued_time":"2025-07-16T19:01:52.4599623Z","session_start_time":null,"execution_start_time":"2025-07-16T19:02:05.7662657Z","execution_finish_time":"2025-07-16T19:02:42.9500342Z","parent_msg_id":"2e303b56-0a89-4d94-95fe-1844dec3b8e9"},"text/plain":"StatementMeta(, afe84ec1-147d-4374-909d-9e60157c61a7, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- type: string (nullable = true)\n |-- gateway_status: string (nullable = true)\n |-- gateway_annotation: string (nullable = true)\n |-- public_key_exponent: string (nullable = true)\n |-- public_key_modulus: string (nullable = true)\n |-- is_active: boolean (nullable = false)\n |-- extraction_timestamp: timestamp (nullable = false)\n\n+------------------------------------+-------------------------------+--------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+--------------------------+\n|id                                  |name                           |type    |gateway_status|gateway_annotation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |public_key_exponent|public_key_modulus                                                                                                                                                                                                                                                                                                                                      |is_active|extraction_timestamp      |\n+------------------------------------+-------------------------------+--------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+--------------------------+\n|211a986c-1e4a-4029-a064-126e534f40bb|Academic Analytics & Technology|Resource|Unknown       |{\"gatewayContactInformation\":[\"PIIaeger@mdanderson.org\"],\"gatewayVersion\":\"3000.6.204+g6523bf7046\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"zaL0UMav0QtH8ajzs22ynhRIqz8CrHdORE1AaDCh5ZzyF+d4vk8K5SMsU3QgdfjOP4kDPJi8ukLOWllLliJLJp7z9Ze6vg1LrD+awczCFite+xEyzfJQYACLmz9ZHunXOwnpnBxkBmjDJl8zoCo82Q==\\\",\\\"IV\\\":\\\"bEy2CQTfJxfgv1xgbb9TWA==\\\",\\\"Signature\\\":\\\"VbbBX0NecSKrE2XeC5Rmf2xWLZnceiMxVDcUdc76Sds=\\\"}\",\"gatewayMachine\":\"IR--W0CL0152173\"}                                                                                                                                                                                                                                                                                                                                                                                                                          |AQAB               |3Sul0tItmplAPmt9H695...                                                                                                                                                                                                                                                                                                                                 |false    |2025-07-16 19:02:08.183111|\n|7505cd2d-8438-4fac-9b32-d43a1182d32e|mdaEnterprise                  |Resource|Unknown       |{\"gatewayContactInformation\":[\"DXHuynh@mdanderson.org\"],\"gatewayVersion\":\"3000.45.7\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"Pj3MQlm9Es5pNAiqWrGjAtPicWwUr1yT/DKgJeyA9MBCA/tRQTyaiDCgxujFcOlj0JjiBELa6U5v0Nt0BONA1+ZsyY00xdeg2jg58Gv2lhVYfUjSI6qW8/Jljx1p1DTA3axdegTcKqt11uKuPqeTrg==\\\",\\\"IV\\\":\\\"VPTxMbjAcZFOKF9IbSP6ug==\\\",\\\"Signature\\\":\\\"sX91klAY7HV+O4jRiheQbvXTTiP/3XvZBytWS36MGsA=\\\"}\",\"gatewayMachine\":\"X1TWLROADSRND2\",\"gatewaySalt\":\"kcIhwg6ZsatPfLUp+BkaTA==\",\"gatewayWitnessStringLegacy\":null,\"gatewaySaltLegacy\":null,\"gatewayDepartment\":null}                                                                                                                                                                                                                                                                                                            |AQAB               |joV3MNm4IQ6Aj/x6jm7x/UwGXQjaqyLMbppsLup9stiO27eUTBq7T85z0cn5v9vyfWuKwe/q/+t9TY/lZt/ygQ9X7raGrso1ccTYqwh6e8SoRJINh5Kc3oHNWIDQFfK2ISHdGrv/MkzUlvf10dUtYdfXskyGnM6jFhoeTFBm1CdbYIiC+a+DVAU4ejitjzoKw0Ik8hRuGhyf7k63CIU0XT2d7ug1/CUY/TtOszDXZHA0ase9/JgpGosLtjJE6rxpTDXt4MfWRLDH0/x+6DAPq/1cV8WDipN4MwRI1MJMn2N5P+nqZiMWBsgtIikK392dcoKRmgoF7hGLl6D6ned+2w==|false    |2025-07-16 19:02:08.183111|\n|4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c|MDA EDI Analytics Gateway 2    |Resource|Unknown       |{\"gatewayContactInformation\":[\"SAbraham7@mdanderson.org\"],\"gatewayVersion\":\"3000.45.7\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"4wI1voNM3WwA0PN+HUOmbM6bG+RAawcE9iyevPJNLveDq6IcbsmvfWuyBFT2ScCQGYctqFYq0AbphTidcmxS9guvE2+3u8OIkZnYbj7t7y5uPdjNAJrFse4lWr+rjxlm1ZGfEk4FLs2C/lnFXdo33A==\\\",\\\"IV\\\":\\\"YVYU6wdL/TLNV7/9ZGygIA==\\\",\\\"Signature\\\":\\\"QQLy6UhHoiJOJ4qB8nmPKcnRtpdggZr7rvoW4sF/G+Y=\\\"}\",\"gatewayMachine\":\"X1TWLROADSRND1\",\"gatewaySalt\":\"MryW1rpgxV2BS0G/RyL7Tg==\",\"gatewayWitnessStringLegacy\":\"{\\\"EncryptedResult\\\":\\\"zxKJu00iQv6hSWfwfF6cvQxMiAEfafMRqRmYrE2jCOOY/4uSjDaq+5IXPoHDyBDLcS/U8l8E/STUqqlgmTb6KkTHZshzox7yl4I/qgeTjinP5SLn6b3Om+dnD2cqsQKWg8MMsoUwexkVdAHnO7Inag==\\\",\\\"IV\\\":\\\"u2I1JZwClZtHV2hFi8jgVA==\\\",\\\"Signature\\\":\\\"OYyoG1o5XsmXvwfmZpJczp1FibGBwbeige1Mo0wNBms=\\\"}\",\"gatewaySaltLegacy\":\"Wdo9PjbzAo1eCck4tnhrPw==\",\"gatewayDepartment\":\"EDI\"}|AQAB               |1sKY7mf5G3tHzGPi50xB/MltHWmvDKZOQN8N9ePSKb6sGwioKEJb8JXOrSuCX8ErL4wcfwDwPTqo7qgCGAsj6Bl4YCKbZmze/hcxqZD0mevlIxwLF/6p0RyDLdisdi/URZcY7CcQtua4DD3sxDkvYkj1+Ac0OJ56b1E1/bEswMbCxc6pM2iRt8wVOOl8nVv1QKyedvpjbT43nEq8E6O3AWYrDuHeSUtVCuU6CT9zQo+LK9pNT4upls+4jgCIu3t0LhkPmqR4u6ITWaIL3NsxaJLmbmQPCzjLyv8U+uJ7NxqFtQpXprGSCgAbzSss8wD4tcGKdgOWgwWZFBJqKHJFoQ==|false    |2025-07-16 19:02:08.183111|\n|ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194|PowerBI                        |Resource|Unknown       |{\"gatewayContactInformation\":[\"DZheng@mdanderson.org\"],\"gatewayVersion\":\"3000.51.7\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"WY7UjZSO7t65yZPQEma8fnFxzEbk3dB91Sutvmwiu+5eEG8Is63JQg7GXlIvQAOq9MNskit+fLSSPL9p6iyhqELSbqjIAwHpKZvtL/0xv7wB4T1f38+uN3uR/I8bwGCy4PodBP2K3xCJqldUEcHkXQ==\\\",\\\"IV\\\":\\\"WufBRSkD04cqgPOeNOdDTg==\\\",\\\"Signature\\\":\\\"jXI/gA0kTqwj/hPToo+sjC4DOpzqq9r5n0UxFa0i/og=\\\"}\",\"gatewayMachine\":\"W10L-4QBQK13\",\"gatewaySalt\":\"8CfcvL6QJtKE8XDhOfih/A==\",\"gatewayWitnessStringLegacy\":null,\"gatewaySaltLegacy\":null,\"gatewayDepartment\":\"BHS\"}                                                                                                                                                                                                                                                                                                              |AQAB               |sAkIPZzByG5kwvgBY5M0AkDndiWDfesnXWQOp1NR6j4+a590Iv5tFKBGlKd46Vcxk5RJmYsGOP36hex5aHuD93Xz1b5Hu7KWeo7/N8FFZdgCLVUnr9sEUWX0zZIofgBdRb4KObHpApNy6Wvj9rIMpo+RYZRGKvm6P9cmIN4PqJkB/eQQKjycuzX5PQS+xUIi66ogRTXYHuXAHgwlz/OYJL28u7wpj/MQJ+1BGTSM6XQSy6lU1vtKRng4hfqK3A5BTKK/tN47cJRNOnG7mBq3Mnbuadlu/PKeELtDmFhtK8EPyv0oP6RpjIC+r71Z4Y6+ocOm8u2LOrKrFyvcrOiMCQ==|false    |2025-07-16 19:02:08.183111|\n|47d3ea9b-fb98-4e41-8516-2aa7c02b7eca|MDAENTDGWY01-DEV               |Resource|Unknown       |{\"gatewayContactInformation\":[\"RPReddy@mdanderson.org\"],\"gatewayVersion\":\"3000.63.6\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"j2pGODDvUXdOhd5MqrgsrG9P6BUul/Cue5F+VE8CPSR17zL/NGvV1GPRldbqyNvSvGAjboGA1ImX38Itj38kgo8QwPSZMlKIRqasHB7w06p8J/FvOeUWZvD9wC49AYHFEZtxSpa6fN11ylHyL2nvig==\\\",\\\"IV\\\":\\\"sGzVs2nfBEvoimHv0nDYBA==\\\",\\\"Signature\\\":\\\"IpnFKhXavycTWaPKcaE2ceRYRWevWYfru6Dp6uJYzao=\\\"}\",\"gatewayMachine\":\"D1DWLENTDGAWY1\",\"gatewaySalt\":\"6lkKl5JWZ9lrI5X9KdFdvw==\",\"gatewayWitnessStringLegacy\":null,\"gatewaySaltLegacy\":null,\"gatewayDepartment\":\"EDI / IAI Co-managed\",\"gatewayVirtualNetworkSubnetId\":null}                                                                                                                                                                                                                                                     |AQAB               |xWLuaqjgbKaplZJEcFCyRrrRCMJL0v/PY1Fq52uCCRaUlwGwLfc6dNBfk0rjOBdy6dH/LJ+/HPV5mBkFTHCiXxmYcx6OcWCDtfhnp5dfI1zubgmGj7fYk7x5t2Lwq0iGJotzUS/qcUuSjRTAaI9TCM34w67XH5d04vL3CBHMvsV0J6ne14SV97xmL7FH7b9YskpwfBkvsL0XEnO6QcwKgATJWK6AJLEAWxmEOJGjdlRQlgwZyBwenH52Y4emJupgwzITPIdzzAYnK51hWykJay3qKODAU4GQzIkTQiVifQanxOtrwQwH0aPZSYHqwF2aBtD3DoCIuejvyJakVyZzjQ==|false    |2025-07-16 19:02:08.183111|\n+------------------------------------+-------------------------------+--------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+--------------------------+\nonly showing top 5 rows\n\nroot\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- type: string (nullable = true)\n |-- gateway_status: string (nullable = true)\n |-- gateway_annotation: string (nullable = true)\n |-- public_key_exponent: string (nullable = true)\n |-- public_key_modulus: string (nullable = true)\n |-- is_active: boolean (nullable = false)\n |-- extraction_timestamp: timestamp (nullable = false)\n\n+------------------------------------+-------------------------------+--------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+--------------------------+\n|id                                  |name                           |type    |gateway_status|gateway_annotation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |public_key_exponent|public_key_modulus                                                                                                                                                                                                                                                                                                                                      |is_active|extraction_timestamp      |\n+------------------------------------+-------------------------------+--------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+--------------------------+\n|211a986c-1e4a-4029-a064-126e534f40bb|Academic Analytics & Technology|Resource|Unknown       |{\"gatewayContactInformation\":[\"PIIaeger@mdanderson.org\"],\"gatewayVersion\":\"3000.6.204+g6523bf7046\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"zaL0UMav0QtH8ajzs22ynhRIqz8CrHdORE1AaDCh5ZzyF+d4vk8K5SMsU3QgdfjOP4kDPJi8ukLOWllLliJLJp7z9Ze6vg1LrD+awczCFite+xEyzfJQYACLmz9ZHunXOwnpnBxkBmjDJl8zoCo82Q==\\\",\\\"IV\\\":\\\"bEy2CQTfJxfgv1xgbb9TWA==\\\",\\\"Signature\\\":\\\"VbbBX0NecSKrE2XeC5Rmf2xWLZnceiMxVDcUdc76Sds=\\\"}\",\"gatewayMachine\":\"IR--W0CL0152173\"}                                                                                                                                                                                                                                                                                                                                                                                                                          |AQAB               |3Sul0tItmplAPmt9H695...                                                                                                                                                                                                                                                                                                                                 |false    |2025-07-16 19:02:18.158081|\n|7505cd2d-8438-4fac-9b32-d43a1182d32e|mdaEnterprise                  |Resource|Unknown       |{\"gatewayContactInformation\":[\"DXHuynh@mdanderson.org\"],\"gatewayVersion\":\"3000.45.7\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"Pj3MQlm9Es5pNAiqWrGjAtPicWwUr1yT/DKgJeyA9MBCA/tRQTyaiDCgxujFcOlj0JjiBELa6U5v0Nt0BONA1+ZsyY00xdeg2jg58Gv2lhVYfUjSI6qW8/Jljx1p1DTA3axdegTcKqt11uKuPqeTrg==\\\",\\\"IV\\\":\\\"VPTxMbjAcZFOKF9IbSP6ug==\\\",\\\"Signature\\\":\\\"sX91klAY7HV+O4jRiheQbvXTTiP/3XvZBytWS36MGsA=\\\"}\",\"gatewayMachine\":\"X1TWLROADSRND2\",\"gatewaySalt\":\"kcIhwg6ZsatPfLUp+BkaTA==\",\"gatewayWitnessStringLegacy\":null,\"gatewaySaltLegacy\":null,\"gatewayDepartment\":null}                                                                                                                                                                                                                                                                                                            |AQAB               |joV3MNm4IQ6Aj/x6jm7x/UwGXQjaqyLMbppsLup9stiO27eUTBq7T85z0cn5v9vyfWuKwe/q/+t9TY/lZt/ygQ9X7raGrso1ccTYqwh6e8SoRJINh5Kc3oHNWIDQFfK2ISHdGrv/MkzUlvf10dUtYdfXskyGnM6jFhoeTFBm1CdbYIiC+a+DVAU4ejitjzoKw0Ik8hRuGhyf7k63CIU0XT2d7ug1/CUY/TtOszDXZHA0ase9/JgpGosLtjJE6rxpTDXt4MfWRLDH0/x+6DAPq/1cV8WDipN4MwRI1MJMn2N5P+nqZiMWBsgtIikK392dcoKRmgoF7hGLl6D6ned+2w==|false    |2025-07-16 19:02:18.158081|\n|4aa1be79-5442-4bc7-b7f5-6e44fc5e5f4c|MDA EDI Analytics Gateway 2    |Resource|Unknown       |{\"gatewayContactInformation\":[\"SAbraham7@mdanderson.org\"],\"gatewayVersion\":\"3000.45.7\",\"gatewayWitnessString\":\"{\\\"EncryptedResult\\\":\\\"4wI1voNM3WwA0PN+HUOmbM6bG+RAawcE9iyevPJNLveDq6IcbsmvfWuyBFT2ScCQGYctqFYq0AbphTidcmxS9guvE2+3u8OIkZnYbj7t7y5uPdjNAJrFse4lWr+rjxlm1ZGfEk4FLs2C/lnFXdo33A==\\\",\\\"IV\\\":\\\"YVYU6wdL/TLNV7/9ZGygIA==\\\",\\\"Signature\\\":\\\"QQLy6UhHoiJOJ4qB8nmPKcnRtpdggZr7rvoW4sF/G+Y=\\\"}\",\"gatewayMachine\":\"X1TWLROADSRND1\",\"gatewaySalt\":\"MryW1rpgxV2BS0G/RyL7Tg==\",\"gatewayWitnessStringLegacy\":\"{\\\"EncryptedResult\\\":\\\"zxKJu00iQv6hSWfwfF6cvQxMiAEfafMRqRmYrE2jCOOY/4uSjDaq+5IXPoHDyBDLcS/U8l8E/STUqqlgmTb6KkTHZshzox7yl4I/qgeTjinP5SLn6b3Om+dnD2cqsQKWg8MMsoUwexkVdAHnO7Inag==\\\",\\\"IV\\\":\\\"u2I1JZwClZtHV2hFi8jgVA==\\\",\\\"Signature\\\":\\\"OYyoG1o5XsmXvwfmZpJczp1FibGBwbeige1Mo0wNBms=\\\"}\",\"gatewaySaltLegacy\":\"Wdo9PjbzAo1eCck4tnhrPw==\",\"gatewayDepartment\":\"EDI\"}|AQAB               |1sKY7mf5G3tHzGPi50xB/MltHWmvDKZOQN8N9ePSKb6sGwioKEJb8JXOrSuCX8ErL4wcfwDwPTqo7qgCGAsj6Bl4YCKbZmze/hcxqZD0mevlIxwLF/6p0RyDLdisdi/URZcY7CcQtua4DD3sxDkvYkj1+Ac0OJ56b1E1/bEswMbCxc6pM2iRt8wVOOl8nVv1QKyedvpjbT43nEq8E6O3AWYrDuHeSUtVCuU6CT9zQo+LK9pNT4upls+4jgCIu3t0LhkPmqR4u6ITWaIL3NsxaJLmbmQPCzjLyv8U+uJ7NxqFtQpXprGSCgAbzSss8wD4tcGKdgOWgwWZFBJqKHJFoQ==|false    |2025-07-16 19:02:18.158081|\n+------------------------------------+-------------------------------+--------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+--------------------------+\nonly showing top 3 rows\n\n+------+--------+-----------+----------------+\n|format|numFiles|sizeInBytes|partitionColumns|\n+------+--------+-----------+----------------+\n|delta |1       |21154      |[]              |\n+------+--------+-----------+----------------+\n\n+------+--------+-----------+\n|format|numFiles|sizeInBytes|\n+------+--------+-----------+\n|delta |1       |21154      |\n+------+--------+-----------+\n\n+--------+-----+\n|status  |count|\n+--------+-----+\n|Inactive|14   |\n+--------+-----+\n\n"]},{"output_type":"stream","name":"stderr","text":["2025-07-16 19:02:21,590 - INFO - Checking existing table schema...\n2025-07-16 19:02:21,783 - INFO - Existing table schema:\n2025-07-16 19:02:21,784 - INFO -   id: StringType()\n2025-07-16 19:02:21,784 - INFO -   name: StringType()\n2025-07-16 19:02:21,785 - INFO -   type: StringType()\n2025-07-16 19:02:21,785 - INFO -   gateway_status: StringType()\n2025-07-16 19:02:21,786 - INFO -   gateway_annotation: StringType()\n2025-07-16 19:02:21,786 - INFO -   public_key_exponent: StringType()\n2025-07-16 19:02:21,787 - INFO -   public_key_modulus: StringType()\n2025-07-16 19:02:21,787 - INFO -   is_active: BooleanType()\n2025-07-16 19:02:21,788 - INFO -   extraction_timestamp: TimestampType()\n2025-07-16 19:02:21,789 - INFO - Schemas are compatible. Proceeding with normal merge...\n2025-07-16 19:02:35,053 - INFO - Merge operation completed successfully\n2025-07-16 19:02:35,055 - INFO - Total records in table after merge: 14\n2025-07-16 19:02:35,056 - INFO - ✓ Data merge completed successfully\n2025-07-16 19:02:35,057 - INFO - Step 6: Optimizing Delta table...\n2025-07-16 19:02:35,057 - INFO - Optimizing Delta table 'pbi_gateways'\n2025-07-16 19:02:40,028 - INFO - 🔧 Gateway Distribution by Type:\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e46084cf-0496-43c3-aa06-5012ae9a9550"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE pbi_gateways\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"afe84ec1-147d-4374-909d-9e60157c61a7","normalized_state":"finished","queued_time":"2025-07-16T19:01:52.5482231Z","session_start_time":null,"execution_start_time":"2025-07-16T19:02:42.9524453Z","execution_finish_time":"2025-07-16T19:02:43.3951642Z","parent_msg_id":"49441bec-a51f-4b6b-a439-efac543321ea"},"text/plain":"StatementMeta(, afe84ec1-147d-4374-909d-9e60157c61a7, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e771f58d-8a24-416c-82d2-3aa919b25e19"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}