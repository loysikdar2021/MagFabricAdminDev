{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Capacities List - PySpark Notebook\n","# This notebook retrieves a list of all Microsoft Fabric capacities and displays selected properties\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, explode, when\n","from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n","import logging\n","from typing import Dict, List, Optional\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session (this is already available in Fabric notebooks)\n","# The Spark session is your entry point to using PySpark\n","spark = SparkSession.builder.appName(\"FabricCapacitiesList\").getOrCreate()\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","# You can modify these as needed for your environment\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"MAX_RETRIES\": 3,\n","    \"PAGE_SIZE\": 100,  # Number of items per page for API calls\n","    \"TIMEOUT\": 30  # API request timeout in seconds\n","}\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        # This token includes the necessary permissions for Fabric API\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries if the request fails\n","    - Error handling\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/capacities\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Retry logic - sometimes API calls can fail temporarily\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Check if the request was successful\n","            response.raise_for_status()\n","            \n","            return response.json()\n","            \n","        except requests.exceptions.RequestException as e:\n","            logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","            \n","            if attempt == CONFIG['MAX_RETRIES'] - 1:\n","                logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                raise\n","            \n","            # Wait before retrying (exponential backoff)\n","            import time\n","            time.sleep(2 ** attempt)\n","# ==================================\n","\n","\n","# CELL 7 - Get All Capacities Function\n","# ==================================\n","def get_all_capacities(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all Fabric capacities, handling pagination if necessary.\n","    \n","    The Fabric API may return results in pages if there are many capacities.\n","    This function handles the pagination automatically to get all capacities.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all capacity objects\n","    \"\"\"\n","    all_capacities = []\n","    continuation_token = None\n","    \n","    while True:\n","        # Set up parameters for the API call\n","        params = {\"top\": CONFIG['PAGE_SIZE']}\n","        if continuation_token:\n","            params[\"continuationToken\"] = continuation_token\n","        \n","        # Call the API\n","        response = call_fabric_api(\"/capacities\", access_token, params)\n","        \n","        # Extract capacities from the response\n","        capacities = response.get(\"value\", [])\n","        all_capacities.extend(capacities)\n","        \n","        logger.info(f\"Retrieved {len(capacities)} capacities. Total so far: {len(all_capacities)}\")\n","        \n","        # Check if there are more pages\n","        continuation_token = response.get(\"continuationToken\")\n","        if not continuation_token:\n","            break\n","    \n","    logger.info(f\"Finished retrieving capacities. Total count: {len(all_capacities)}\")\n","    return all_capacities\n","# ==================================\n","\n","\n","# CELL 8 - Create DataFrame Function\n","# ==================================\n","def create_capacities_dataframe(capacities_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the capacities data into a PySpark DataFrame with the specified columns.\n","    \n","    This function takes the raw JSON data from the API and creates a structured\n","    PySpark DataFrame with only the columns we want to display.\n","    \n","    Args:\n","        capacities_data: List of capacity dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame with the specified columns\n","    \"\"\"\n","    # Define the schema for our DataFrame\n","    # This tells PySpark what columns we want and their data types\n","    schema = StructType([\n","        StructField(\"id\", StringType(), True),\n","        StructField(\"displayName\", StringType(), True),  # True means the field can be null\n","        StructField(\"sku\", StringType(), True),\n","        StructField(\"region\", StringType(), True),\n","        StructField(\"state\", StringType(), True)\n","    ])\n","    \n","    # Extract only the fields we need from each capacity\n","    filtered_data = []\n","    for capacity in capacities_data:\n","        filtered_capacity = {\n","            \"id\": capacity.get(\"id\"),\n","            \"displayName\": capacity.get(\"displayName\"),\n","            \"sku\": capacity.get(\"sku\"),\n","            \"region\": capacity.get(\"region\"),\n","            \"state\": capacity.get(\"state\")\n","        }\n","        filtered_data.append(filtered_capacity)\n","    \n","    # Create a PySpark DataFrame\n","    # First, we create a Pandas DataFrame, then convert it to PySpark\n","    # This is often easier for small datasets\n","    pandas_df = pd.DataFrame(filtered_data)\n","    spark_df = spark.createDataFrame(pandas_df, schema=schema)\n","    \n","    return spark_df\n","# ==================================\n","\n","\n","# CELL 9 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all capacities from the API\n","    3. Creates a PySpark DataFrame\n","    4. Displays the results\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric Capacities retrieval process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve all capacities\n","        logger.info(\"Retrieving capacities from Fabric API...\")\n","        capacities_data = get_all_capacities(access_token)\n","        \n","        if not capacities_data:\n","            logger.warning(\"No capacities found\")\n","            return\n","        \n","        # Step 3: Create PySpark DataFrame\n","        logger.info(\"Creating PySpark DataFrame...\")\n","        capacities_df = create_capacities_dataframe(capacities_data)\n","        \n","        # Step 4: Display results\n","        logger.info(\"Displaying capacities...\")\n","        \n","        # Show the schema (structure) of our DataFrame\n","        print(\"\\nDataFrame Schema:\")\n","        capacities_df.printSchema()\n","        \n","        # Show the data\n","        print(\"\\nFabric Capacities:\")\n","        capacities_df.show(truncate=False)  # truncate=False shows full values\n","        \n","        # Show row count\n","        row_count = capacities_df.count()\n","        print(f\"\\nTotal number of capacities: {row_count}\")\n","        \n","        '''\n","        # Optional: Save to a temporary table for SQL queries\n","        capacities_df.createOrReplaceTempView(\"fabric_capacities\")\n","        logger.info(\"Created temporary view 'fabric_capacities' for SQL queries\")\n","        '''\n","        '''\n","        # Example SQL query\n","        print(\"\\nExample: Capacities by Region (using SQL):\")\n","        spark.sql(\"\"\"\n","            SELECT region, COUNT(*) as capacity_count \n","            FROM fabric_capacities \n","            GROUP BY region \n","            ORDER BY capacity_count DESC\n","        \"\"\").show()\n","        '''\n","        # Return the DataFrame for further use\n","        return capacities_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 10 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    capacities_df = main()\n","# ==================================\n","\n","\n","# CELL 11 - Analysis Helper Functions\n","# ==================================\n","# Additional helper functions for data analysis\n","\n","def analyze_capacities(capacities_df):\n","    \"\"\"\n","    Perform basic analysis on the capacities DataFrame.\n","    \n","    This function demonstrates how to use PySpark DataFrame operations\n","    to analyze the data we've retrieved.\n","    \"\"\"\n","    print(\"\\n=== Capacity Analysis ===\")\n","    \n","    # Count by SKU\n","    print(\"\\nCapacities by SKU:\")\n","    capacities_df.groupBy(\"sku\").count().orderBy(\"count\", ascending=False).show()\n","    \n","    # Count by state\n","    print(\"\\nCapacities by State:\")\n","    capacities_df.groupBy(\"state\").count().orderBy(\"count\", ascending=False).show()\n","    \n","    # Count by region\n","    print(\"\\nCapacities by Region:\")\n","    capacities_df.groupBy(\"region\").count().orderBy(\"count\", ascending=False).show()\n","    \n","    # Find capacities with specific states\n","    print(\"\\nActive Capacities:\")\n","    active_capacities = capacities_df.filter(col(\"state\") == \"Active\")\n","    active_capacities.select(\"displayName\", \"sku\", \"region\").show(truncate=False)\n","# ==================================\n","\n","\n","# CELL 12 - Run Analysis (Optional)\n","# ==================================\n","# Usage example:\n","# To run the analysis after getting the data, uncomment the following line:\n","    analyze_capacities(capacities_df)\n","# ==================================\n","\n","\n","# CELL 13 - Usage Notes for Beginners\n","# ==================================\n","\"\"\"\n","USAGE NOTES FOR BEGINNERS:\n","\n","1. AUTHENTICATION:\n","   - The notebook automatically handles authentication using your Fabric credentials\n","   - No need to manually provide tokens or credentials\n","\n","2. RUNNING THE CODE:\n","   - Simply run the cells in order\n","   - The main() function will execute the entire process\n","\n","3. UNDERSTANDING THE OUTPUT:\n","   - Schema: Shows the structure of your data (column names and types)\n","   - Data: Shows the actual capacity information\n","   - Analysis: Shows summary statistics about your capacities\n","\n","4. CUSTOMIZATION:\n","   - To filter results: Use DataFrame.filter() method\n","   - To sort results: Use DataFrame.orderBy() method\n","   - To select specific columns: Use DataFrame.select() method\n","\n","5. ERROR HANDLING:\n","   - The code includes comprehensive error handling\n","   - Check the logs for detailed error messages\n","   - Common issues: network connectivity, permissions\n","\n","6. SAVING RESULTS:\n","   - To save to a table: df.write.saveAsTable(\"table_name\")\n","   - To save to CSV: df.write.csv(\"path/to/file.csv\")\n","   - To save to Parquet: df.write.parquet(\"path/to/file.parquet\")\n","\n","Example of filtering and saving:\n","```python\n","# Filter only active capacities in a specific region\n","filtered_df = capacities_df.filter(\n","    (col(\"state\") == \"Active\") & \n","    (col(\"region\") == \"West US\")\n",")\n","\n","# Save to a table\n","filtered_df.write.mode(\"overwrite\").saveAsTable(\"active_west_us_capacities\")\n","```\n","\"\"\"\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"70bcc750-b585-4132-bc60-f21285f06451","normalized_state":"finished","queued_time":"2025-05-14T18:21:32.9266407Z","session_start_time":null,"execution_start_time":"2025-05-14T18:21:32.9283658Z","execution_finish_time":"2025-05-14T18:21:40.8502993Z","parent_msg_id":"0c59c708-897c-4fcf-9874-f314c046a30e"},"text/plain":"StatementMeta(, 70bcc750-b585-4132-bc60-f21285f06451, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-05-14 18:21:33,039 - INFO - Starting Fabric Capacities retrieval process\n2025-05-14 18:21:33,039 - INFO - Getting access token...\n2025-05-14 18:21:33,042 - INFO - Successfully obtained access token\n2025-05-14 18:21:33,042 - INFO - Retrieving capacities from Fabric API...\n2025-05-14 18:21:33,043 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/capacities (Attempt 1)\n"]},{"output_type":"stream","name":"stdout","text":["\nDataFrame Schema:\nroot\n |-- id: string (nullable = true)\n |-- displayName: string (nullable = true)\n |-- sku: string (nullable = true)\n |-- region: string (nullable = true)\n |-- state: string (nullable = true)\n\n\nFabric Capacities:\n+------------------------------------+---------------------------------+---+----------------+--------+\n|id                                  |displayName                      |sku|region          |state   |\n+------------------------------------+---------------------------------+---+----------------+--------+\n|ab3b62c5-cff1-4341-a584-4ef86a529e8a|f64nonprodsouthcentral001        |F64|South Central US|Inactive|\n|250aef2d-b24b-43a8-8564-8fefc5152522|f8nonprodsouthcentral001         |F8 |South Central US|Active  |\n|d94bc350-4bb9-4f24-9d89-fd633996eb28|f64x002                          |F64|South Central US|Active  |\n|56125c55-2f69-4fa3-bac0-e9407fc17374|f64x001                          |F64|South Central US|Active  |\n|8e0020ba-3162-4e4d-9d3f-83b6ce695c5d|f32x001                          |F32|South Central US|Active  |\n|4dc39e58-c232-494c-b629-45298de2fa27|f16x001                          |F16|South Central US|Active  |\n|c73a5223-9ef6-4514-83cc-3e70297ee377|MDA Institutional Capacity - PROD|P1 |West US         |Active  |\n|db9a247d-9b0d-4c13-8ad3-443fa3a6b50a|Premium Per User - Reserved      |PP3|West US         |Active  |\n+------------------------------------+---------------------------------+---+----------------+--------+\n\n\nTotal number of capacities: 8\n"]},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"'\\nUSAGE NOTES FOR BEGINNERS:\\n\\n1. AUTHENTICATION:\\n   - The notebook automatically handles authentication using your Fabric credentials\\n   - No need to manually provide tokens or credentials\\n\\n2. RUNNING THE CODE:\\n   - Simply run the cells in order\\n   - The main() function will execute the entire process\\n\\n3. UNDERSTANDING THE OUTPUT:\\n   - Schema: Shows the structure of your data (column names and types)\\n   - Data: Shows the actual capacity information\\n   - Analysis: Shows summary statistics about your capacities\\n\\n4. CUSTOMIZATION:\\n   - To filter results: Use DataFrame.filter() method\\n   - To sort results: Use DataFrame.orderBy() method\\n   - To select specific columns: Use DataFrame.select() method\\n\\n5. ERROR HANDLING:\\n   - The code includes comprehensive error handling\\n   - Check the logs for detailed error messages\\n   - Common issues: network connectivity, permissions\\n\\n6. SAVING RESULTS:\\n   - To save to a table: df.write.saveAsTable(\"table_name\")\\n   - To save to CSV: df.write.csv(\"path/to/file.csv\")\\n   - To save to Parquet: df.write.parquet(\"path/to/file.parquet\")\\n\\nExample of filtering and saving:\\n```python\\n# Filter only active capacities in a specific region\\nfiltered_df = capacities_df.filter(\\n    (col(\"state\") == \"Active\") & \\n    (col(\"region\") == \"West US\")\\n)\\n\\n# Save to a table\\nfiltered_df.write.mode(\"overwrite\").saveAsTable(\"active_west_us_capacities\")\\n```\\n'"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc1dd0a8-0f67-4d42-ac5b-5f1e080df109"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Capacities List - PySpark Notebook with Service Principal Authentication\n","# This notebook retrieves a list of all Microsoft Fabric capacities using Service Principal (SPN) authentication\n","# and displays selected properties\n","# \n","# PREREQUISITES:\n","# 1. Create a Service Principal in Azure AD\n","# 2. Grant the SPN appropriate permissions in Fabric (Fabric Administrator or Workspace Admin)\n","# 3. Configure the SPN credentials in the configuration section\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, explode, when\n","from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n","import logging\n","from typing import Dict, List, Optional\n","import os\n","from datetime import datetime, timedelta\n","\n","# Azure authentication libraries\n","try:\n","    from azure.identity import ClientSecretCredential\n","    AZURE_IDENTITY_AVAILABLE = True\n","except ImportError:\n","    AZURE_IDENTITY_AVAILABLE = False\n","    # If azure-identity is not available, we'll use direct OAuth2 flow\n","    pass\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session (this is already available in Fabric notebooks)\n","# The Spark session is your entry point to using PySpark\n","spark = SparkSession.builder.appName(\"FabricCapacitiesList_SPN\").getOrCreate()\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","\n","# API Configuration\n","API_CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"MAX_RETRIES\": 3,\n","    \"PAGE_SIZE\": 100,  # Number of items per page for API calls\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"FABRIC_SCOPE\": \"https://api.fabric.microsoft.com/.default\"  # Scope for Fabric API\n","}\n","\n","# Authentication Method Selection\n","# Options: \"service_principal\", \"user_auth\" (fallback to original method)\n","AUTH_METHOD = \"service_principal\"\n","\n","# Service Principal Configuration\n","# SECURITY WARNING: Do not hardcode secrets in production code!\n","# Use environment variables, Azure Key Vault, or Fabric secrets\n","SPN_CONFIG = {\n","    # Option 1: Environment Variables (Recommended)\n","    #\"TENANT_ID\": os.environ.get(\"FABRIC_SPN_TENANT_ID\", \"\"),\n","    #\"CLIENT_ID\": os.environ.get(\"FABRIC_SPN_CLIENT_ID\", \"\"),\n","    #\"CLIENT_SECRET\": os.environ.get(\"FABRIC_SPN_CLIENT_SECRET\", \"\"),\n","    \n","    # Option 2: Direct Configuration (Use only for testing)\n","    # Uncomment and fill these if not using environment variables\n","     \"TENANT_ID\": \"2d51fc70-177a-4852-ba7e-54d34883bb15\",\n","     \"CLIENT_ID\": \"f9d144fe-dc6f-4d3e-98e0-456e84bc6e23\",\n","     \"CLIENT_SECRET\": \"H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb\",\n","    \n","    # OAuth2 endpoints\n","    \"AUTHORITY_URL\": \"https://login.microsoftonline.com/\",\n","    \"TOKEN_ENDPOINT\": \"/oauth2/v2.0/token\"\n","}\n","\n","# Key Vault Configuration (Optional)\n","# If you want to retrieve secrets from Azure Key Vault\n","KEY_VAULT_CONFIG = {\n","    \"USE_KEY_VAULT\": False,  # Set to True to use Key Vault\n","    \"VAULT_NAME\": \"your-key-vault-name\",\n","    \"SECRET_NAME_CLIENT_ID\": \"fabric-spn-client-id\",\n","    \"SECRET_NAME_CLIENT_SECRET\": \"fabric-spn-client-secret\"\n","}\n","# ==================================\n","\n","\n","# CELL 5 - Validation Functions\n","# ==================================\n","def validate_spn_config() -> bool:\n","    \"\"\"\n","    Validate that all required SPN configuration parameters are present.\n","    \n","    Returns:\n","        bool: True if configuration is valid, False otherwise\n","    \"\"\"\n","    required_fields = [\"TENANT_ID\", \"CLIENT_ID\", \"CLIENT_SECRET\"]\n","    \n","    missing_fields = []\n","    for field in required_fields:\n","        if not SPN_CONFIG.get(field):\n","            missing_fields.append(field)\n","    \n","    if missing_fields:\n","        logger.error(f\"Missing required SPN configuration: {', '.join(missing_fields)}\")\n","        logger.info(\"Please set these as environment variables or in the SPN_CONFIG section\")\n","        return False\n","    \n","    return True\n","\n","\n","def get_secret_from_key_vault(secret_name: str) -> str:\n","    \"\"\"\n","    Retrieve a secret from Azure Key Vault.\n","    \n","    Note: This requires the azure-keyvault-secrets package and appropriate permissions.\n","    \n","    Args:\n","        secret_name: Name of the secret in Key Vault\n","    \n","    Returns:\n","        str: The secret value\n","    \"\"\"\n","    if not KEY_VAULT_CONFIG[\"USE_KEY_VAULT\"]:\n","        return \"\"\n","    \n","    try:\n","        from azure.keyvault.secrets import SecretClient\n","        from azure.identity import DefaultAzureCredential\n","        \n","        vault_url = f\"https://{KEY_VAULT_CONFIG['VAULT_NAME']}.vault.azure.net\"\n","        credential = DefaultAzureCredential()\n","        client = SecretClient(vault_url=vault_url, credential=credential)\n","        \n","        secret = client.get_secret(secret_name)\n","        return secret.value\n","    except Exception as e:\n","        logger.error(f\"Failed to retrieve secret from Key Vault: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - Service Principal Authentication Functions\n","# ==================================\n","def get_token_using_azure_identity(tenant_id: str, client_id: str, client_secret: str) -> str:\n","    \"\"\"\n","    Get access token using azure-identity library.\n","    \n","    This is the preferred method as it handles token caching and refresh automatically.\n","    \n","    Args:\n","        tenant_id: Azure AD tenant ID\n","        client_id: Service Principal client ID\n","        client_secret: Service Principal client secret\n","    \n","    Returns:\n","        str: Access token for Fabric API\n","    \"\"\"\n","    if not AZURE_IDENTITY_AVAILABLE:\n","        raise ImportError(\"azure-identity package is not available. Please install it or use direct OAuth2 method.\")\n","    \n","    try:\n","        credential = ClientSecretCredential(\n","            tenant_id=tenant_id,\n","            client_id=client_id,\n","            client_secret=client_secret\n","        )\n","        \n","        # Get token for Fabric API\n","        token = credential.get_token(API_CONFIG[\"FABRIC_SCOPE\"])\n","        return token.token\n","        \n","    except Exception as e:\n","        logger.error(f\"Failed to get token using azure-identity: {str(e)}\")\n","        raise\n","\n","\n","def get_token_using_direct_oauth(tenant_id: str, client_id: str, client_secret: str) -> str:\n","    \"\"\"\n","    Get access token using direct OAuth2 client credentials flow.\n","    \n","    This method doesn't require additional Azure libraries.\n","    \n","    Args:\n","        tenant_id: Azure AD tenant ID\n","        client_id: Service Principal client ID\n","        client_secret: Service Principal client secret\n","    \n","    Returns:\n","        str: Access token for Fabric API\n","    \"\"\"\n","    token_url = f\"{SPN_CONFIG['AUTHORITY_URL']}{tenant_id}{SPN_CONFIG['TOKEN_ENDPOINT']}\"\n","    \n","    # Prepare the token request\n","    token_data = {\n","        \"grant_type\": \"client_credentials\",\n","        \"client_id\": client_id,\n","        \"client_secret\": client_secret,\n","        \"scope\": API_CONFIG[\"FABRIC_SCOPE\"]\n","    }\n","    \n","    try:\n","        logger.info(\"Requesting access token from Azure AD...\")\n","        response = requests.post(\n","            token_url,\n","            data=token_data,\n","            timeout=API_CONFIG[\"TIMEOUT\"]\n","        )\n","        \n","        response.raise_for_status()\n","        token_response = response.json()\n","        \n","        if \"access_token\" not in token_response:\n","            raise ValueError(\"No access token in response\")\n","        \n","        logger.info(\"Successfully obtained access token\")\n","        return token_response[\"access_token\"]\n","        \n","    except requests.exceptions.RequestException as e:\n","        logger.error(f\"Failed to get token via direct OAuth: {str(e)}\")\n","        if response.text:\n","            logger.error(f\"Response: {response.text}\")\n","        raise\n","\n","\n","def get_access_token_spn() -> str:\n","    \"\"\"\n","    Get access token using Service Principal authentication.\n","    \n","    This function handles the SPN authentication flow, including:\n","    - Validating configuration\n","    - Retrieving secrets from Key Vault if configured\n","    - Getting token using available method\n","    \n","    Returns:\n","        str: Access token for Fabric API\n","    \"\"\"\n","    # Get SPN credentials\n","    if KEY_VAULT_CONFIG[\"USE_KEY_VAULT\"]:\n","        logger.info(\"Retrieving SPN credentials from Key Vault...\")\n","        client_id = get_secret_from_key_vault(KEY_VAULT_CONFIG[\"SECRET_NAME_CLIENT_ID\"])\n","        client_secret = get_secret_from_key_vault(KEY_VAULT_CONFIG[\"SECRET_NAME_CLIENT_SECRET\"])\n","        tenant_id = SPN_CONFIG[\"TENANT_ID\"]  # Tenant ID usually doesn't need to be secret\n","    else:\n","        client_id = SPN_CONFIG[\"CLIENT_ID\"]\n","        client_secret = SPN_CONFIG[\"CLIENT_SECRET\"]\n","        tenant_id = SPN_CONFIG[\"TENANT_ID\"]\n","    \n","    # Validate we have all required values\n","    if not all([tenant_id, client_id, client_secret]):\n","        raise ValueError(\"Missing required SPN credentials. Please check configuration.\")\n","    \n","    # Try to get token using azure-identity first (preferred method)\n","    if AZURE_IDENTITY_AVAILABLE:\n","        logger.info(\"Using azure-identity for authentication...\")\n","        return get_token_using_azure_identity(tenant_id, client_id, client_secret)\n","    else:\n","        logger.info(\"Using direct OAuth2 flow for authentication...\")\n","        return get_token_using_direct_oauth(tenant_id, client_id, client_secret)\n","\n","\n","def get_access_token_user() -> str:\n","    \"\"\"\n","    Get access token using user authentication (original method).\n","    \n","    This is the fallback method that uses the current user's credentials.\n","    \n","    Returns:\n","        str: Access token for Fabric API\n","    \"\"\"\n","    try:\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get user access token: {str(e)}\")\n","        raise\n","\n","\n","def get_access_token() -> str:\n","    \"\"\"\n","    Get access token based on configured authentication method.\n","    \n","    This function routes to the appropriate authentication method based on configuration.\n","    \n","    Returns:\n","        str: Access token for Fabric API\n","    \"\"\"\n","    if AUTH_METHOD == \"service_principal\":\n","        logger.info(\"Using Service Principal authentication...\")\n","        return get_access_token_spn()\n","    else:\n","        logger.info(\"Using user authentication...\")\n","        return get_access_token_user()\n","# ==================================\n","\n","\n","# CELL 7 - API Call Function (Unchanged)\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries if the request fails\n","    - Error handling\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/capacities\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{API_CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Retry logic - sometimes API calls can fail temporarily\n","    for attempt in range(API_CONFIG['MAX_RETRIES']):\n","        try:\n","            logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=API_CONFIG['TIMEOUT']\n","            )\n","            \n","            # Check if the request was successful\n","            response.raise_for_status()\n","            \n","            return response.json()\n","            \n","        except requests.exceptions.RequestException as e:\n","            logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","            \n","            if attempt == API_CONFIG['MAX_RETRIES'] - 1:\n","                logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                raise\n","            \n","            # Wait before retrying (exponential backoff)\n","            import time\n","            time.sleep(2 ** attempt)\n","# ==================================\n","\n","\n","# CELL 8 - Get All Capacities Function (Minor Update)\n","# ==================================\n","def get_all_capacities(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all Fabric capacities, handling pagination if necessary.\n","    \n","    The Fabric API may return results in pages if there are many capacities.\n","    This function handles the pagination automatically to get all capacities.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all capacity objects\n","    \"\"\"\n","    all_capacities = []\n","    continuation_token = None\n","    \n","    while True:\n","        # Set up parameters for the API call\n","        params = {\"top\": API_CONFIG['PAGE_SIZE']}\n","        if continuation_token:\n","            params[\"continuationToken\"] = continuation_token\n","        \n","        # Call the API\n","        response = call_fabric_api(\"/capacities\", access_token, params)\n","        \n","        # Extract capacities from the response\n","        capacities = response.get(\"value\", [])\n","        all_capacities.extend(capacities)\n","        \n","        logger.info(f\"Retrieved {len(capacities)} capacities. Total so far: {len(all_capacities)}\")\n","        \n","        # Check if there are more pages\n","        continuation_token = response.get(\"continuationToken\")\n","        if not continuation_token:\n","            break\n","    \n","    logger.info(f\"Finished retrieving capacities. Total count: {len(all_capacities)}\")\n","    return all_capacities\n","# ==================================\n","\n","\n","# CELL 9 - Create DataFrame Function (Unchanged)\n","# ==================================\n","def create_capacities_dataframe(capacities_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the capacities data into a PySpark DataFrame with the specified columns.\n","    \n","    This function takes the raw JSON data from the API and creates a structured\n","    PySpark DataFrame with only the columns we want to display.\n","    \n","    Args:\n","        capacities_data: List of capacity dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame with the specified columns\n","    \"\"\"\n","    # Define the schema for our DataFrame\n","    # This tells PySpark what columns we want and their data types\n","    schema = StructType([\n","        StructField(\"id\", StringType(), True),\n","        StructField(\"displayName\", StringType(), True),  # True means the field can be null\n","        StructField(\"sku\", StringType(), True),\n","        StructField(\"region\", StringType(), True),\n","        StructField(\"state\", StringType(), True)\n","    ])\n","    \n","    # Extract only the fields we need from each capacity\n","    filtered_data = []\n","    for capacity in capacities_data:\n","        filtered_capacity = {\n","            \"id\": capacity.get(\"id\"),\n","            \"displayName\": capacity.get(\"displayName\"),\n","            \"sku\": capacity.get(\"sku\"),\n","            \"region\": capacity.get(\"region\"),\n","            \"state\": capacity.get(\"state\")\n","        }\n","        filtered_data.append(filtered_capacity)\n","    \n","    # Create a PySpark DataFrame\n","    # First, we create a Pandas DataFrame, then convert it to PySpark\n","    # This is often easier for small datasets\n","    pandas_df = pd.DataFrame(filtered_data)\n","    spark_df = spark.createDataFrame(pandas_df, schema=schema)\n","    \n","    return spark_df\n","# ==================================\n","\n","\n","# CELL 10 - Test Authentication Function\n","# ==================================\n","def test_authentication():\n","    \"\"\"\n","    Test the authentication configuration before running the main process.\n","    \n","    This function helps diagnose authentication issues by:\n","    - Validating configuration\n","    - Attempting to get a token\n","    - Making a simple API call\n","    \"\"\"\n","    print(\"\\n=== Testing Authentication Configuration ===\")\n","    \n","    # Check authentication method\n","    print(f\"Authentication Method: {AUTH_METHOD}\")\n","    \n","    if AUTH_METHOD == \"service_principal\":\n","        # Validate SPN configuration\n","        print(\"\\nValidating Service Principal configuration...\")\n","        if not validate_spn_config():\n","            print(\"❌ Invalid SPN configuration. Please check your settings.\")\n","            return False\n","        print(\"✅ SPN configuration is valid\")\n","        \n","        # Check if azure-identity is available\n","        print(f\"\\nazure-identity available: {'✅ Yes' if AZURE_IDENTITY_AVAILABLE else '❌ No'}\")\n","    \n","    # Try to get a token\n","    print(\"\\nAttempting to get access token...\")\n","    try:\n","        token = get_access_token()\n","        print(\"✅ Successfully obtained access token\")\n","        \n","        # Try a simple API call\n","        print(\"\\nTesting API access...\")\n","        response = call_fabric_api(\"/capacities\", token, {\"top\": 1})\n","        print(\"✅ API call successful\")\n","        \n","        return True\n","        \n","    except Exception as e:\n","        print(f\"❌ Authentication failed: {str(e)}\")\n","        return False\n","# ==================================\n","\n","\n","# CELL 11 - Main Execution Function (Updated)\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Validates authentication configuration\n","    2. Gets the authentication token\n","    3. Retrieves all capacities from the API\n","    4. Creates a PySpark DataFrame\n","    5. Displays the results\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Fabric Capacities retrieval process\")\n","        \n","        # Step 1: Validate authentication if using SPN\n","        if AUTH_METHOD == \"service_principal\":\n","            logger.info(\"Validating Service Principal configuration...\")\n","            if not validate_spn_config():\n","                raise ValueError(\"Invalid Service Principal configuration\")\n","        \n","        # Step 2: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 3: Retrieve all capacities\n","        logger.info(\"Retrieving capacities from Fabric API...\")\n","        capacities_data = get_all_capacities(access_token)\n","        \n","        if not capacities_data:\n","            logger.warning(\"No capacities found\")\n","            return\n","        \n","        # Step 4: Create PySpark DataFrame\n","        logger.info(\"Creating PySpark DataFrame...\")\n","        capacities_df = create_capacities_dataframe(capacities_data)\n","        \n","        # Step 5: Display results\n","        logger.info(\"Displaying capacities...\")\n","        \n","        # Show the schema (structure) of our DataFrame\n","        print(\"\\nDataFrame Schema:\")\n","        capacities_df.printSchema()\n","        \n","        # Show the data\n","        print(\"\\nFabric Capacities:\")\n","        capacities_df.show(truncate=False)  # truncate=False shows full values\n","        \n","        # Show row count\n","        row_count = capacities_df.count()\n","        print(f\"\\nTotal number of capacities: {row_count}\")\n","        \n","        # Optional: Save to a temporary table for SQL queries\n","        capacities_df.createOrReplaceTempView(\"fabric_capacities\")\n","        logger.info(\"Created temporary view 'fabric_capacities' for SQL queries\")\n","        \n","        # Return the DataFrame for further use\n","        return capacities_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 12 - Execute Main Function\n","# ==================================\n","# Test authentication before running main process\n","if test_authentication():\n","    print(\"\\n✅ Authentication test passed. Proceeding with main execution...\\n\")\n","    capacities_df = main()\n","else:\n","    print(\"\\n❌ Authentication test failed. Please fix configuration before proceeding.\")\n","# ==================================\n","\n","\n","# CELL 13 - Analysis Helper Functions (Unchanged)\n","# ==================================\n","# Additional helper functions for data analysis\n","\n","def analyze_capacities(capacities_df):\n","    \"\"\"\n","    Perform basic analysis on the capacities DataFrame.\n","    \n","    This function demonstrates how to use PySpark DataFrame operations\n","    to analyze the data we've retrieved.\n","    \"\"\"\n","    print(\"\\n=== Capacity Analysis ===\")\n","    \n","    # Count by SKU\n","    print(\"\\nCapacities by SKU:\")\n","    capacities_df.groupBy(\"sku\").count().orderBy(\"count\", ascending=False).show()\n","    \n","    # Count by state\n","    print(\"\\nCapacities by State:\")\n","    capacities_df.groupBy(\"state\").count().orderBy(\"count\", ascending=False).show()\n","    \n","    # Count by region\n","    print(\"\\nCapacities by Region:\")\n","    capacities_df.groupBy(\"region\").count().orderBy(\"count\", ascending=False).show()\n","    \n","    # Find capacities with specific states\n","    print(\"\\nActive Capacities:\")\n","    active_capacities = capacities_df.filter(col(\"state\") == \"Active\")\n","    active_capacities.select(\"displayName\", \"sku\", \"region\").show(truncate=False)\n","# ==================================\n","\n","\n","# CELL 14 - Run Analysis (Optional)\n","# ==================================\n","# Usage example:\n","# To run the analysis after getting the data, uncomment the following line:\n","# analyze_capacities(capacities_df)\n","# ==================================\n","\n","\n","# CELL 15 - Service Principal Setup Guide\n","# ==================================\n","\"\"\"\n","SERVICE PRINCIPAL SETUP GUIDE FOR BEGINNERS:\n","\n","1. CREATE SERVICE PRINCIPAL IN AZURE:\n","   a. Go to Azure Portal (portal.azure.com)\n","   b. Navigate to Azure Active Directory > App registrations\n","   c. Click \"New registration\"\n","   d. Give it a name (e.g., \"Fabric-API-Access-SPN\")\n","   e. Select \"Accounts in this organizational directory only\"\n","   f. Click \"Register\"\n","   g. Note down the \"Application (client) ID\" and \"Directory (tenant) ID\"\n","\n","2. CREATE CLIENT SECRET:\n","   a. In your app registration, go to \"Certificates & secrets\"\n","   b. Click \"New client secret\"\n","   c. Add a description and select expiration\n","   d. Click \"Add\"\n","   e. IMPORTANT: Copy the secret value immediately (it won't be shown again)\n","\n","3. GRANT PERMISSIONS IN FABRIC:\n","   a. Go to the Fabric portal\n","   b. Navigate to Admin portal > Tenant settings\n","   c. Find \"Service principals can use Fabric APIs\"\n","   d. Enable this setting\n","   e. Add your service principal to the allowed list\n","   f. OR grant workspace/capacity admin permissions as needed\n","\n","4. CONFIGURE IN THIS NOTEBOOK:\n","   Method 1 - Environment Variables (Recommended):\n","   ```python\n","   # Set these in your environment or notebook\n","   import os\n","   os.environ[\"FABRIC_SPN_TENANT_ID\"] = \"your-tenant-id\"\n","   os.environ[\"FABRIC_SPN_CLIENT_ID\"] = \"your-client-id\"\n","   os.environ[\"FABRIC_SPN_CLIENT_SECRET\"] = \"your-secret\"\n","   ```\n","   \n","   Method 2 - Direct Configuration (Testing only):\n","   Update the SPN_CONFIG dictionary in Cell 4\n","   \n","   Method 3 - Azure Key Vault (Most secure):\n","   a. Store secrets in Azure Key Vault\n","   b. Update KEY_VAULT_CONFIG in Cell 4\n","   c. Grant your notebook identity access to Key Vault\n","\n","5. TROUBLESHOOTING COMMON ISSUES:\n","   - \"Invalid client\" error: Check client ID and tenant ID\n","   - \"Invalid client secret\" error: Check the secret hasn't expired\n","   - \"Unauthorized\" error: Check SPN has Fabric API permissions\n","   - \"Access denied\" error: Check SPN has permissions on specific resources\n","\n","6. SECURITY BEST PRACTICES:\n","   - Never hardcode secrets in production code\n","   - Use Key Vault or environment variables\n","   - Rotate secrets regularly\n","   - Use certificate-based auth for production\n","   - Apply principle of least privilege\n","\"\"\"\n","# ==================================\n","\n","\n","# CELL 16 - Usage Notes for Beginners (Updated)\n","# ==================================\n","\"\"\"\n","USAGE NOTES FOR BEGINNERS:\n","\n","1. AUTHENTICATION OPTIONS:\n","   - Service Principal: For automated/scheduled jobs\n","   - User Authentication: For interactive notebook sessions\n","   - Switch between methods using AUTH_METHOD variable\n","\n","2. RUNNING THE CODE:\n","   - First run the authentication test to verify setup\n","   - Then run the main() function for the full process\n","   - Check logs for detailed information\n","\n","3. UNDERSTANDING THE OUTPUT:\n","   - Schema: Shows the structure of your data\n","   - Data: Shows the actual capacity information\n","   - Analysis: Shows summary statistics\n","\n","4. ERROR HANDLING:\n","   - Authentication errors: Check SPN configuration\n","   - API errors: Check permissions and network\n","   - Look at log messages for details\n","\n","5. SAVING RESULTS:\n","   ```python\n","   # Save to Delta table\n","   capacities_df.write.mode(\"overwrite\").saveAsTable(\"capacities_table\")\n","   \n","   # Save to CSV\n","   capacities_df.write.mode(\"overwrite\").csv(\"/path/to/file.csv\")\n","   \n","   # Save to Parquet\n","   capacities_df.write.mode(\"overwrite\").parquet(\"/path/to/file.parquet\")\n","   ```\n","\n","6. SWITCHING AUTHENTICATION METHODS:\n","   ```python\n","   # To use Service Principal\n","   AUTH_METHOD = \"service_principal\"\n","   \n","   # To use user authentication (original method)\n","   AUTH_METHOD = \"user_auth\"\n","   ```\n","\n","7. USING WITH SCHEDULED JOBS:\n","   - Service Principal auth is required for automation\n","   - Store credentials securely (Key Vault recommended)\n","   - Monitor token expiration and renewal\n","\"\"\"\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"32251843-d63f-4da0-b33c-bff8ec0dc264","normalized_state":"finished","queued_time":"2025-05-14T20:19:17.3498271Z","session_start_time":null,"execution_start_time":"2025-05-14T20:19:17.3521637Z","execution_finish_time":"2025-05-14T20:19:40.3049789Z","parent_msg_id":"872c053f-0f00-4b63-aa15-767a8cb09147"},"text/plain":"StatementMeta(, 32251843-d63f-4da0-b33c-bff8ec0dc264, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-05-14 20:19:17,486 - INFO - Using user authentication...\n"]},{"output_type":"stream","name":"stdout","text":["\n=== Testing Authentication Configuration ===\nAuthentication Method: user_auth\n\nAttempting to get access token...\n✅ Successfully obtained access token\n\nTesting API access...\n✅ API call successful\n\n✅ Authentication test passed. Proceeding with main execution...\n\n\nDataFrame Schema:\nroot\n |-- id: string (nullable = true)\n |-- displayName: string (nullable = true)\n |-- sku: string (nullable = true)\n |-- region: string (nullable = true)\n |-- state: string (nullable = true)\n\n\nFabric Capacities:\n+------------------------------------+---------------------------------+---+----------------+--------+\n|id                                  |displayName                      |sku|region          |state   |\n+------------------------------------+---------------------------------+---+----------------+--------+\n|ab3b62c5-cff1-4341-a584-4ef86a529e8a|f64nonprodsouthcentral001        |F64|South Central US|Inactive|\n|250aef2d-b24b-43a8-8564-8fefc5152522|f8nonprodsouthcentral001         |F8 |South Central US|Active  |\n|d94bc350-4bb9-4f24-9d89-fd633996eb28|f64x002                          |F64|South Central US|Active  |\n|56125c55-2f69-4fa3-bac0-e9407fc17374|f64x001                          |F64|South Central US|Active  |\n|8e0020ba-3162-4e4d-9d3f-83b6ce695c5d|f32x001                          |F32|South Central US|Active  |\n|4dc39e58-c232-494c-b629-45298de2fa27|f16x001                          |F16|South Central US|Active  |\n|c73a5223-9ef6-4514-83cc-3e70297ee377|MDA Institutional Capacity - PROD|P1 |West US         |Active  |\n|db9a247d-9b0d-4c13-8ad3-443fa3a6b50a|Premium Per User - Reserved      |PP3|West US         |Active  |\n+------------------------------------+---------------------------------+---+----------------+--------+\n\n\nTotal number of capacities: 8\n"]},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'\\nUSAGE NOTES FOR BEGINNERS:\\n\\n1. AUTHENTICATION OPTIONS:\\n   - Service Principal: For automated/scheduled jobs\\n   - User Authentication: For interactive notebook sessions\\n   - Switch between methods using AUTH_METHOD variable\\n\\n2. RUNNING THE CODE:\\n   - First run the authentication test to verify setup\\n   - Then run the main() function for the full process\\n   - Check logs for detailed information\\n\\n3. UNDERSTANDING THE OUTPUT:\\n   - Schema: Shows the structure of your data\\n   - Data: Shows the actual capacity information\\n   - Analysis: Shows summary statistics\\n\\n4. ERROR HANDLING:\\n   - Authentication errors: Check SPN configuration\\n   - API errors: Check permissions and network\\n   - Look at log messages for details\\n\\n5. SAVING RESULTS:\\n   ```python\\n   # Save to Delta table\\n   capacities_df.write.mode(\"overwrite\").saveAsTable(\"capacities_table\")\\n   \\n   # Save to CSV\\n   capacities_df.write.mode(\"overwrite\").csv(\"/path/to/file.csv\")\\n   \\n   # Save to Parquet\\n   capacities_df.write.mode(\"overwrite\").parquet(\"/path/to/file.parquet\")\\n   ```\\n\\n6. SWITCHING AUTHENTICATION METHODS:\\n   ```python\\n   # To use Service Principal\\n   AUTH_METHOD = \"service_principal\"\\n   \\n   # To use user authentication (original method)\\n   AUTH_METHOD = \"user_auth\"\\n   ```\\n\\n7. USING WITH SCHEDULED JOBS:\\n   - Service Principal auth is required for automation\\n   - Store credentials securely (Key Vault recommended)\\n   - Monitor token expiration and renewal\\n'"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe9f3f79-9e40-4d06-a61e-42ae86c3952b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}