{"cells":[{"cell_type":"code","source":["#Load Lakehouse table data into Warehouse stage schema\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import StructType, StructField, StringType\n","from pyspark.sql import DataFrame\n","import os\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Lakehouse to Warehouse\") \\\n","    .getOrCreate()\n","\n","# Define the path to your Lakehouse\n","lakehouse_path = \"Tables\"\n","\n","# List all tables (directories) in the Lakehouse\n","tables = [table for table in os.listdir(lakehouse_path) if os.path.isdir(os.path.join(lakehouse_path, table))]\n","\n","# Loop through each table\n","for table in tables:\n","    # Read data from Lakehouse\n","    table_path = os.path.join(lakehouse_path, table)\n","    df = spark.read.format(\"parquet\").load(table_path)\n","   \n","    # Write data to Warehouse using SPN\n","#    warehouse_url = \"jdbc:sqlserver://your_server.database.windows.net:1433;database=your_database\"\n","    warehouse_url = \"od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com\"\n","    warehouse_properties = {\n","        \"user\": \"8ed7e13a-9a60-4048-baba-200faf9425b2\",\n","        \"password\": \"8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc\",\n","        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n","        \"authentication\": \"ActiveDirectoryServicePrincipal\",\n","        \"tenant\": \"2d51fc70-177a-4852-ba7e-54d34883bb15\"\n","    }\n","   \n","    # Write to staging schema\n","    staging_table = f\"stage.{table}\"\n","    df.write.jdbc(url=warehouse_url, table=staging_table, mode=\"overwrite\", properties=warehouse_properties)\n","\n","# Close Spark session\n","spark.stop()\n","\n","#Connection String: od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com\n","#Display Name: “Informatica IDMC Microsoft Fabric SPN Dev”\n","#Application (client) ID: 8ed7e13a-9a60-4048-baba-200faf9425b2\n","#Object ID: 11e7cc5c-ef71-49d6-8a0f-cbdd46ca3f87\n","#Directory (tenant) ID: 2d51fc70-177a-4852-ba7e-54d34883bb15\n","#Secret ID: e71a9ffc-234a-4f8d-88a9-e2dbf6c71c6f\n","#Secret Value: 8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e09cdb4e-a19a-4412-9337-55cb69ab7404"},{"cell_type":"code","source":["#Testing Warehouse READ\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","\n","# Create the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# JDBC connection properties\n","#jdbc_url = \"jdbc:sqlserver://your_server_name.database.windows.net:1433;database=your_database_name\"\n","jdbc_url = \"jdbc:sqlserver://od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com:1433;database=FabricAdmin_Warehouse\"\n","\n","jdbc_properties = {\n","    \"user\": client_id,  # Using the client ID as the user\n","    \"password\": client_secret,  # Using the client secret as the password\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n","    \"authentication\": \"ActiveDirectoryServicePrincipal\",\n","    \"authenticationProperties\": f\"TenantId={tenant_id}\",\n","    \"socketTimeout\": \"30000\",\n","    \"loginTimeout\": \"30\"\n","}\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Data Warehouse using JDBC and SPN\") \\\n","    .getOrCreate()\n","\n","# Read data from the SQL Server table using JDBC\n","table_name = \"TestTable\"\n","df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","\n","# Write the DataFrame to the Lakehouse table using JDBC\n","spark_df.write.jdbc(url=jdbc_url, table='workspaces', mode='overwrite', properties=jdbc_properties)\n","\n","# Show the DataFrame\n","df.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3e0ac0f-ee62-4053-9975-2ab3af5f37b3"},{"cell_type":"code","source":["#Testing Warehouse WRITE by copying TestTable\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","\n","# Create the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# JDBC connection properties\n","#jdbc_url = \"jdbc:sqlserver://your_server_name.database.windows.net:1433;database=your_database_name\"\n","jdbc_url = \"jdbc:sqlserver://od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com:1433;database=FabricAdmin_Warehouse\"\n","\n","jdbc_properties = {\n","    \"user\": client_id,  # Using the client ID as the user\n","    \"password\": client_secret,  # Using the client secret as the password\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n","    \"authentication\": \"ActiveDirectoryServicePrincipal\",\n","    \"authenticationProperties\": f\"TenantId={tenant_id}\",\n","    \"socketTimeout\": \"30000\",\n","    \"loginTimeout\": \"30\"\n","}\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Data Warehouse using JDBC and SPN\") \\\n","    .getOrCreate()\n","\n","# Read data from the SQL Server table using JDBC\n","table_name = \"TestTable\"\n","df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","\n","# Write the DataFrame to the Lakehouse table using JDBC\n","spark_df.write.jdbc(url=jdbc_url, table='workspaces', mode='overwrite', properties=jdbc_properties)\n","\n","# Show the DataFrame\n","df.show()\n","\n","# Write the DataFrame to the Lakehouse table using JDBC\n","df.write.jdbc(url=jdbc_url, table='TestTable_Copy', mode='overwrite', properties=jdbc_properties)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02cd4a3b-475d-487e-9315-fde8b20c6879"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import time\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com'\n","database_name = 'FabricAdmin_Lakehouse'\n","table_name = 'TestTable'\n","\n","# Construct the JDBC URL with the correct properties\n","jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};authentication=ActiveDirectoryServicePrincipal;user={client_id};password={client_secret};encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.fabric.microsoft.com\"\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read data from Lakehouse using JDBC\") \\\n","    .getOrCreate()\n","\n","# JDBC connection properties\n","jdbc_properties = {\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n","}\n","\n","# Function to attempt connection with retry logic\n","def attempt_connection(retries=3, delay=5):\n","    for i in range(retries):\n","        try:\n","            df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","            return df\n","        except Exception as e:\n","            print(f\"Attempt {i+1} failed: {e}\")\n","            time.sleep(delay)\n","    raise Exception(\"All connection attempts failed\")\n","\n","# Attempt to connect and read data\n","try:\n","    df = attempt_connection()\n","    df.show()\n","except Exception as e:\n","    print(f\"Error: {e}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"22271f0c-9314-4ac3-b407-8f599c3967ad"},{"cell_type":"code","source":["#Testing Lakehouse READ parquet using abfss\n","# Onelake needs to be enabled to use abfss\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","import os\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","\n","# Initialize the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# Set environment variables for Spark to use the credentials\n","os.environ[\"SPARK_DATASOURCE_AZURE_USERNAME\"] = client_id\n","os.environ[\"SPARK_DATASOURCE_AZURE_PASSWORD\"] = client_secret\n","os.environ[\"SPARK_DATASOURCE_AZURE_TENANT_ID\"] = tenant_id\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Lakehouse using SPN\") \\\n","    .getOrCreate()\n","\n","#lakehouse_file_path = \"{Lakehouse_abfss_path}/Files/workspaces.parquet\"\n","#lakehouse_file_path = \"Files/workspaces.parquet\"\n","lakehouse_file_path = \"abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/workspaces.parquet\"\n","\n","# Define the path to your Lakehouse file and the table name\n","#table_name = \"workspaces\"\n","\n","# Read data from Lakehouse file\n","df = spark.read.parquet(lakehouse_file_path)\n","\n","# Show the DataFrame\n","df.show()\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3166f47-47e6-417a-ab29-3711576ec637"},{"cell_type":"code","source":["#Testing Lakehouse READ parquet using abfss and create a copy in parquet format\n","# Onelake needs to be enabled to use abfss\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","import os\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","\n","# Initialize the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# Set environment variables for Spark to use the credentials\n","os.environ[\"SPARK_DATASOURCE_AZURE_USERNAME\"] = client_id\n","os.environ[\"SPARK_DATASOURCE_AZURE_PASSWORD\"] = client_secret\n","os.environ[\"SPARK_DATASOURCE_AZURE_TENANT_ID\"] = tenant_id\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Lakehouse using SPN\") \\\n","    .getOrCreate()\n","\n","#lakehouse_file_path = \"{Lakehouse_abfss_path}/Files/workspaces.parquet\"\n","#lakehouse_file_path = \"Files/workspaces.parquet\"\n","lakehouse_file_path = \"abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/workspaces.parquet\"\n","\n","# Define the path to your Lakehouse file and the table name\n","#table_name = \"workspaces\"\n","\n","# Read data from Lakehouse file\n","df = spark.read.parquet(lakehouse_file_path)\n","\n","# Show the DataFrame\n","df.show()\n","\n","#create a copy and write as table\n","#df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspaces_copy\")\n","\n","# Write DataFrame to Lakehouse as Parquet file\n","df.write.mode(\"overwrite\").parquet(\"Files/workspaces_copy.parquet\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"12eab307-c54e-48e4-b8c7-db4748d1758c"},{"cell_type":"code","source":["#using jdbc on Lakehouse READ\n","\n","from pyspark.sql import SparkSession\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com'\n","#server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a'\n","\n","database_name = 'FabricAdmin_Lakehouse'\n","table_name = 'workspaces'\n","\n","# Construct the JDBC URL\n","jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};authentication=ActiveDirectoryServicePrincipal;user={client_id};password={client_secret};encrypt=true;trustServerCertificate=true;\"\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read data from Lakehouse using JDBC\") \\\n","    .getOrCreate()\n","\n","# JDBC connection properties\n","jdbc_properties = {\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n","}\n","\n","# Read data from the table using JDBC\n","df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","\n","# Show the DataFrame\n","df.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2176b710-4fe0-43cf-b2d9-2bb25c01c1b5"},{"cell_type":"code","source":["#using read csv and write to Lakehouse using abfss\n","\n","from pyspark.sql import SparkSession\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read CSV and Write Data to Microsoft Fabric Lakehouse using ABFSS and SPN\") \\\n","    .getOrCreate()\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","storage_account_name = 'onelake'\n","container_name = '7a21dc44-c8b8-446e-9e80-59458a88ece8'\n","directory_name = '51872361-e484-4aa7-a0b4-853eaa971e47/Files'\n","\n","# Set up the Hadoop configuration with SPN credentials\n","spark.conf.set(f'fs.azure.account.auth.type.{storage_account_name}.dfs.fabric.microsoft.com', 'OAuth')\n","spark.conf.set(f'fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.fabric.microsoft.com', 'org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider')\n","spark.conf.set(f'fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.fabric.microsoft.com', client_id)\n","spark.conf.set(f'fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.fabric.microsoft.com', client_secret)\n","spark.conf.set(f'fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.fabric.microsoft.com', f'https://login.microsoftonline.com/{tenant_id}/oauth2/token')\n","\n","# Read data from the CSV file\n","#csv_file_path = f\"abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/sales.csv\"\n","csv_file_path = f\"abfss://{container_name}@{storage_account_name}.dfs.fabric.microsoft.com/{directory_name}/sales2.csv\"\n","\n","# Read data from the CSV file\n","df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n","\n","# Show the DataFrame\n","df.show()\n","\n","# Optionally, display the schema\n","df.printSchema()\n","\n","# Define the path where you want to write the Parquet file in the Lakehouse\n","lakehouse_path = f\"abfss://{container_name}@{storage_account_name}.dfs.fabric.microsoft.com/{directory_name}/sales2.parquet\"\n","\n","# Write the DataFrame to the Lakehouse in Parquet format\n","df.write.mode('overwrite').parquet(lakehouse_path)\n","#df.write.mode(\"overwrite\").parquet(\"Files/sales2.parquet\")\n","\n","print(\"Data read from CSV and written to Lakehouse successfully!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"9cd93a4f-f2f1-4f46-8fae-ff15323c6a3b","normalized_state":"finished","queued_time":"2024-12-09T20:50:57.6720733Z","session_start_time":null,"execution_start_time":"2024-12-09T20:50:58.248964Z","execution_finish_time":"2024-12-09T20:51:00.8141382Z","parent_msg_id":"90751244-06e0-4976-9383-6e13f08de778"},"text/plain":"StatementMeta(, 9cd93a4f-f2f1-4f46-8fae-ff15323c6a3b, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+--------------------+---------+--------+---------+---------+\n|SalesOrderLineNumber|OrderDate|Quantity|UnitPrice|TaxAmount|\n+--------------------+---------+--------+---------+---------+\n|                   1| 7/1/2019|       1|  3399.99| 271.9992|\n|                   1| 7/1/2019|       1|  3374.99| 269.9992|\n|                   1| 7/1/2019|       1|  3399.99| 271.9992|\n|                   1| 7/1/2019|       1| 699.0982|  55.9279|\n|                   1| 7/1/2019|       1|  3578.27| 286.2616|\n|                   1| 7/1/2019|       1|  3578.27| 286.2616|\n|                   1| 7/1/2019|       1|  3399.99| 271.9992|\n|                   1| 7/1/2019|       1|  3578.27| 286.2616|\n|                   1| 7/1/2019|       1|  3399.99| 271.9992|\n|                   1| 7/2/2019|       1|  3578.27| 286.2616|\n|                   1| 7/2/2019|       1|  3578.27| 286.2616|\n|                   1| 7/2/2019|       1|  3578.27| 286.2616|\n|                   1| 7/2/2019|       1| 699.0982|  55.9279|\n|                   1| 7/2/2019|       1|  3578.27| 286.2616|\n|                   1| 7/2/2019|       1|  3578.27| 286.2616|\n|                   1| 7/2/2019|       1|  3578.27| 286.2616|\n|                   1| 7/3/2019|       1|  3578.27| 286.2616|\n|                   1| 7/3/2019|       1|  3578.27| 286.2616|\n|                   1| 7/3/2019|       1|  3578.27| 286.2616|\n|                   1| 7/3/2019|       1|  3578.27| 286.2616|\n+--------------------+---------+--------+---------+---------+\nonly showing top 20 rows\n\nroot\n |-- SalesOrderLineNumber: integer (nullable = true)\n |-- OrderDate: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- TaxAmount: double (nullable = true)\n\nData read from CSV and written to Lakehouse successfully!\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8155c7dd-59a6-490d-a925-2b5bc12e5d7d"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from azure.identity import DefaultAzureCredential\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read CSV and Write Data to Microsoft Fabric Lakehouse using ABFSS and DefaultAzureCredential\") \\\n","    .getOrCreate()\n","\n","# Create the DefaultAzureCredential\n","credential = DefaultAzureCredential()\n","\n","# Read data from the CSV file\n","csv_file_path = 'abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/sales.csv'\n","#csv_file_path = 'Files/sales.csv'\n","df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n","\n","df.write\n","\n","df.show()\n","\n","\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"41b0415d-fda0-4001-b57b-781c7fc7ae7e"},{"cell_type":"code","source":["#Get all workspaces even if workspace count is more than 10,000 rows using continuation_token.\n","\n","import pandas as pd\n","import sempy.fabric as fabric\n","from pyspark.sql import SparkSession\n","\n","# Initialize FabricRestClient for authentication\n","client = fabric.FabricRestClient()\n","\n","# Function to fetch all workspaces\n","def fetch_all_workspaces():\n","    all_data = []\n","    continuation_token = None\n","    \n","    while True:\n","        # Construct the URL\n","        if continuation_token:\n","            url = f\"v1/admin/workspaces?continuationToken={continuation_token}\"\n","        else:\n","            url = \"v1/admin/workspaces\"\n","        \n","        # Make API request to fetch workspaces\n","        response = client.get(url)\n","        data = response.json()\n","        \n","        # Debug: Print the response data to check the structure\n","        #print(\"Response data:\", data)\n","        \n","        # Append the data if the structure is correct\n","        if 'workspaces' in data:\n","            all_data.extend(data['workspaces'])\n","        else:\n","            print(\"Key 'workspaces' not found in response\")\n","            break\n","        \n","        # Check for continuation token\n","        continuation_token = data.get('continuationToken')\n","        if not continuation_token:\n","            break\n","    \n","    return pd.DataFrame(all_data)\n","\n","# Fetch workspaces and display DataFrame\n","df = fetch_all_workspaces()\n","print(f\"Total number of rows: {df.shape[0]}\")\n","#print(df)\n","\n","# Rename columns\n","df.columns = ['WorkspaceID', 'WorkspaceName', 'WorkspaceStatus', 'WorkspaceType', 'CapacityID']\n","\n","# Display DataFrame with custom column names\n","#print(df)\n","#display(df)\n","\n","# Convert pandas DataFrame to PySpark DataFrame\n","spark_df = spark.createDataFrame(df)\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = '8ed7e13a-9a60-4048-baba-200faf9425b2'\n","client_secret = '8Dw8Q~mw7hFPziwDqzCJwylyOPfyGQnSK_0eodBc'\n","server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com'\n","#server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a'\n","\n","database_name = 'FabricAdmin_Lakehouse'\n","table_name = 'dbo.workspaces'\n","\n","# Construct the JDBC URL\n","jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};authentication=ActiveDirectoryServicePrincipal;user={client_id};password={client_secret};encrypt=true;trustServerCertificate=true;\"\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Write data into Lakehouse using JDBC\") \\\n","    .getOrCreate()\n","\n","# JDBC connection properties\n","jdbc_properties = {\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n","}\n","\n","# Write the DataFrame to the Lakehouse table using JDBC\n","spark_df.write.jdbc(url=jdbc_url, table=table_name, mode='overwrite', properties=jdbc_properties)\n","\n","print(\"Data written to Lakehouse successfully!\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d00df073-44ea-497f-b562-7e7e6317546d"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"warehouse":{}}},"nbformat":4,"nbformat_minor":5}