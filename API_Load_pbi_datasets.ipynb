{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Admin - Datasets GetDatasetsAsAdmin\n","# Command:  GET https://api.powerbi.com/v1.0/myorg/admin/datasets\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/power-bi/admin/datasets-get-datasets-as-admin\n","\n","# Loads table: pbi_datasets"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"e62ae15b-0278-4dbe-b75e-b37192526633","normalized_state":"finished","queued_time":"2025-07-17T15:20:47.3996969Z","session_start_time":"2025-07-17T15:20:47.400569Z","execution_start_time":"2025-07-17T15:21:01.3673921Z","execution_finish_time":"2025-07-17T15:21:01.792812Z","parent_msg_id":"39bfbd65-0124-4d59-a6b4-a902f1ba36d0"},"text/plain":"StatementMeta(, e62ae15b-0278-4dbe-b75e-b37192526633, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7acff316-6126-4fad-a25d-dbfc6ca5e980"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Power BI Datasets to Delta Lake - PySpark Notebook\n","# This notebook retrieves Power BI datasets using the GetDatasetsAsAdmin API and loads them into a Delta Lake table\n","# with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"PowerBIDatasetsToDeltalake\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.powerbi.com/v1.0/myorg\",\n","    \"DATASETS_ENDPOINT\": \"/admin/datasets\",  # GetDatasetsAsAdmin endpoint\n","    \"MAX_RETRIES\": 5,  # Number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"DATASETS_TABLE_NAME\": \"pbi_datasets\",  # Name of the target Delta table\n","    \"LAKEHOUSE_PATH\": \"Tables\",  # Default Tables folder in Fabric Lakehouse\n","    \"DEBUG_MODE\": True  # Set to True to enable extra debugging output\n","}\n","\n","# Note: The Power BI Admin API supports standard OData parameters ($top, $skip, $filter)\n","# We'll rely on @odata.nextLink for pagination as provided by the API\n","# No artificial record limits are imposed\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Power BI API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Power BI REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","        For Power BI API, we need a token with the scope for https://analysis.windows.net/powerbi/api\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        # Power BI API requires a specific resource/scope\n","        token_response = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_powerbi_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Power BI with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Power BI API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/admin/datasets\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            # Log the full URL with parameters for debugging\n","            logger.info(f\"Making API call to: {url} with params: {params} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Log the full response for debugging\n","            logger.info(f\"Response status: {response.status_code}\")\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # Log the response for debugging in case of errors\n","            if response.status_code >= 400:\n","                logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                logger.error(f\"Request URL: {response.request.url}\")\n","                logger.error(f\"Request headers: {response.request.headers}\")\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            # Log a sample of the response content for debugging\n","            try:\n","                response_json = response.json()\n","                if \"value\" in response_json and isinstance(response_json[\"value\"], list):\n","                    logger.info(f\"Response contains {len(response_json['value'])} items in 'value' array\")\n","                if \"@odata.nextLink\" in response_json:\n","                    logger.info(f\"Response contains @odata.nextLink for pagination\")\n","                return response_json\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Failed to parse response as JSON: {str(e)}\")\n","                logger.error(f\"Response content: {response.text[:1000]}\")  # Log first 1000 chars of response\n","                raise\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Get Datasets Function\n","# ==================================\n","def get_datasets(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all datasets from the Power BI Admin API, handling pagination.\n","    \n","    This function makes requests to the GetDatasetsAsAdmin API endpoint and\n","    handles pagination using the @odata.nextLink to retrieve all datasets.\n","    \n","    According to the API documentation, this endpoint returns datasets from all workspaces\n","    that the calling user has admin permissions on.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all dataset objects\n","    \"\"\"\n","    all_datasets = []\n","    next_link = None\n","    page_count = 0\n","    \n","    while True:\n","        page_count += 1\n","        \n","        if next_link:\n","            # Use the @odata.nextLink URL directly for subsequent pages\n","            url = next_link\n","            \n","            if CONFIG['DEBUG_MODE']:\n","                logger.info(f\"Page {page_count}: Making request with next link: {url}\")\n","            \n","            # Make direct API call with proper headers and retry logic\n","            headers = {\n","                \"Authorization\": f\"Bearer {access_token}\",\n","                \"Content-Type\": \"application/json\"\n","            }\n","            \n","            # Use the same retry logic as in call_powerbi_api\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            for attempt in range(CONFIG['MAX_RETRIES']):\n","                try:\n","                    logger.info(f\"Making API call to: {url} (Attempt {attempt + 1})\")\n","                    \n","                    response = requests.get(\n","                        url,\n","                        headers=headers,\n","                        timeout=CONFIG['TIMEOUT']\n","                    )\n","                    \n","                    logger.info(f\"Response status: {response.status_code}\")\n","                    \n","                    # Handle rate limiting\n","                    if response.status_code == 429:\n","                        retry_after = response.headers.get('Retry-After')\n","                        if retry_after and retry_after.isdigit():\n","                            wait_time = int(retry_after)\n","                        else:\n","                            jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                            wait_time = backoff_time + jitter\n","                            backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                        \n","                        logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                        time.sleep(wait_time)\n","                        continue\n","                    \n","                    # Log errors\n","                    if response.status_code >= 400:\n","                        logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                        logger.error(f\"Request URL: {response.request.url}\")\n","                    \n","                    response.raise_for_status()\n","                    response_data = response.json()\n","                    break  # Success, exit retry loop\n","                    \n","                except requests.exceptions.RequestException as e:\n","                    if attempt == CONFIG['MAX_RETRIES'] - 1:\n","                        logger.error(f\"All retry attempts failed for page {page_count}\")\n","                        raise\n","                    \n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                    \n","                    logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                    logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                    time.sleep(wait_time)\n","        else:\n","            # First page - use the standard call_powerbi_api function\n","            # No artificial $top limit - let the API return its natural page size\n","            \n","            if CONFIG['DEBUG_MODE']:\n","                logger.info(f\"Page {page_count}: Making initial request without $top limit\")\n","            \n","            try:\n","                response_data = call_powerbi_api(CONFIG['DATASETS_ENDPOINT'], access_token)\n","            except requests.exceptions.RequestException as e:\n","                logger.error(f\"API call failed on page {page_count}: {str(e)}\")\n","                raise\n","        \n","        # Log the response structure for debugging\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(f\"Response keys: {list(response_data.keys())}\")\n","        \n","        # Extract datasets from the response\n","        # Power BI API returns datasets in the \"value\" array\n","        datasets = response_data.get(\"value\", [])\n","        \n","        if datasets:\n","            all_datasets.extend(datasets)\n","            logger.info(f\"Retrieved {len(datasets)} datasets on page {page_count}. Running total: {len(all_datasets)}\")\n","            \n","            # Log first dataset for debugging\n","            if CONFIG['DEBUG_MODE'] and datasets:\n","                logger.info(f\"Sample dataset keys: {list(datasets[0].keys())}\")\n","        else:\n","            logger.warning(f\"No datasets found on page {page_count}\")\n","        \n","        # Check if there are more pages using @odata.nextLink\n","        next_link = response_data.get(\"@odata.nextLink\")\n","        \n","        if next_link:\n","            logger.info(f\"Found @odata.nextLink for pagination\")\n","        else:\n","            logger.info(\"No @odata.nextLink found - this is the last page\")\n","            break\n","    \n","    logger.info(f\"Finished retrieving all datasets. Total count: {len(all_datasets)}\")\n","    return all_datasets\n","# ==================================\n","\n","\n","# CELL 8 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_datasets_dataframe(datasets_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the datasets data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the dataset data\n","    - Extracts only the required fields as specified in the requirements\n","    - Handles missing fields gracefully with proper data types\n","    - Prevents VOID column issues in Delta Lake\n","    - Adds metadata columns for tracking\n","    \n","    Based on the Power BI API documentation and the provided image, we extract these fields:\n","    - isEffectiveIdentityRequired (boolean)\n","    - isEffectiveIdentityRolesRequired (boolean) \n","    - isInPlaceSharingEnabled (boolean)\n","    - isOnPremGatewayRequired (boolean)\n","    - isRefreshable (boolean)\n","    - addRowsAPIEnabled (boolean)\n","    - configuredBy (string)\n","    - createReportEmbedURL (string)\n","    - createdDate (string/datetime)\n","    - description (string)\n","    - id (string)\n","    - name (string)\n","    - qnaEmbedURL (string)\n","    - targetStorageMode (string)\n","    - webUrl (string)\n","    - workspaceId (string)\n","    \n","    Args:\n","        datasets_data: List of dataset dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Define the target schema with explicit data types to prevent VOID issues\n","    target_schema = StructType([\n","        StructField(\"isEffectiveIdentityRequired\", BooleanType(), True),\n","        StructField(\"isEffectiveIdentityRolesRequired\", BooleanType(), True),\n","        StructField(\"isInPlaceSharingEnabled\", BooleanType(), True),\n","        StructField(\"isOnPremGatewayRequired\", BooleanType(), True),\n","        StructField(\"isRefreshable\", BooleanType(), True),\n","        StructField(\"addRowsAPIEnabled\", BooleanType(), True),\n","        StructField(\"configuredBy\", StringType(), True),\n","        StructField(\"createReportEmbedURL\", StringType(), True),\n","        StructField(\"createdDate\", StringType(), True),\n","        StructField(\"description\", StringType(), True),\n","        StructField(\"id\", StringType(), True),  # Changed to nullable to handle missing IDs\n","        StructField(\"name\", StringType(), True),\n","        StructField(\"qnaEmbedURL\", StringType(), True),\n","        StructField(\"targetStorageMode\", StringType(), True),\n","        StructField(\"webUrl\", StringType(), True),\n","        StructField(\"workspaceId\", StringType(), True),\n","        StructField(\"extraction_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # First, let's examine what fields are actually available in the data\n","    if datasets_data and CONFIG['DEBUG_MODE']:\n","        sample_dataset = datasets_data[0]\n","        logger.info(f\"Available fields in dataset: {list(sample_dataset.keys())}\")\n","    \n","    # Convert the data to a DataFrame\n","    if not datasets_data:\n","        logger.warning(\"No datasets found. Creating empty DataFrame with predefined schema.\")\n","        # Create an empty DataFrame with the target schema\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        enhanced_df = spark.createDataFrame(empty_rdd, target_schema)\n","        return enhanced_df\n","    \n","    # Create structured data with explicit type handling to prevent VOID columns\n","    structured_data = []\n","    \n","    # Get current timestamp once to use for all records\n","    from datetime import datetime\n","    current_ts = datetime.now()\n","    \n","    for dataset in datasets_data:\n","        # Create a record with explicit type casting and defaults\n","        record = []\n","        \n","        # Boolean fields - convert None to False or handle properly\n","        boolean_fields = [\n","            \"isEffectiveIdentityRequired\", \"isEffectiveIdentityRolesRequired\", \n","            \"isInPlaceSharingEnabled\", \"isOnPremGatewayRequired\", \"isRefreshable\", \"addRowsAPIEnabled\"\n","        ]\n","        \n","        for field in boolean_fields:\n","            value = dataset.get(field)\n","            # Convert to proper boolean or None (not VOID)\n","            if value is None:\n","                record.append(None)\n","            elif isinstance(value, bool):\n","                record.append(value)\n","            elif isinstance(value, str):\n","                record.append(value.lower() in ['true', '1', 'yes'])\n","            else:\n","                record.append(bool(value))\n","        \n","        # String fields - ensure they're strings or None, with validation for ID\n","        string_fields = [\n","            \"configuredBy\", \"createReportEmbedURL\", \"createdDate\", \"description\", \n","            \"id\", \"name\", \"qnaEmbedURL\", \"targetStorageMode\", \"webUrl\", \"workspaceId\"\n","        ]\n","        \n","        for field in string_fields:\n","            value = dataset.get(field)\n","            if value is None:\n","                # For ID field, generate a placeholder if missing\n","                if field == \"id\":\n","                    logger.warning(f\"Missing ID for dataset: {dataset.get('name', 'Unknown')}\")\n","                    record.append(f\"missing_id_{len(structured_data)}\")  # Generate unique placeholder\n","                else:\n","                    record.append(None)\n","            else:\n","                record.append(str(value))\n","        \n","        # Add extraction timestamp as actual datetime object (not None)\n","        record.append(current_ts)\n","        \n","        structured_data.append(record)\n","    \n","    # Log the data we're about to create\n","    if CONFIG['DEBUG_MODE']:\n","        logger.info(f\"Creating DataFrame with {len(structured_data)} records\")\n","        if structured_data:\n","            logger.info(f\"Sample record: {structured_data[0]}\")\n","    \n","    # Create the Spark DataFrame with explicit schema\n","    if structured_data:\n","        spark_df = spark.createDataFrame(structured_data, target_schema)\n","        # The timestamp is already set during record creation, so no need to add current_timestamp()\n","        enhanced_df = spark_df\n","    else:\n","        # Create empty DataFrame with schema\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        spark_df = spark.createDataFrame(empty_rdd, target_schema)\n","        enhanced_df = spark_df\n","    \n","    # Log final schema\n","    if CONFIG['DEBUG_MODE']:\n","        logger.info(\"Final DataFrame schema:\")\n","        enhanced_df.printSchema()\n","        \n","        # Show a sample of the data\n","        logger.info(\"Sample data:\")\n","        enhanced_df.show(2, truncate=False)\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 9 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it with the proper schema if necessary.\n","    \n","    This function handles VOID column issues by ensuring the table is created\n","    with the correct schema before any data operations.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        existing_table = spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","        \n","        # Log the existing schema for comparison\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(\"Existing table schema:\")\n","            existing_table.show(truncate=False)\n","        \n","    except Exception:\n","        # Table doesn't exist, create it with explicit schema\n","        logger.info(f\"Creating Delta table '{table_name}' with explicit schema\")\n","        \n","        # Create an empty DataFrame with the target schema to prevent VOID issues\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table with schema enforcement\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .option(\"mergeSchema\", \"false\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully with explicit schema\")\n","        \n","        # Verify the table was created correctly\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(\"Created table schema:\")\n","            spark.sql(f\"DESCRIBE TABLE {table_name}\").show(truncate=False)\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new dataset data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation with explicit handling for VOID columns:\n","    - Updates existing records if dataset ID matches\n","    - Inserts new records if dataset ID doesn't exist\n","    - Handles schema compatibility issues\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Ensure both DataFrames have compatible schemas\n","    target_df = spark.table(table_name)\n","    \n","    if CONFIG['DEBUG_MODE']:\n","        logger.info(\"Source DataFrame schema:\")\n","        source_df.printSchema()\n","        logger.info(\"Target table schema:\")\n","        target_df.printSchema()\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"dataset_updates\")\n","    \n","    # Check if the table is empty to decide on strategy\n","    target_count = target_df.count()\n","    \n","    if target_count == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records directly.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        logger.info(\"Direct insert completed successfully\")\n","        return\n","    \n","    # For non-empty tables, perform merge with explicit column handling\n","    try:\n","        # Perform the merge operation with explicit column references\n","        merge_query = f\"\"\"\n","        MERGE INTO {table_name} AS target\n","        USING dataset_updates AS source\n","        ON target.id = source.id\n","        WHEN MATCHED THEN\n","            UPDATE SET \n","                target.isEffectiveIdentityRequired = source.isEffectiveIdentityRequired,\n","                target.isEffectiveIdentityRolesRequired = source.isEffectiveIdentityRolesRequired,\n","                target.isInPlaceSharingEnabled = source.isInPlaceSharingEnabled,\n","                target.isOnPremGatewayRequired = source.isOnPremGatewayRequired,\n","                target.isRefreshable = source.isRefreshable,\n","                target.addRowsAPIEnabled = source.addRowsAPIEnabled,\n","                target.configuredBy = source.configuredBy,\n","                target.createReportEmbedURL = source.createReportEmbedURL,\n","                target.createdDate = source.createdDate,\n","                target.description = source.description,\n","                target.name = source.name,\n","                target.qnaEmbedURL = source.qnaEmbedURL,\n","                target.targetStorageMode = source.targetStorageMode,\n","                target.webUrl = source.webUrl,\n","                target.workspaceId = source.workspaceId,\n","                target.extraction_timestamp = source.extraction_timestamp\n","        WHEN NOT MATCHED THEN\n","            INSERT (\n","                isEffectiveIdentityRequired, isEffectiveIdentityRolesRequired, \n","                isInPlaceSharingEnabled, isOnPremGatewayRequired, isRefreshable,\n","                addRowsAPIEnabled, configuredBy, createReportEmbedURL, \n","                createdDate, description, id, name, qnaEmbedURL,\n","                targetStorageMode, webUrl, workspaceId, extraction_timestamp\n","            )\n","            VALUES (\n","                source.isEffectiveIdentityRequired, source.isEffectiveIdentityRolesRequired,\n","                source.isInPlaceSharingEnabled, source.isOnPremGatewayRequired, source.isRefreshable,\n","                source.addRowsAPIEnabled, source.configuredBy, source.createReportEmbedURL,\n","                source.createdDate, source.description, source.id, source.name, source.qnaEmbedURL,\n","                source.targetStorageMode, source.webUrl, source.workspaceId, source.extraction_timestamp\n","            )\n","        \"\"\"\n","        \n","        spark.sql(merge_query)\n","        logger.info(\"Merge operation completed successfully\")\n","        \n","    except Exception as e:\n","        logger.error(f\"Merge operation failed: {str(e)}\")\n","        logger.info(\"Attempting fallback approach with INSERT OVERWRITE\")\n","        \n","        # Fallback: Overwrite the entire table (for smaller datasets)\n","        try:\n","            source_df.write \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .saveAsTable(table_name)\n","            logger.info(\"Fallback INSERT OVERWRITE completed successfully\")\n","        except Exception as fallback_error:\n","            logger.error(f\"Fallback approach also failed: {str(fallback_error)}\")\n","            raise\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Updates table statistics for query optimization\n","    - Uses a more compatible method for Microsoft Fabric\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # Note: In Microsoft Fabric, Delta table optimization may be handled automatically\n","        # or through different commands than traditional Delta Lake\n","        \n","        # Alternative approach: Use Delta table properties to hint at optimization\n","        delta_table = DeltaTable.forName(spark, table_name)\n","        delta_table_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n","        \n","        logger.info(\"Delta table optimization completed via statistics computation\")\n","        logger.info(\"Note: Microsoft Fabric may automatically optimize Delta tables\")\n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 10 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all datasets from the Power BI Admin API\n","    3. Creates an enhanced PySpark DataFrame with the dataset data\n","    4. Loads data into a Delta Lake table\n","    5. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Power BI Datasets to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve all datasets\n","        logger.info(\"Retrieving datasets from Power BI Admin API...\")\n","        datasets_data = get_datasets(access_token)\n","        \n","        if not datasets_data:\n","            logger.warning(\"No datasets found. Please check your permissions and API access.\")\n","            # Create empty dataframe with schema for consistent table structure\n","            empty_schema = StructType([\n","                StructField(\"isEffectiveIdentityRequired\", BooleanType(), True),\n","                StructField(\"isEffectiveIdentityRolesRequired\", BooleanType(), True),\n","                StructField(\"isInPlaceSharingEnabled\", BooleanType(), True),\n","                StructField(\"isOnPremGatewayRequired\", BooleanType(), True),\n","                StructField(\"isRefreshable\", BooleanType(), True),\n","                StructField(\"addRowsAPIEnabled\", BooleanType(), True),\n","                StructField(\"configuredBy\", StringType(), True),\n","                StructField(\"createReportEmbedURL\", StringType(), True),\n","                StructField(\"createdDate\", StringType(), True),\n","                StructField(\"description\", StringType(), True),\n","                StructField(\"id\", StringType(), True),\n","                StructField(\"name\", StringType(), True),\n","                StructField(\"qnaEmbedURL\", StringType(), True),\n","                StructField(\"targetStorageMode\", StringType(), True),\n","                StructField(\"webUrl\", StringType(), True),\n","                StructField(\"workspaceId\", StringType(), True),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            datasets_df = spark.createDataFrame([], empty_schema)\n","        else:\n","            # Step 3: Create enhanced DataFrame\n","            logger.info(f\"Creating DataFrame for {len(datasets_data)} datasets...\")\n","            datasets_df = create_enhanced_datasets_dataframe(datasets_data)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of enhanced datasets data:\")\n","        datasets_df.show(5, truncate=False)\n","        \n","        # Step 4: Prepare Delta table\n","        table_name = CONFIG[\"DATASETS_TABLE_NAME\"]\n","        ensure_delta_table_exists(table_name, datasets_df.schema)\n","        \n","        # Step 5: Merge data into Delta table (if we have data)\n","        if datasets_data:\n","            merge_data_to_delta(datasets_df, table_name)\n","            \n","            # Step 6: Optimize the Delta table\n","            optimize_delta_table(table_name)\n","        \n","        # Step 7: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        spark.sql(f\"DESCRIBE DETAIL {table_name}\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"Total rows in {table_name}: {row_count}\")\n","        \n","        # Show summary statistics\n","        summary_stats = spark.sql(f\"\"\"\n","            SELECT \n","                COUNT(*) as total_datasets,\n","                COUNT(DISTINCT workspaceId) as unique_workspaces,\n","                COUNT(DISTINCT configuredBy) as unique_owners,\n","                COUNT(DISTINCT targetStorageMode) as storage_modes,\n","                SUM(CASE WHEN isRefreshable = true THEN 1 ELSE 0 END) as refreshable_datasets,\n","                SUM(CASE WHEN isOnPremGatewayRequired = true THEN 1 ELSE 0 END) as onprem_gateway_required,\n","                MAX(extraction_timestamp) as last_updated\n","            FROM {table_name}\n","            WHERE 1=1  -- Always true condition to make query more robust\n","        \"\"\")\n","        \n","        logger.info(\"Summary statistics:\")\n","        try:\n","            summary_stats.show(truncate=False)\n","        except Exception as e:\n","            logger.warning(f\"Could not display summary statistics: {str(e)}\")\n","            # Try a simpler query\n","            simple_stats = spark.sql(f\"SELECT COUNT(*) as total_datasets FROM {table_name}\")\n","            simple_stats.show()\n","        \n","        # Optional: Show distribution by storage mode (only if column exists)\n","        try:\n","            storage_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    targetStorageMode,\n","                    COUNT(*) as count\n","                FROM {table_name}\n","                WHERE targetStorageMode IS NOT NULL\n","                GROUP BY targetStorageMode\n","                ORDER BY count DESC\n","            \"\"\")\n","            \n","            logger.info(\"Dataset distribution by storage mode:\")\n","            storage_distribution.show(truncate=False)\n","        except Exception as e:\n","            logger.warning(f\"Could not display storage mode distribution: {str(e)}\")\n","        \n","        # Optional: Show refreshable vs non-refreshable datasets (only if column exists)\n","        try:\n","            refresh_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    isRefreshable,\n","                    COUNT(*) as count\n","                FROM {table_name}\n","                GROUP BY isRefreshable\n","                ORDER BY count DESC\n","            \"\"\")\n","            \n","            logger.info(\"Dataset distribution by refresh capability:\")\n","            refresh_distribution.show(truncate=False)\n","        except Exception as e:\n","            logger.warning(f\"Could not display refresh distribution: {str(e)}\")\n","        \n","        # Show the actual columns in the table\n","        logger.info(\"Actual table schema:\")\n","        spark.sql(f\"DESCRIBE {table_name}\").show(truncate=False)\n","        \n","        return datasets_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 11 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    datasets_df = main()\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"e62ae15b-0278-4dbe-b75e-b37192526633","normalized_state":"finished","queued_time":"2025-07-17T15:20:47.4529393Z","session_start_time":null,"execution_start_time":"2025-07-17T15:21:01.7973073Z","execution_finish_time":"2025-07-17T15:21:43.1462614Z","parent_msg_id":"7f7490db-0c47-43d6-9683-1c6243724357"},"text/plain":"StatementMeta(, e62ae15b-0278-4dbe-b75e-b37192526633, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-17 15:21:02,383 - INFO - Starting Power BI Datasets to Delta Lake process\n2025-07-17 15:21:02,383 - INFO - Getting access token...\n2025-07-17 15:21:02,398 - INFO - Successfully obtained access token\n2025-07-17 15:21:02,399 - INFO - Retrieving datasets from Power BI Admin API...\n2025-07-17 15:21:02,400 - INFO - Page 1: Making initial request without $top limit\n2025-07-17 15:21:02,400 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/admin/datasets with params: None (Attempt 1)\n2025-07-17 15:21:32,820 - INFO - Merge operation completed successfully\n2025-07-17 15:21:32,821 - INFO - Optimizing Delta table 'pbi_datasets'\n2025-07-17 15:21:36,103 - INFO - Table statistics updated successfully\n2025-07-17 15:21:37,450 - INFO - Total rows in pbi_datasets: 11460\n2025-07-17 15:21:37,661 - INFO - Summary statistics:\n2025-07-17 15:21:40,777 - INFO - Actual table schema:\n"]},{"output_type":"stream","name":"stdout","text":["root\n |-- isEffectiveIdentityRequired: boolean (nullable = true)\n |-- isEffectiveIdentityRolesRequired: boolean (nullable = true)\n |-- isInPlaceSharingEnabled: boolean (nullable = true)\n |-- isOnPremGatewayRequired: boolean (nullable = true)\n |-- isRefreshable: boolean (nullable = true)\n |-- addRowsAPIEnabled: boolean (nullable = true)\n |-- configuredBy: string (nullable = true)\n |-- createReportEmbedURL: string (nullable = true)\n |-- createdDate: string (nullable = true)\n |-- description: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- qnaEmbedURL: string (nullable = true)\n |-- targetStorageMode: string (nullable = true)\n |-- webUrl: string (nullable = true)\n |-- workspaceId: string (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = false)\n\n+---------------------------+--------------------------------+-----------------------+-----------------------+-------------+-----------------+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+-----------+------------------------------------+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------+--------------------------+\n|isEffectiveIdentityRequired|isEffectiveIdentityRolesRequired|isInPlaceSharingEnabled|isOnPremGatewayRequired|isRefreshable|addRowsAPIEnabled|configuredBy            |createReportEmbedURL                                                                                                                                                                                       |createdDate             |description|id                                  |name                            |qnaEmbedURL                                                                                                                                                                                             |targetStorageMode|webUrl                                                                                                           |workspaceId                         |extraction_timestamp      |\n+---------------------------+--------------------------------+-----------------------+-----------------------+-------------+-----------------+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+-----------+------------------------------------+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------+--------------------------+\n|true                       |true                            |false                  |NULL                   |true         |false            |KTHodges@mdanderson.org |https://app.powerbi.com/reportEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|2021-06-21T19:32:21.137Z|NULL       |6df3a43c-83a0-4586-ab8e-f8802edd4904|APM Productivity                |https://app.powerbi.com/qnaEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|Abf              |https://app.powerbi.com/groups/c259764b-c0ea-4621-a223-4aafc97bd745/datasets/6df3a43c-83a0-4586-ab8e-f8802edd4904|c259764b-c0ea-4621-a223-4aafc97bd745|2025-07-17 15:21:07.306172|\n|false                      |false                           |false                  |NULL                   |true         |false            |RAHaynes1@mdanderson.org|https://app.powerbi.com/reportEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|2021-06-22T16:25:28.97Z |NULL       |2a471b47-2f42-4f66-9c7a-fa088e6529c3|FCT 3.5007 Faculty Lounge Access|https://app.powerbi.com/qnaEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|Abf              |https://app.powerbi.com/groups/5cfe16ba-338b-4fb2-a7a3-ed2c84eebf6c/datasets/2a471b47-2f42-4f66-9c7a-fa088e6529c3|5cfe16ba-338b-4fb2-a7a3-ed2c84eebf6c|2025-07-17 15:21:07.306172|\n+---------------------------+--------------------------------+-----------------------+-----------------------+-------------+-----------------+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+-----------+------------------------------------+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------+--------------------------+\nonly showing top 2 rows\n\n+---------------------------+--------------------------------+-----------------------+-----------------------+-------------+-----------------+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+-----------+------------------------------------+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------+--------------------------+\n|isEffectiveIdentityRequired|isEffectiveIdentityRolesRequired|isInPlaceSharingEnabled|isOnPremGatewayRequired|isRefreshable|addRowsAPIEnabled|configuredBy            |createReportEmbedURL                                                                                                                                                                                       |createdDate             |description|id                                  |name                            |qnaEmbedURL                                                                                                                                                                                             |targetStorageMode|webUrl                                                                                                           |workspaceId                         |extraction_timestamp      |\n+---------------------------+--------------------------------+-----------------------+-----------------------+-------------+-----------------+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+-----------+------------------------------------+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------+--------------------------+\n|true                       |true                            |false                  |NULL                   |true         |false            |KTHodges@mdanderson.org |https://app.powerbi.com/reportEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|2021-06-21T19:32:21.137Z|NULL       |6df3a43c-83a0-4586-ab8e-f8802edd4904|APM Productivity                |https://app.powerbi.com/qnaEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|Abf              |https://app.powerbi.com/groups/c259764b-c0ea-4621-a223-4aafc97bd745/datasets/6df3a43c-83a0-4586-ab8e-f8802edd4904|c259764b-c0ea-4621-a223-4aafc97bd745|2025-07-17 15:21:07.306172|\n|false                      |false                           |false                  |NULL                   |true         |false            |RAHaynes1@mdanderson.org|https://app.powerbi.com/reportEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|2021-06-22T16:25:28.97Z |NULL       |2a471b47-2f42-4f66-9c7a-fa088e6529c3|FCT 3.5007 Faculty Lounge Access|https://app.powerbi.com/qnaEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|Abf              |https://app.powerbi.com/groups/5cfe16ba-338b-4fb2-a7a3-ed2c84eebf6c/datasets/2a471b47-2f42-4f66-9c7a-fa088e6529c3|5cfe16ba-338b-4fb2-a7a3-ed2c84eebf6c|2025-07-17 15:21:07.306172|\n|false                      |false                           |false                  |NULL                   |false        |false            |DAVidro@mdanderson.org  |https://app.powerbi.com/reportEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|2021-06-22T19:39:13.42Z |NULL       |b5adcf99-1c4b-4a06-be96-38f145b40582|Dashboard Usage Metrics Model   |https://app.powerbi.com/qnaEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|Abf              |https://app.powerbi.com/groups/610c8e42-3219-4576-b68c-39f83224b123/datasets/b5adcf99-1c4b-4a06-be96-38f145b40582|610c8e42-3219-4576-b68c-39f83224b123|2025-07-17 15:21:07.306172|\n|false                      |false                           |false                  |NULL                   |true         |false            |rhgeorges@mdanderson.org|https://app.powerbi.com/reportEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|2021-06-23T19:58:26.557Z|NULL       |4f7368eb-3ca6-40b6-bf91-0d2e7640984d|Table                           |https://app.powerbi.com/qnaEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|Abf              |https://app.powerbi.com/groups/ce5d4614-c762-4760-8878-437b3ca46a10/datasets/4f7368eb-3ca6-40b6-bf91-0d2e7640984d|ce5d4614-c762-4760-8878-437b3ca46a10|2025-07-17 15:21:07.306172|\n|false                      |false                           |false                  |NULL                   |true         |false            |KATurner1@mdanderson.org|https://app.powerbi.com/reportEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|2021-06-23T20:12:52.5Z  |NULL       |61c35a93-811d-4848-bcb1-bc5eadc85b0d|Table                           |https://app.powerbi.com/qnaEmbed?config=eyJjbHVzdGVyVXJsIjoiaHR0cHM6Ly9XQUJJLVdFU1QtVVMtQi1QUklNQVJZLXJlZGlyZWN0LmFuYWx5c2lzLndpbmRvd3MubmV0IiwiZW1iZWRGZWF0dXJlcyI6eyJ1c2FnZU1ldHJpY3NWTmV4dCI6dHJ1ZX19|Abf              |https://app.powerbi.com/groups/c81b0a47-a358-41e1-9ad3-5c35deeca6e7/datasets/61c35a93-811d-4848-bcb1-bc5eadc85b0d|c81b0a47-a358-41e1-9ad3-5c35deeca6e7|2025-07-17 15:21:07.306172|\n+---------------------------+--------------------------------+-----------------------+-----------------------+-------------+-----------------+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+-----------+------------------------------------+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------+--------------------------+\nonly showing top 5 rows\n\n+--------------------------------+---------+-------+\n|col_name                        |data_type|comment|\n+--------------------------------+---------+-------+\n|isEffectiveIdentityRequired     |boolean  |NULL   |\n|isEffectiveIdentityRolesRequired|boolean  |NULL   |\n|isInPlaceSharingEnabled         |boolean  |NULL   |\n|isOnPremGatewayRequired         |boolean  |NULL   |\n|isRefreshable                   |boolean  |NULL   |\n|addRowsAPIEnabled               |boolean  |NULL   |\n|configuredBy                    |string   |NULL   |\n|createReportEmbedURL            |string   |NULL   |\n|createdDate                     |string   |NULL   |\n|description                     |string   |NULL   |\n|id                              |string   |NULL   |\n|name                            |string   |NULL   |\n|qnaEmbedURL                     |string   |NULL   |\n|targetStorageMode               |string   |NULL   |\n|webUrl                          |string   |NULL   |\n|workspaceId                     |string   |NULL   |\n|extraction_timestamp            |timestamp|NULL   |\n+--------------------------------+---------+-------+\n\nroot\n |-- isEffectiveIdentityRequired: boolean (nullable = true)\n |-- isEffectiveIdentityRolesRequired: boolean (nullable = true)\n |-- isInPlaceSharingEnabled: boolean (nullable = true)\n |-- isOnPremGatewayRequired: boolean (nullable = true)\n |-- isRefreshable: boolean (nullable = true)\n |-- addRowsAPIEnabled: boolean (nullable = true)\n |-- configuredBy: string (nullable = true)\n |-- createReportEmbedURL: string (nullable = true)\n |-- createdDate: string (nullable = true)\n |-- description: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- qnaEmbedURL: string (nullable = true)\n |-- targetStorageMode: string (nullable = true)\n |-- webUrl: string (nullable = true)\n |-- workspaceId: string (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = false)\n\nroot\n |-- isEffectiveIdentityRequired: boolean (nullable = true)\n |-- isEffectiveIdentityRolesRequired: boolean (nullable = true)\n |-- isInPlaceSharingEnabled: boolean (nullable = true)\n |-- isOnPremGatewayRequired: boolean (nullable = true)\n |-- isRefreshable: boolean (nullable = true)\n |-- addRowsAPIEnabled: boolean (nullable = true)\n |-- configuredBy: string (nullable = true)\n |-- createReportEmbedURL: string (nullable = true)\n |-- createdDate: string (nullable = true)\n |-- description: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- qnaEmbedURL: string (nullable = true)\n |-- targetStorageMode: string (nullable = true)\n |-- webUrl: string (nullable = true)\n |-- workspaceId: string (nullable = true)\n |-- extraction_timestamp: timestamp (nullable = true)\n\n+-------------+-----+\n|isRefreshable|count|\n+-------------+-----+\n|true         |11093|\n|false        |367  |\n+-------------+-----+\n\n+--------------------------------+---------+-------+\n|col_name                        |data_type|comment|\n+--------------------------------+---------+-------+\n|isEffectiveIdentityRequired     |boolean  |NULL   |\n|isEffectiveIdentityRolesRequired|boolean  |NULL   |\n|isInPlaceSharingEnabled         |boolean  |NULL   |\n|isOnPremGatewayRequired         |boolean  |NULL   |\n|isRefreshable                   |boolean  |NULL   |\n|addRowsAPIEnabled               |boolean  |NULL   |\n|configuredBy                    |string   |NULL   |\n|createReportEmbedURL            |string   |NULL   |\n|createdDate                     |string   |NULL   |\n|description                     |string   |NULL   |\n|id                              |string   |NULL   |\n|name                            |string   |NULL   |\n|qnaEmbedURL                     |string   |NULL   |\n|targetStorageMode               |string   |NULL   |\n|webUrl                          |string   |NULL   |\n|workspaceId                     |string   |NULL   |\n|extraction_timestamp            |timestamp|NULL   |\n+--------------------------------+---------+-------+\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05009a46-9a89-4112-b9c1-dcd33611dc75"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE pbi_datasets\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"e62ae15b-0278-4dbe-b75e-b37192526633","normalized_state":"finished","queued_time":"2025-07-17T15:20:47.5120205Z","session_start_time":null,"execution_start_time":"2025-07-17T15:21:43.148173Z","execution_finish_time":"2025-07-17T15:21:43.9861285Z","parent_msg_id":"b1d76f7f-cd77-4922-b130-4648cd91ae31"},"text/plain":"StatementMeta(, e62ae15b-0278-4dbe-b75e-b37192526633, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6377627-2629-4243-a2ef-63394f9494c2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}