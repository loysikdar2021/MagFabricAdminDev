{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# ***IMPORTANT*** this is a work in progress! (WIP)  This is only running the API against a single workspace for testing purposes. \n","\n","# API Name: Workspaces - List Workspace Access Details\n","# Command:  GET https://api.fabric.microsoft.com/v1/admin/workspaces/{workspaceId}/users\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/fabric/admin/workspaces/list-workspace-access-details\n","\n","# Loads tables: fabric_workspaces_access_core\n","# Loads tables: fabric_workspaces_access_users\n","# Loads tables: fabric_workspaces_access_groups\n","# Loads tables: fabric_workspaces_access_service_principals\n","# Loads tables: fabric_workspaces_access_service_principal_profiles"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65f3f3b2-a346-41ff-9019-eb72f075b279"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Microsoft Fabric Workspace Access Details to Delta Lake - PySpark Notebook\n","# This notebook retrieves Microsoft Fabric workspace access details and loads them into Delta Lake tables\n","# with optimization for analytics workloads using a multi-table approach for different principal types\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json, when, isnotnull\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"FabricWorkspaceAccessDetailsToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.fabric.microsoft.com/v1\",\n","    \"WORKSPACE_ACCESS_ENDPOINT\": \"/admin/workspaces/{workspaceId}/users\",  # Endpoint for workspace access details\n","    \"MAX_RETRIES\": 5,  # Increased number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    # Multi-table approach for different principal types\n","    \"ACCESS_CORE_TABLE_NAME\": \"fabric_workspaces_access_core\",  # Core access table\n","    \"ACCESS_USERS_TABLE_NAME\": \"fabric_workspaces_access_users\",  # User details table\n","    \"ACCESS_GROUPS_TABLE_NAME\": \"fabric_workspaces_access_groups\",  # Group details table\n","    \"ACCESS_SERVICE_PRINCIPALS_TABLE_NAME\": \"fabric_workspaces_access_service_principals\",  # Service principal details table\n","    \"ACCESS_SERVICE_PRINCIPAL_PROFILES_TABLE_NAME\": \"fabric_workspaces_access_service_principal_profiles\",  # Service principal profile details table\n","    \"LAKEHOUSE_PATH\": \"Tables\",  # Default Tables folder in Fabric Lakehouse\n","    \"DEBUG_MODE\": True,  # Set to True to enable extra debugging output\n","    # Test workspace ID parameter - CHANGE THIS FOR TESTING\n","    \"TEST_WORKSPACE_ID\": \"7a21dc44-c8b8-446e-9e80-59458a88ece8\"  # Replace with actual workspace ID for testing\n","}\n","\n","# IMPORTANT: Update the TEST_WORKSPACE_ID above with a real workspace ID for testing\n","# You can get workspace IDs from the workspaces API or from the Fabric portal\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Fabric API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Fabric REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_fabric_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Microsoft Fabric with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Fabric API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/admin/workspaces/{workspaceId}/users\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            # Log the full URL with parameters for debugging\n","            logger.info(f\"Making API call to: {url} with params: {params} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Log the full response for debugging\n","            logger.info(f\"Response status: {response.status_code}\")\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # Log the response for debugging in case of errors\n","            if response.status_code >= 400:\n","                logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                logger.error(f\"Request URL: {response.request.url}\")\n","                logger.error(f\"Request headers: {response.request.headers}\")\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            # Log a sample of the response content for debugging\n","            try:\n","                response_json = response.json()\n","                if \"accessDetails\" in response_json and isinstance(response_json[\"accessDetails\"], list):\n","                    logger.info(f\"Response contains {len(response_json['accessDetails'])} items in 'accessDetails' array\")\n","                return response_json\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Failed to parse response as JSON: {str(e)}\")\n","                logger.error(f\"Response content: {response.text[:1000]}\")  # Log first 1000 chars of response\n","                raise\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Get Workspace Access Details Function\n","# ==================================\n","def get_workspace_access_details(access_token: str, workspace_id: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve workspace access details for a specific workspace from the Fabric API.\n","    \n","    This function makes a request to the List Workspace Access Details API endpoint\n","    to get all users, groups, and service principals that have access to the workspace.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","        workspace_id: The workspace ID to get access details for\n","    \n","    Returns:\n","        list: A list of all access detail objects containing principal and workspace access information\n","    \"\"\"\n","    logger.info(f\"Retrieving access details for workspace: {workspace_id}\")\n","    \n","    # Construct the endpoint with the workspace ID\n","    endpoint = CONFIG['WORKSPACE_ACCESS_ENDPOINT'].format(workspaceId=workspace_id)\n","    \n","    try:\n","        # Make the API call\n","        response_data = call_fabric_api(endpoint, access_token)\n","        \n","        # Log the response structure for debugging\n","        if CONFIG['DEBUG_MODE']:\n","            logger.info(f\"Response keys: {list(response_data.keys())}\")\n","        \n","        # Extract access details from the response\n","        access_details = response_data.get(\"accessDetails\", [])\n","        \n","        if access_details:\n","            logger.info(f\"Retrieved {len(access_details)} access details for workspace {workspace_id}\")\n","            \n","            # Log first access detail for debugging\n","            if CONFIG['DEBUG_MODE'] and access_details:\n","                logger.info(f\"Sample access detail: {json.dumps(access_details[0], indent=2)}\")\n","        else:\n","            logger.warning(f\"No access details found for workspace {workspace_id}\")\n","        \n","        return access_details\n","        \n","    except requests.exceptions.RequestException as e:\n","        logger.error(f\"API call failed for workspace {workspace_id}: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 8 - Create Core Access DataFrame Function\n","# ==================================\n","def create_core_access_dataframe(access_details: List[Dict], workspace_id: str) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the access details data into a core access PySpark DataFrame.\n","    \n","    This function creates the main access table containing the core information\n","    that's common across all principal types.\n","    \n","    Args:\n","        access_details: List of access detail dictionaries from the API\n","        workspace_id: The workspace ID this data relates to\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame with core access information\n","    \"\"\"\n","    logger.info(\"Creating core access DataFrame\")\n","    \n","    # Extract core access information from each access detail\n","    core_access_data = []\n","    \n","    for access_detail in access_details:\n","        principal = access_detail.get(\"principal\", {})\n","        workspace_access = access_detail.get(\"workspaceAccessDetails\", {})\n","        \n","        core_record = {\n","            \"workspace_id\": workspace_id,\n","            \"principal_id\": principal.get(\"id\"),\n","            \"principal_display_name\": principal.get(\"displayName\"),\n","            \"principal_type\": principal.get(\"type\"),\n","            \"workspace_role\": workspace_access.get(\"workspaceRole\"),\n","            \"workspace_type\": workspace_access.get(\"type\")\n","        }\n","        core_access_data.append(core_record)\n","    \n","    # Define the schema for the core access table\n","    schema = StructType([\n","        StructField(\"workspace_id\", StringType(), False),\n","        StructField(\"principal_id\", StringType(), False),\n","        StructField(\"principal_display_name\", StringType(), True),\n","        StructField(\"principal_type\", StringType(), False),\n","        StructField(\"workspace_role\", StringType(), True),\n","        StructField(\"workspace_type\", StringType(), True),\n","        StructField(\"last_updated_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Create DataFrame\n","    if not core_access_data:\n","        logger.warning(\"No access details found. Creating empty core DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        core_df = spark.createDataFrame(empty_rdd, schema)\n","        return core_df\n","    \n","    # Convert to pandas DataFrame first, then to Spark DataFrame\n","    pandas_df = pd.DataFrame(core_access_data)\n","    \n","    # Create the initial Spark DataFrame\n","    required_columns = [\"workspace_id\", \"principal_id\", \"principal_display_name\", \"principal_type\", \"workspace_role\", \"workspace_type\"]\n","    \n","    # Ensure all columns exist in the pandas DataFrame\n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","    \n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Add metadata column for tracking when this data was extracted\n","    enhanced_df = spark_df.withColumn(\"last_updated_timestamp\", current_timestamp())\n","    \n","    logger.info(f\"Created core access DataFrame with {enhanced_df.count()} records\")\n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 9 - Create Principal-Specific DataFrames Functions\n","# ==================================\n","def create_user_details_dataframe(access_details: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Create a DataFrame specifically for user principal details.\n","    \n","    Args:\n","        access_details: List of access detail dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame with user-specific details\n","    \"\"\"\n","    logger.info(\"Creating user details DataFrame\")\n","    \n","    user_details_data = []\n","    \n","    for access_detail in access_details:\n","        principal = access_detail.get(\"principal\", {})\n","        \n","        # Only process User principals\n","        if principal.get(\"type\") == \"User\":\n","            user_details = principal.get(\"userDetails\", {})\n","            \n","            user_record = {\n","                \"principal_id\": principal.get(\"id\"),\n","                \"user_principal_name\": user_details.get(\"userPrincipalName\")\n","            }\n","            user_details_data.append(user_record)\n","    \n","    # Define schema\n","    schema = StructType([\n","        StructField(\"principal_id\", StringType(), False),\n","        StructField(\"user_principal_name\", StringType(), True),\n","        StructField(\"last_updated_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Create DataFrame\n","    if not user_details_data:\n","        logger.info(\"No user principals found. Creating empty user details DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        user_df = spark.createDataFrame(empty_rdd, schema)\n","        return user_df\n","    \n","    pandas_df = pd.DataFrame(user_details_data)\n","    spark_df = spark.createDataFrame(pandas_df)\n","    enhanced_df = spark_df.withColumn(\"last_updated_timestamp\", current_timestamp())\n","    \n","    logger.info(f\"Created user details DataFrame with {enhanced_df.count()} records\")\n","    return enhanced_df\n","\n","\n","def create_group_details_dataframe(access_details: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Create a DataFrame specifically for group principal details.\n","    \n","    Args:\n","        access_details: List of access detail dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame with group-specific details\n","    \"\"\"\n","    logger.info(\"Creating group details DataFrame\")\n","    \n","    group_details_data = []\n","    \n","    for access_detail in access_details:\n","        principal = access_detail.get(\"principal\", {})\n","        \n","        # Only process Group principals\n","        if principal.get(\"type\") == \"Group\":\n","            group_details = principal.get(\"groupDetails\", {})\n","            \n","            group_record = {\n","                \"principal_id\": principal.get(\"id\"),\n","                \"group_type\": group_details.get(\"groupType\")\n","            }\n","            group_details_data.append(group_record)\n","    \n","    # Define schema\n","    schema = StructType([\n","        StructField(\"principal_id\", StringType(), False),\n","        StructField(\"group_type\", StringType(), True),\n","        StructField(\"last_updated_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Create DataFrame\n","    if not group_details_data:\n","        logger.info(\"No group principals found. Creating empty group details DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        group_df = spark.createDataFrame(empty_rdd, schema)\n","        return group_df\n","    \n","    pandas_df = pd.DataFrame(group_details_data)\n","    spark_df = spark.createDataFrame(pandas_df)\n","    enhanced_df = spark_df.withColumn(\"last_updated_timestamp\", current_timestamp())\n","    \n","    logger.info(f\"Created group details DataFrame with {enhanced_df.count()} records\")\n","    return enhanced_df\n","\n","\n","def create_service_principal_details_dataframe(access_details: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Create a DataFrame specifically for service principal details.\n","    \n","    Args:\n","        access_details: List of access detail dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame with service principal-specific details\n","    \"\"\"\n","    logger.info(\"Creating service principal details DataFrame\")\n","    \n","    sp_details_data = []\n","    \n","    for access_detail in access_details:\n","        principal = access_detail.get(\"principal\", {})\n","        \n","        # Only process ServicePrincipal principals\n","        if principal.get(\"type\") == \"ServicePrincipal\":\n","            sp_details = principal.get(\"servicePrincipalDetails\", {})\n","            \n","            sp_record = {\n","                \"principal_id\": principal.get(\"id\"),\n","                \"aad_app_id\": sp_details.get(\"aadAppId\")\n","            }\n","            sp_details_data.append(sp_record)\n","    \n","    # Define schema\n","    schema = StructType([\n","        StructField(\"principal_id\", StringType(), False),\n","        StructField(\"aad_app_id\", StringType(), True),\n","        StructField(\"last_updated_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Create DataFrame\n","    if not sp_details_data:\n","        logger.info(\"No service principals found. Creating empty service principal details DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        sp_df = spark.createDataFrame(empty_rdd, schema)\n","        return sp_df\n","    \n","    pandas_df = pd.DataFrame(sp_details_data)\n","    spark_df = spark.createDataFrame(pandas_df)\n","    enhanced_df = spark_df.withColumn(\"last_updated_timestamp\", current_timestamp())\n","    \n","    logger.info(f\"Created service principal details DataFrame with {enhanced_df.count()} records\")\n","    return enhanced_df\n","\n","\n","def create_service_principal_profile_details_dataframe(access_details: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Create a DataFrame specifically for service principal profile details.\n","    \n","    Args:\n","        access_details: List of access detail dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame with service principal profile-specific details\n","    \"\"\"\n","    logger.info(\"Creating service principal profile details DataFrame\")\n","    \n","    spp_details_data = []\n","    \n","    for access_detail in access_details:\n","        principal = access_detail.get(\"principal\", {})\n","        \n","        # Only process ServicePrincipalProfile principals\n","        if principal.get(\"type\") == \"ServicePrincipalProfile\":\n","            spp_details = principal.get(\"servicePrincipalProfileDetails\", {})\n","            parent_principal = spp_details.get(\"parentPrincipal\", {})\n","            \n","            spp_record = {\n","                \"principal_id\": principal.get(\"id\"),\n","                \"parent_principal_id\": parent_principal.get(\"id\"),\n","                \"parent_principal_display_name\": parent_principal.get(\"displayName\"),\n","                \"parent_principal_type\": parent_principal.get(\"type\")\n","            }\n","            spp_details_data.append(spp_record)\n","    \n","    # Define schema\n","    schema = StructType([\n","        StructField(\"principal_id\", StringType(), False),\n","        StructField(\"parent_principal_id\", StringType(), True),\n","        StructField(\"parent_principal_display_name\", StringType(), True),\n","        StructField(\"parent_principal_type\", StringType(), True),\n","        StructField(\"last_updated_timestamp\", TimestampType(), False)\n","    ])\n","    \n","    # Create DataFrame\n","    if not spp_details_data:\n","        logger.info(\"No service principal profiles found. Creating empty service principal profile details DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        spp_df = spark.createDataFrame(empty_rdd, schema)\n","        return spp_df\n","    \n","    pandas_df = pd.DataFrame(spp_details_data)\n","    spark_df = spark.createDataFrame(pandas_df)\n","    enhanced_df = spark_df.withColumn(\"last_updated_timestamp\", current_timestamp())\n","    \n","    logger.info(f\"Created service principal profile details DataFrame with {enhanced_df.count()} records\")\n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 10 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_core_access_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new core access data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if workspace_id + principal_id matches\n","    - Inserts new records if the combination doesn't exist\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"access_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    if spark.table(table_name).count() == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Perform the merge operation (composite key: workspace_id + principal_id)\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING access_updates AS source\n","    ON target.workspace_id = source.workspace_id AND target.principal_id = source.principal_id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.principal_display_name = source.principal_display_name,\n","            target.principal_type = source.principal_type,\n","            target.workspace_role = source.workspace_role,\n","            target.workspace_type = source.workspace_type,\n","            target.last_updated_timestamp = source.last_updated_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(\"Core access merge operation completed successfully\")\n","\n","\n","def merge_principal_details_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge principal-specific details into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation based on principal_id.\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    temp_view_name = f\"{table_name.replace('fabric_', '')}_updates\"\n","    source_df.createOrReplaceTempView(temp_view_name)\n","    \n","    # If the table is empty, just insert all records\n","    if spark.table(table_name).count() == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Get column names excluding principal_id and last_updated_timestamp for dynamic UPDATE SET\n","    columns = [col for col in source_df.columns if col not in ['principal_id', 'last_updated_timestamp']]\n","    update_set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in columns])\n","    update_set_clause += \", target.last_updated_timestamp = source.last_updated_timestamp\"\n","    \n","    # Perform the merge operation\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING {temp_view_name} AS source\n","    ON target.principal_id = source.principal_id\n","    WHEN MATCHED THEN\n","        UPDATE SET {update_set_clause}\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(f\"Principal details merge operation completed successfully for {table_name}\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(f\"Table statistics updated successfully for {table_name}\")\n","        \n","        logger.info(f\"Delta table optimization completed for {table_name}\")\n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue for {table_name}: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 11 - Main Execution Function\n","# ==================================\n","def main(workspace_id: str = None):\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves workspace access details from the API\n","    3. Creates multiple DataFrames for different principal types\n","    4. Loads data into Delta Lake tables using a multi-table approach\n","    5. Optimizes the tables for analytics\n","    \n","    Args:\n","        workspace_id: The workspace ID to process. If None, uses the test workspace ID from config.\n","    \"\"\"\n","    try:\n","        # Use provided workspace_id or fall back to test config\n","        target_workspace_id = workspace_id or CONFIG[\"TEST_WORKSPACE_ID\"]\n","        \n","        if target_workspace_id == \"your-workspace-id-here\":\n","            raise ValueError(\"Please update the TEST_WORKSPACE_ID in CONFIG or provide a workspace_id parameter\")\n","        \n","        logger.info(f\"Starting Fabric Workspace Access Details to Delta Lake process for workspace: {target_workspace_id}\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve workspace access details\n","        logger.info(f\"Retrieving access details for workspace {target_workspace_id}...\")\n","        access_details = get_workspace_access_details(access_token, target_workspace_id)\n","        \n","        if not access_details:\n","            logger.warning(\"No access details found. Please check your permissions and workspace ID.\")\n","            return None\n","        \n","        # Step 3: Create DataFrames for different principal types\n","        logger.info(\"Creating DataFrames for different principal types...\")\n","        \n","        # Core access DataFrame (always created)\n","        core_access_df = create_core_access_dataframe(access_details, target_workspace_id)\n","        \n","        # Principal-specific DataFrames (only created if data exists)\n","        user_details_df = create_user_details_dataframe(access_details)\n","        group_details_df = create_group_details_dataframe(access_details)\n","        sp_details_df = create_service_principal_details_dataframe(access_details)\n","        spp_details_df = create_service_principal_profile_details_dataframe(access_details)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of core access data:\")\n","        core_access_df.show(5, truncate=False)\n","        \n","        # Step 4: Prepare and load Delta tables\n","        table_configs = [\n","            (CONFIG[\"ACCESS_CORE_TABLE_NAME\"], core_access_df, merge_core_access_data_to_delta),\n","            (CONFIG[\"ACCESS_USERS_TABLE_NAME\"], user_details_df, merge_principal_details_to_delta),\n","            (CONFIG[\"ACCESS_GROUPS_TABLE_NAME\"], group_details_df, merge_principal_details_to_delta),\n","            (CONFIG[\"ACCESS_SERVICE_PRINCIPALS_TABLE_NAME\"], sp_details_df, merge_principal_details_to_delta),\n","            (CONFIG[\"ACCESS_SERVICE_PRINCIPAL_PROFILES_TABLE_NAME\"], spp_details_df, merge_principal_details_to_delta)\n","        ]\n","        \n","        for table_name, df, merge_function in table_configs:\n","            logger.info(f\"Processing table: {table_name}\")\n","            \n","            # Ensure table exists\n","            ensure_delta_table_exists(table_name, df.schema)\n","            \n","            # Merge data if DataFrame has records\n","            if df.count() > 0:\n","                merge_function(df, table_name)\n","                logger.info(f\"Successfully loaded data into {table_name}\")\n","            else:\n","                logger.info(f\"No data to load for {table_name}\")\n","            \n","            # Optimize the table\n","            optimize_delta_table(table_name)\n","        \n","        # Step 5: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show statistics for each table\n","        for table_name, df, _ in table_configs:\n","            try:\n","                row_count = spark.table(table_name).count()\n","                logger.info(f\"Total rows in {table_name}: {row_count}\")\n","                \n","                if row_count > 0:\n","                    logger.info(f\"Sample data from {table_name}:\")\n","                    spark.table(table_name).show(3, truncate=False)\n","            except Exception as e:\n","                logger.warning(f\"Could not display statistics for {table_name}: {str(e)}\")\n","        \n","        # Show summary analytics\n","        logger.info(\"=== WORKSPACE ACCESS SUMMARY ===\")\n","        \n","        try:\n","            # Core access summary\n","            core_summary = spark.sql(f\"\"\"\n","                SELECT \n","                    workspace_id,\n","                    COUNT(*) as total_principals,\n","                    COUNT(DISTINCT principal_type) as unique_principal_types,\n","                    COUNT(DISTINCT workspace_role) as unique_roles\n","                FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","                WHERE workspace_id = '{target_workspace_id}'\n","                GROUP BY workspace_id\n","            \"\"\")\n","            \n","            logger.info(\"Core access summary:\")\n","            core_summary.show(truncate=False)\n","            \n","            # Principal type distribution\n","            principal_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    principal_type,\n","                    workspace_role,\n","                    COUNT(*) as count\n","                FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","                WHERE workspace_id = '{target_workspace_id}'\n","                GROUP BY principal_type, workspace_role\n","                ORDER BY principal_type, workspace_role\n","            \"\"\")\n","            \n","            logger.info(\"Principal type and role distribution:\")\n","            principal_distribution.show(truncate=False)\n","            \n","        except Exception as e:\n","            logger.warning(f\"Could not generate summary analytics: {str(e)}\")\n","        \n","        # Return the core DataFrame for further analysis if needed\n","        return {\n","            \"core_access\": core_access_df,\n","            \"user_details\": user_details_df,\n","            \"group_details\": group_details_df,\n","            \"service_principal_details\": sp_details_df,\n","            \"service_principal_profile_details\": spp_details_df\n","        }\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 12 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    # Option 1: Use the test workspace ID from config\n","    dataframes = main()\n","    \n","    # Option 2: Specify a workspace ID directly (uncomment and replace with actual ID)\n","    # dataframes = main(\"f089354e-8366-4e18-aea3-4cb4a3a50b48\")\n","# ==================================\n","\n","\n","# CELL 13 - Utility Functions for Analysis\n","# ==================================\n","def analyze_workspace_access(workspace_id: str):\n","    \"\"\"\n","    Perform detailed analysis of workspace access patterns.\n","    \n","    This function provides various analytical queries to understand\n","    the access patterns for a specific workspace.\n","    \n","    Args:\n","        workspace_id: The workspace ID to analyze\n","    \"\"\"\n","    logger.info(f\"Analyzing workspace access patterns for workspace: {workspace_id}\")\n","    \n","    try:\n","        # 1. Overall access summary\n","        print(\"=== WORKSPACE ACCESS ANALYSIS ===\\n\")\n","        \n","        overall_summary = spark.sql(f\"\"\"\n","            SELECT \n","                'Total Principals' as metric,\n","                CAST(COUNT(*) as STRING) as value\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}'\n","            \n","            UNION ALL\n","            \n","            SELECT \n","                'Unique Principal Types' as metric,\n","                CAST(COUNT(DISTINCT principal_type) as STRING) as value\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}'\n","            \n","            UNION ALL\n","            \n","            SELECT \n","                'Admin Users' as metric,\n","                CAST(COUNT(*) as STRING) as value\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}' AND workspace_role = 'Admin'\n","        \"\"\")\n","        \n","        print(\"Overall Summary:\")\n","        overall_summary.show(truncate=False)\n","        \n","        # 2. Detailed breakdowns\n","        role_breakdown = spark.sql(f\"\"\"\n","            SELECT \n","                workspace_role,\n","                principal_type,\n","                COUNT(*) as count,\n","                ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}'\n","            GROUP BY workspace_role, principal_type\n","            ORDER BY workspace_role, principal_type\n","        \"\"\")\n","        \n","        print(\"\\nRole and Principal Type Breakdown:\")\n","        role_breakdown.show(truncate=False)\n","        \n","        # 3. User details (if any users exist)\n","        user_details_query = f\"\"\"\n","            SELECT \n","                c.principal_display_name,\n","                c.workspace_role,\n","                u.user_principal_name,\n","                c.last_updated_timestamp\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]} c\n","            LEFT JOIN {CONFIG[\"ACCESS_USERS_TABLE_NAME\"]} u \n","                ON c.principal_id = u.principal_id\n","            WHERE c.workspace_id = '{workspace_id}' \n","                AND c.principal_type = 'User'\n","            ORDER BY c.workspace_role, c.principal_display_name\n","        \"\"\"\n","        \n","        user_details_result = spark.sql(user_details_query)\n","        if user_details_result.count() > 0:\n","            print(\"\\nUser Access Details:\")\n","            user_details_result.show(truncate=False)\n","        \n","        # 4. Group details (if any groups exist)\n","        group_details_query = f\"\"\"\n","            SELECT \n","                c.principal_display_name,\n","                c.workspace_role,\n","                g.group_type,\n","                c.last_updated_timestamp\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]} c\n","            LEFT JOIN {CONFIG[\"ACCESS_GROUPS_TABLE_NAME\"]} g \n","                ON c.principal_id = g.principal_id\n","            WHERE c.workspace_id = '{workspace_id}' \n","                AND c.principal_type = 'Group'\n","            ORDER BY c.workspace_role, c.principal_display_name\n","        \"\"\"\n","        \n","        group_details_result = spark.sql(group_details_query)\n","        if group_details_result.count() > 0:\n","            print(\"\\nGroup Access Details:\")\n","            group_details_result.show(truncate=False)\n","        \n","        # 5. Service principal details (if any exist)\n","        sp_details_query = f\"\"\"\n","            SELECT \n","                c.principal_display_name,\n","                c.workspace_role,\n","                sp.aad_app_id,\n","                c.last_updated_timestamp\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]} c\n","            LEFT JOIN {CONFIG[\"ACCESS_SERVICE_PRINCIPALS_TABLE_NAME\"]} sp \n","                ON c.principal_id = sp.principal_id\n","            WHERE c.workspace_id = '{workspace_id}' \n","                AND c.principal_type = 'ServicePrincipal'\n","            ORDER BY c.workspace_role, c.principal_display_name\n","        \"\"\"\n","        \n","        sp_details_result = spark.sql(sp_details_query)\n","        if sp_details_result.count() > 0:\n","            print(\"\\nService Principal Access Details:\")\n","            sp_details_result.show(truncate=False)\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in workspace access analysis: {str(e)}\")\n","        raise\n","\n","\n","def get_workspace_security_report(workspace_id: str):\n","    \"\"\"\n","    Generate a security-focused report for the workspace.\n","    \n","    Args:\n","        workspace_id: The workspace ID to analyze\n","    \n","    Returns:\n","        dict: Security metrics and findings\n","    \"\"\"\n","    logger.info(f\"Generating security report for workspace: {workspace_id}\")\n","    \n","    try:\n","        # Count admins\n","        admin_count = spark.sql(f\"\"\"\n","            SELECT COUNT(*) as count\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}' AND workspace_role = 'Admin'\n","        \"\"\").collect()[0]['count']\n","        \n","        # Count external/service principals\n","        sp_count = spark.sql(f\"\"\"\n","            SELECT COUNT(*) as count\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}' AND principal_type IN ('ServicePrincipal', 'ServicePrincipalProfile')\n","        \"\"\").collect()[0]['count']\n","        \n","        # Count groups\n","        group_count = spark.sql(f\"\"\"\n","            SELECT COUNT(*) as count\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}' AND principal_type = 'Group'\n","        \"\"\").collect()[0]['count']\n","        \n","        # Total principals\n","        total_count = spark.sql(f\"\"\"\n","            SELECT COUNT(*) as count\n","            FROM {CONFIG[\"ACCESS_CORE_TABLE_NAME\"]}\n","            WHERE workspace_id = '{workspace_id}'\n","        \"\"\").collect()[0]['count']\n","        \n","        security_report = {\n","            \"workspace_id\": workspace_id,\n","            \"total_principals\": total_count,\n","            \"admin_count\": admin_count,\n","            \"service_principal_count\": sp_count,\n","            \"group_count\": group_count,\n","            \"admin_percentage\": round((admin_count / total_count * 100) if total_count > 0 else 0, 2),\n","            \"service_principal_percentage\": round((sp_count / total_count * 100) if total_count > 0 else 0, 2)\n","        }\n","        \n","        # Security recommendations\n","        recommendations = []\n","        if admin_count > 5:\n","            recommendations.append(\"Consider reducing the number of workspace administrators\")\n","        if sp_count > total_count * 0.3:\n","            recommendations.append(\"High number of service principals - review for necessity\")\n","        if group_count == 0:\n","            recommendations.append(\"Consider using groups for easier access management\")\n","        \n","        security_report[\"recommendations\"] = recommendations\n","        \n","        logger.info(\"Security Report Generated:\")\n","        for key, value in security_report.items():\n","            logger.info(f\"  {key}: {value}\")\n","        \n","        return security_report\n","        \n","    except Exception as e:\n","        logger.error(f\"Error generating security report: {str(e)}\")\n","        raise\n","\n","\n","# Example usage function\n","def run_analysis_example():\n","    \"\"\"\n","    Example function showing how to use the analysis functions.\n","    Update the workspace_id to match your test workspace.\n","    \"\"\"\n","    # Replace with your actual workspace ID\n","    example_workspace_id = CONFIG[\"TEST_WORKSPACE_ID\"]\n","    \n","    if example_workspace_id != \"your-workspace-id-here\":\n","        print(\"Running workspace access analysis example...\")\n","        analyze_workspace_access(example_workspace_id)\n","        \n","        print(\"\\n\" + \"=\"*50)\n","        print(\"Security Report:\")\n","        security_report = get_workspace_security_report(example_workspace_id)\n","    else:\n","        print(\"Please update the TEST_WORKSPACE_ID in CONFIG to run the analysis example\")\n","\n","# Uncomment the line below to run the analysis example\n","# run_analysis_example()\n","# ==================================\n","\n","\n","# CELL 14 - Maintenance and Best Practices\n","# ==================================\n","\"\"\"\n","MAINTENANCE AND BEST PRACTICES FOR WORKSPACE ACCESS DETAILS:\n","\n","1. SCHEDULED UPDATES:\n","   - Schedule this notebook to run regularly (daily/weekly) to keep access data current\n","   - Use Fabric pipelines to orchestrate multiple workspace processing\n","   - Consider processing multiple workspaces in batches\n","\n","2. MULTI-WORKSPACE PROCESSING:\n","   - Modify main() function to accept a list of workspace IDs\n","   - Implement parallel processing for large numbers of workspaces\n","   - Use workspace metadata from the workspaces API to get all workspace IDs\n","\n","3. DELTA LAKE MAINTENANCE:\n","   - Run VACUUM periodically on all tables to clean old files\n","   - Monitor table sizes and partition strategies\n","   - Use time travel for auditing access changes over time\n","\n","4. SECURITY AND COMPLIANCE:\n","   - Track admin access changes over time\n","   - Monitor service principal access patterns\n","   - Create alerts for unexpected access modifications\n","   - Implement data retention policies for access history\n","\n","5. ANALYTICS AND REPORTING:\n","   - Create Power BI dashboards for access governance\n","   - Build reports showing access patterns across workspaces\n","   - Track compliance with least-privilege principles\n","   - Monitor external access (service principals)\n","\n","6. PERFORMANCE OPTIMIZATION:\n","   - Consider partitioning core table by workspace_id for large tenants\n","   - Create indexes on frequently queried columns\n","   - Use Delta table clustering for better query performance\n","\n","7. DATA QUALITY MONITORING:\n","   - Implement checks for orphaned principal details\n","   - Monitor for schema changes in API responses\n","   - Validate referential integrity between tables\n","   - Track API response times and error rates\n","\n","8. INTEGRATION PATTERNS:\n","   - Combine with Azure AD data for complete identity picture\n","   - Link with workspace usage metrics for access analytics\n","   - Integrate with SIEM systems for security monitoring\n","\n","Example queries for ongoing analysis:\n","\n","-- Find workspaces with too many admins\n","SELECT workspace_id, COUNT(*) as admin_count\n","FROM fabric_workspace_access_core\n","WHERE workspace_role = 'Admin'\n","GROUP BY workspace_id\n","HAVING COUNT(*) > 5;\n","\n","-- Track access changes over time (requires historical data)\n","SELECT \n","    workspace_id,\n","    principal_type,\n","    workspace_role,\n","    COUNT(*) as current_count,\n","    LAG(COUNT(*)) OVER (PARTITION BY workspace_id, principal_type, workspace_role ORDER BY DATE(last_updated_timestamp)) as previous_count\n","FROM fabric_workspace_access_core\n","GROUP BY workspace_id, principal_type, workspace_role, DATE(last_updated_timestamp);\n","\n","-- Find service principals with admin access\n","SELECT \n","    c.workspace_id,\n","    c.principal_display_name,\n","    sp.aad_app_id,\n","    c.last_updated_timestamp\n","FROM fabric_workspace_access_core c\n","JOIN fabric_workspace_access_service_principals sp ON c.principal_id = sp.principal_id\n","WHERE c.workspace_role = 'Admin';\n","\n","9. ERROR HANDLING AND MONITORING:\n","   - Implement comprehensive logging for all operations\n","   - Set up alerts for API failures or unexpected data patterns\n","   - Create dashboards for monitoring ETL process health\n","   - Implement retry logic with exponential backoff for resilience\n","\n","10. FUTURE EXTENSIBILITY:\n","    - Design for new principal types that may be added to the API\n","    - Plan for additional workspace access details fields\n","    - Consider federation with other Microsoft 365 access data\n","    - Prepare for potential API versioning changes\n","\"\"\"\n","# ==================================\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"35c566ec-c3b1-4832-81ff-4c8b43c6ca8e","normalized_state":"finished","queued_time":"2025-05-27T19:09:01.0370102Z","session_start_time":"2025-05-27T19:09:01.0407095Z","execution_start_time":"2025-05-27T19:09:14.9618447Z","execution_finish_time":"2025-05-27T19:10:17.8908251Z","parent_msg_id":"ce807711-da1b-4c4d-82f1-28f7f399e0aa"},"text/plain":"StatementMeta(, 35c566ec-c3b1-4832-81ff-4c8b43c6ca8e, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-05-27 19:09:17,636 - INFO - Starting Fabric Workspace Access Details to Delta Lake process for workspace: 7a21dc44-c8b8-446e-9e80-59458a88ece8\n2025-05-27 19:09:17,637 - INFO - Getting access token...\n2025-05-27 19:09:18,513 - INFO - Successfully obtained access token\n2025-05-27 19:09:18,514 - INFO - Retrieving access details for workspace 7a21dc44-c8b8-446e-9e80-59458a88ece8...\n2025-05-27 19:09:18,514 - INFO - Retrieving access details for workspace: 7a21dc44-c8b8-446e-9e80-59458a88ece8\n2025-05-27 19:09:18,515 - INFO - Making API call to: https://api.fabric.microsoft.com/v1/admin/workspaces/7a21dc44-c8b8-446e-9e80-59458a88ece8/users with params: None (Attempt 1)\n2025-05-27 19:09:18,797 - INFO - Response status: 200\n2025-05-27 19:09:18,798 - INFO - Response contains 12 items in 'accessDetails' array\n2025-05-27 19:09:18,801 - INFO - Response keys: ['accessDetails']\n2025-05-27 19:09:18,802 - INFO - Retrieved 12 access details for workspace 7a21dc44-c8b8-446e-9e80-59458a88ece8\n2025-05-27 19:09:18,802 - INFO - Sample access detail: {\n  \"principal\": {\n    \"id\": \"123b890b-86fe-4bd3-91b0-54ae0e368745\",\n    \"displayName\": \"Gatilao,Rey Allen T Gatilao\",\n    \"type\": \"User\",\n    \"userDetails\": {\n      \"userPrincipalName\": \"RTGatilao-mda@mdanderson.org\"\n    }\n  },\n  \"workspaceAccessDetails\": {\n    \"type\": \"Workspace\",\n    \"workspaceRole\": \"Admin\"\n  }\n}\n2025-05-27 19:09:18,803 - INFO - Creating DataFrames for different principal types...\n2025-05-27 19:09:18,803 - INFO - Creating core access DataFrame\n2025-05-27 19:09:19,949 - INFO - Created core access DataFrame with 12 records\n2025-05-27 19:09:19,951 - INFO - Creating user details DataFrame\n2025-05-27 19:09:20,134 - INFO - Created user details DataFrame with 6 records\n2025-05-27 19:09:20,135 - INFO - Creating group details DataFrame\n2025-05-27 19:09:20,290 - INFO - Created group details DataFrame with 2 records\n2025-05-27 19:09:20,291 - INFO - Creating service principal details DataFrame\n2025-05-27 19:09:26,203 - INFO - Creating Delta table 'fabric_workspaces_access_core'\n2025-05-27 19:09:30,319 - INFO - Delta table 'fabric_workspaces_access_core' created successfully\n2025-05-27 19:09:30,449 - INFO - Starting merge operation for fabric_workspaces_access_core\n2025-05-27 19:09:33,718 - INFO - Table fabric_workspaces_access_core is empty. Inserting all records.\n2025-05-27 19:09:37,022 - INFO - Successfully loaded data into fabric_workspaces_access_core\n2025-05-27 19:09:37,023 - INFO - Optimizing Delta table 'fabric_workspaces_access_core'\n2025-05-27 19:09:40,555 - INFO - Table statistics updated successfully for fabric_workspaces_access_core\n2025-05-27 19:09:40,555 - INFO - Delta table optimization completed for fabric_workspaces_access_core\n2025-05-27 19:09:40,556 - INFO - Processing table: fabric_workspaces_access_users\n2025-05-27 19:09:40,734 - INFO - Creating Delta table 'fabric_workspaces_access_users'\n2025-05-27 19:09:43,047 - INFO - Delta table 'fabric_workspaces_access_users' created successfully\n2025-05-27 19:09:43,257 - INFO - Starting merge operation for fabric_workspaces_access_users\n2025-05-27 19:09:45,013 - INFO - Table fabric_workspaces_access_users is empty. Inserting all records.\n2025-05-27 19:09:46,920 - INFO - Successfully loaded data into fabric_workspaces_access_users\n2025-05-27 19:09:46,921 - INFO - Optimizing Delta table 'fabric_workspaces_access_users'\n2025-05-27 19:09:49,492 - INFO - Table statistics updated successfully for fabric_workspaces_access_users\n2025-05-27 19:09:49,493 - INFO - Delta table optimization completed for fabric_workspaces_access_users\n2025-05-27 19:09:49,494 - INFO - Processing table: fabric_workspaces_access_groups\n2025-05-27 19:09:49,665 - INFO - Creating Delta table 'fabric_workspaces_access_groups'\n2025-05-27 19:09:51,717 - INFO - Delta table 'fabric_workspaces_access_groups' created successfully\n2025-05-27 19:09:51,772 - INFO - Starting merge operation for fabric_workspaces_access_groups\n2025-05-27 19:09:53,109 - INFO - Table fabric_workspaces_access_groups is empty. Inserting all records.\n2025-05-27 19:09:54,780 - INFO - Successfully loaded data into fabric_workspaces_access_groups\n2025-05-27 19:09:54,781 - INFO - Optimizing Delta table 'fabric_workspaces_access_groups'\n2025-05-27 19:09:57,356 - INFO - Table statistics updated successfully for fabric_workspaces_access_groups\n2025-05-27 19:09:57,357 - INFO - Delta table optimization completed for fabric_workspaces_access_groups\n2025-05-27 19:09:57,357 - INFO - Processing table: fabric_workspaces_access_service_principals\n2025-05-27 19:09:57,510 - INFO - Creating Delta table 'fabric_workspaces_access_service_principals'\n2025-05-27 19:09:59,955 - INFO - Delta table 'fabric_workspaces_access_service_principals' created successfully\n2025-05-27 19:10:00,010 - INFO - Starting merge operation for fabric_workspaces_access_service_principals\n2025-05-27 19:10:01,275 - INFO - Table fabric_workspaces_access_service_principals is empty. Inserting all records.\n2025-05-27 19:10:02,788 - INFO - Successfully loaded data into fabric_workspaces_access_service_principals\n2025-05-27 19:10:02,789 - INFO - Optimizing Delta table 'fabric_workspaces_access_service_principals'\n2025-05-27 19:10:05,119 - INFO - Table statistics updated successfully for fabric_workspaces_access_service_principals\n2025-05-27 19:10:05,120 - INFO - Delta table optimization completed for fabric_workspaces_access_service_principals\n2025-05-27 19:10:05,120 - INFO - Processing table: fabric_workspaces_access_service_principal_profiles\n2025-05-27 19:10:05,291 - INFO - Creating Delta table 'fabric_workspaces_access_service_principal_profiles'\n2025-05-27 19:10:07,392 - INFO - Delta table 'fabric_workspaces_access_service_principal_profiles' created successfully\n2025-05-27 19:10:07,434 - INFO - No data to load for fabric_workspaces_access_service_principal_profiles\n2025-05-27 19:10:07,435 - INFO - Optimizing Delta table 'fabric_workspaces_access_service_principal_profiles'\n2025-05-27 19:10:10,246 - INFO - Table statistics updated successfully for fabric_workspaces_access_service_principal_profiles\n2025-05-27 19:10:10,247 - INFO - Delta table optimization completed for fabric_workspaces_access_service_principal_profiles\n2025-05-27 19:10:10,247 - INFO - Loading completed successfully!\n2025-05-27 19:10:10,666 - INFO - Total rows in fabric_workspaces_access_core: 12\n2025-05-27 19:10:10,666 - INFO - Sample data from fabric_workspaces_access_core:\n2025-05-27 19:10:12,726 - INFO - Total rows in fabric_workspaces_access_groups: 2\n2025-05-27 19:10:12,726 - INFO - Sample data from fabric_workspaces_access_groups:\n2025-05-27 19:10:14,669 - INFO - Total rows in fabric_workspaces_access_service_principal_profiles: 0\n2025-05-27 19:10:14,670 - INFO - === WORKSPACE ACCESS SUMMARY ===\n2025-05-27 19:10:14,881 - INFO - Core access summary:\n"]},{"output_type":"stream","name":"stdout","text":["+------------------------------------+------------------------------------+---------------------------+--------------+--------------+--------------+--------------------------+\n|workspace_id                        |principal_id                        |principal_display_name     |principal_type|workspace_role|workspace_type|last_updated_timestamp    |\n+------------------------------------+------------------------------------+---------------------------+--------------+--------------+--------------+--------------------------+\n|7a21dc44-c8b8-446e-9e80-59458a88ece8|123b890b-86fe-4bd3-91b0-54ae0e368745|Gatilao,Rey Allen T Gatilao|User          |Admin         |Workspace     |2025-05-27 19:09:20.509256|\n|7a21dc44-c8b8-446e-9e80-59458a88ece8|f12096e4-4ee8-4adc-8f64-1b25cc463fc4|Brent Hand                 |User          |Admin         |Workspace     |2025-05-27 19:09:20.509256|\n|7a21dc44-c8b8-446e-9e80-59458a88ece8|0b536f4a-3e41-4411-8c0a-806c1935121e|Morgan Jolley              |User          |Admin         |Workspace     |2025-05-27 19:09:20.509256|\n|7a21dc44-c8b8-446e-9e80-59458a88ece8|1cf171fe-0766-45fb-99ac-035227141630|Angela Wolf                |User          |Viewer        |Workspace     |2025-05-27 19:09:20.509256|\n|7a21dc44-c8b8-446e-9e80-59458a88ece8|4d3f6cdf-5f58-42e8-83c3-050061933519|Philip Wu                  |User          |Admin         |Workspace     |2025-05-27 19:09:20.509256|\n+------------------------------------+------------------------------------+---------------------------+--------------+--------------+--------------+--------------------------+\nonly showing top 5 rows\n\n+------------------------------------+----------------+----------------------+------------+\n|workspace_id                        |total_principals|unique_principal_types|unique_roles|\n+------------------------------------+----------------+----------------------+------------+\n|7a21dc44-c8b8-446e-9e80-59458a88ece8|12              |3                     |3           |\n+------------------------------------+----------------+----------------------+------------+\n\n+----------------+--------------+-----+\n|principal_type  |workspace_role|count|\n+----------------+--------------+-----+\n|Group           |Admin         |1    |\n|Group           |Contributor   |1    |\n|ServicePrincipal|Admin         |1    |\n|ServicePrincipal|Contributor   |3    |\n|User            |Admin         |5    |\n|User            |Viewer        |1    |\n+----------------+--------------+-----+\n\n"]},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"\"\\nMAINTENANCE AND BEST PRACTICES FOR WORKSPACE ACCESS DETAILS:\\n\\n1. SCHEDULED UPDATES:\\n   - Schedule this notebook to run regularly (daily/weekly) to keep access data current\\n   - Use Fabric pipelines to orchestrate multiple workspace processing\\n   - Consider processing multiple workspaces in batches\\n\\n2. MULTI-WORKSPACE PROCESSING:\\n   - Modify main() function to accept a list of workspace IDs\\n   - Implement parallel processing for large numbers of workspaces\\n   - Use workspace metadata from the workspaces API to get all workspace IDs\\n\\n3. DELTA LAKE MAINTENANCE:\\n   - Run VACUUM periodically on all tables to clean old files\\n   - Monitor table sizes and partition strategies\\n   - Use time travel for auditing access changes over time\\n\\n4. SECURITY AND COMPLIANCE:\\n   - Track admin access changes over time\\n   - Monitor service principal access patterns\\n   - Create alerts for unexpected access modifications\\n   - Implement data retention policies for access history\\n\\n5. ANALYTICS AND REPORTING:\\n   - Create Power BI dashboards for access governance\\n   - Build reports showing access patterns across workspaces\\n   - Track compliance with least-privilege principles\\n   - Monitor external access (service principals)\\n\\n6. PERFORMANCE OPTIMIZATION:\\n   - Consider partitioning core table by workspace_id for large tenants\\n   - Create indexes on frequently queried columns\\n   - Use Delta table clustering for better query performance\\n\\n7. DATA QUALITY MONITORING:\\n   - Implement checks for orphaned principal details\\n   - Monitor for schema changes in API responses\\n   - Validate referential integrity between tables\\n   - Track API response times and error rates\\n\\n8. INTEGRATION PATTERNS:\\n   - Combine with Azure AD data for complete identity picture\\n   - Link with workspace usage metrics for access analytics\\n   - Integrate with SIEM systems for security monitoring\\n\\nExample queries for ongoing analysis:\\n\\n-- Find workspaces with too many admins\\nSELECT workspace_id, COUNT(*) as admin_count\\nFROM fabric_workspace_access_core\\nWHERE workspace_role = 'Admin'\\nGROUP BY workspace_id\\nHAVING COUNT(*) > 5;\\n\\n-- Track access changes over time (requires historical data)\\nSELECT \\n    workspace_id,\\n    principal_type,\\n    workspace_role,\\n    COUNT(*) as current_count,\\n    LAG(COUNT(*)) OVER (PARTITION BY workspace_id, principal_type, workspace_role ORDER BY DATE(last_updated_timestamp)) as previous_count\\nFROM fabric_workspace_access_core\\nGROUP BY workspace_id, principal_type, workspace_role, DATE(last_updated_timestamp);\\n\\n-- Find service principals with admin access\\nSELECT \\n    c.workspace_id,\\n    c.principal_display_name,\\n    sp.aad_app_id,\\n    c.last_updated_timestamp\\nFROM fabric_workspace_access_core c\\nJOIN fabric_workspace_access_service_principals sp ON c.principal_id = sp.principal_id\\nWHERE c.workspace_role = 'Admin';\\n\\n9. ERROR HANDLING AND MONITORING:\\n   - Implement comprehensive logging for all operations\\n   - Set up alerts for API failures or unexpected data patterns\\n   - Create dashboards for monitoring ETL process health\\n   - Implement retry logic with exponential backoff for resilience\\n\\n10. FUTURE EXTENSIBILITY:\\n    - Design for new principal types that may be added to the API\\n    - Plan for additional workspace access details fields\\n    - Consider federation with other Microsoft 365 access data\\n    - Prepare for potential API versioning changes\\n\""},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bdcefeae-7b50-4eee-b388-adab8a1e472a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}