{"cells":[{"cell_type":"code","source":["#Display Name: \"MS Fabric Admin SPN POC\"\n","#Application (client) ID: f9d144fe-dc6f-4d3e-98e0-456e84bc6e23\n","#Object ID: 224ea137-751a-4a9a-bff2-e264d02c2376\n","#Directory (tenant) ID: 2d51fc70-177a-4852-ba7e-54d34883bb15\n","#Secret ID: 9b809eeb-a560-4d1b-8480-7e9f85805303\n","#Secret Value: H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb\n","#Expiration Date: 12/10/2025\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ded4b9ef-8939-4d9d-9ee8-ea08b821eba2"},{"cell_type":"code","source":["#Load Lakehouse table data into Warehouse stage schema\n","#using JDBC\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import StructType, StructField, StringType\n","from pyspark.sql import DataFrame\n","import os\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Lakehouse to Warehouse\") \\\n","    .getOrCreate()\n","\n","# Define the path to your Lakehouse\n","lakehouse_path = \"Tables\"\n","\n","# List all tables (directories) in the Lakehouse\n","tables = [table for table in os.listdir(lakehouse_path) if os.path.isdir(os.path.join(lakehouse_path, table))]\n","\n","# Loop through each table\n","for table in tables:\n","    # Read data from Lakehouse\n","    table_path = os.path.join(lakehouse_path, table)\n","    df = spark.read.format(\"parquet\").load(table_path)\n","   \n","    # Write data to Warehouse using SPN\n","#    warehouse_url = \"jdbc:sqlserver://your_server.database.windows.net:1433;database=your_database\"\n","    warehouse_url = \"od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com\"\n","    warehouse_properties = {\n","        \"user\": \"f9d144fe-dc6f-4d3e-98e0-456e84bc6e23\",\n","        \"password\": \"H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb\",\n","        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n","        \"authentication\": \"ActiveDirectoryServicePrincipal\",\n","        \"tenant\": \"2d51fc70-177a-4852-ba7e-54d34883bb15\"\n","    }\n","   \n","    # Write to staging schema\n","    staging_table = f\"stage.{table}\"\n","    df.write.jdbc(url=warehouse_url, table=staging_table, mode=\"overwrite\", properties=warehouse_properties)\n","\n","# Close Spark session\n","spark.stop()\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e09cdb4e-a19a-4412-9337-55cb69ab7404"},{"cell_type":"code","source":["#Testing Warehouse READ\n","#Using JDBC\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","\n","# Create the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# JDBC connection properties\n","#jdbc_url = \"jdbc:sqlserver://your_server_name.database.windows.net:1433;database=your_database_name\"\n","jdbc_url = \"jdbc:sqlserver://od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com:1433;database=FabricAdmin_Warehouse\"\n","\n","jdbc_properties = {\n","    \"user\": client_id,  # Using the client ID as the user\n","    \"password\": client_secret,  # Using the client secret as the password\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n","    \"authentication\": \"ActiveDirectoryServicePrincipal\",\n","    \"authenticationProperties\": f\"TenantId={tenant_id}\",\n","    \"socketTimeout\": \"30000\",\n","    \"loginTimeout\": \"30\"\n","}\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Data Warehouse using JDBC and SPN\") \\\n","    .getOrCreate()\n","\n","# Read data from the SQL Server table using JDBC\n","table_name = \"TestTable\"\n","df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","\n","# Show the DataFrame\n","df.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"c1c1ac20-7689-40d2-b0a8-4109b246e2c7","normalized_state":"finished","queued_time":"2024-12-10T17:46:22.0785871Z","session_start_time":null,"execution_start_time":"2024-12-10T17:46:22.5640913Z","execution_finish_time":"2024-12-10T17:46:46.2319793Z","parent_msg_id":"ec2c6d28-7654-4e48-b0a5-60c3053a291e"},"text/plain":"StatementMeta(, c1c1ac20-7689-40d2-b0a8-4109b246e2c7, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+----+\n|col1|\n+----+\n+----+\n\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3e0ac0f-ee62-4053-9975-2ab3af5f37b3"},{"cell_type":"code","source":["#Testing Warehouse WRITE by copying TestTable\n","#using JDBC\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","\n","# Create the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# JDBC connection properties\n","#jdbc_url = \"jdbc:sqlserver://your_server_name.database.windows.net:1433;database=your_database_name\"\n","jdbc_url = \"jdbc:sqlserver://od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com:1433;database=FabricAdmin_Warehouse\"\n","\n","jdbc_properties = {\n","    \"user\": client_id,  # Using the client ID as the user\n","    \"password\": client_secret,  # Using the client secret as the password\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n","    \"authentication\": \"ActiveDirectoryServicePrincipal\",\n","    \"authenticationProperties\": f\"TenantId={tenant_id}\",\n","    \"socketTimeout\": \"30000\",\n","    \"loginTimeout\": \"30\"\n","}\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Data Warehouse using JDBC and SPN\") \\\n","    .getOrCreate()\n","\n","# Read data from the SQL Server table using JDBC\n","table_name = \"TestTable\"\n","df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","\n","# Show the DataFrame\n","df.show()\n","\n","# Write the DataFrame to the Lakehouse table using JDBC\n","df.write.jdbc(url=jdbc_url, table='TestTable_Copy', mode='overwrite', properties=jdbc_properties)\n","\n","print(\"Successfully read table from Warehouse and write a table copy into the same Warehouse!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"c1c1ac20-7689-40d2-b0a8-4109b246e2c7","normalized_state":"finished","queued_time":"2024-12-10T17:48:02.0074019Z","session_start_time":null,"execution_start_time":"2024-12-10T17:48:02.5526042Z","execution_finish_time":"2024-12-10T17:48:30.4370889Z","parent_msg_id":"8996688f-e015-4553-8754-f420091a6f74"},"text/plain":"StatementMeta(, c1c1ac20-7689-40d2-b0a8-4109b246e2c7, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+----+\n|col1|\n+----+\n+----+\n\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02cd4a3b-475d-487e-9315-fde8b20c6879"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import time\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com'\n","database_name = 'FabricAdmin_Lakehouse'\n","table_name = 'TestTable'\n","\n","# Construct the JDBC URL with the correct properties\n","jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};authentication=ActiveDirectoryServicePrincipal;user={client_id};password={client_secret};encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.fabric.microsoft.com\"\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read data from Lakehouse using JDBC\") \\\n","    .getOrCreate()\n","\n","# JDBC connection properties\n","jdbc_properties = {\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n","}\n","\n","# Function to attempt connection with retry logic\n","def attempt_connection(retries=3, delay=5):\n","    for i in range(retries):\n","        try:\n","            df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","            return df\n","        except Exception as e:\n","            print(f\"Attempt {i+1} failed: {e}\")\n","            time.sleep(delay)\n","    raise Exception(\"All connection attempts failed\")\n","\n","# Attempt to connect and read data\n","try:\n","    df = attempt_connection()\n","    df.show()\n","except Exception as e:\n","    print(f\"Error: {e}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"22271f0c-9314-4ac3-b407-8f599c3967ad"},{"cell_type":"code","source":["#Testing Lakehouse READ parquet using abfss\n","# Onelake needs to be enabled to use abfss\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","import os\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","\n","# Initialize the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# Set environment variables for Spark to use the credentials\n","os.environ[\"SPARK_DATASOURCE_AZURE_USERNAME\"] = client_id\n","os.environ[\"SPARK_DATASOURCE_AZURE_PASSWORD\"] = client_secret\n","os.environ[\"SPARK_DATASOURCE_AZURE_TENANT_ID\"] = tenant_id\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Lakehouse using SPN\") \\\n","    .getOrCreate()\n","\n","#lakehouse_file_path = \"{Lakehouse_abfss_path}/Files/workspaces.parquet\"\n","#lakehouse_file_path = \"Files/workspaces.parquet\"\n","lakehouse_file_path = \"abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/workspaces.parquet\"\n","\n","# Define the path to your Lakehouse file and the table name\n","#table_name = \"workspaces\"\n","\n","# Read data from Lakehouse file\n","df = spark.read.parquet(lakehouse_file_path)\n","\n","# Show the DataFrame\n","df.show()\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3166f47-47e6-417a-ab29-3711576ec637"},{"cell_type":"code","source":["#Testing Lakehouse READ parquet using abfss and create a copy in parquet format\n","# Onelake needs to be enabled to use abfss\n","\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","import os\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","\n","# Initialize the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# Set environment variables for Spark to use the credentials\n","os.environ[\"SPARK_DATASOURCE_AZURE_USERNAME\"] = client_id\n","os.environ[\"SPARK_DATASOURCE_AZURE_PASSWORD\"] = client_secret\n","os.environ[\"SPARK_DATASOURCE_AZURE_TENANT_ID\"] = tenant_id\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read Data from Lakehouse using SPN\") \\\n","    .getOrCreate()\n","\n","#lakehouse_file_path = \"{Lakehouse_abfss_path}/Files/workspaces.parquet\"\n","#lakehouse_file_path = \"Files/workspaces.parquet\"\n","lakehouse_file_path = \"abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/workspaces.parquet\"\n","\n","# Define the path to your Lakehouse file and the table name\n","#table_name = \"workspaces\"\n","\n","# Read data from Lakehouse file\n","df = spark.read.parquet(lakehouse_file_path)\n","\n","# Show the DataFrame\n","df.show()\n","\n","#create a copy and write as table\n","#df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspaces_copy\")\n","\n","# Write DataFrame to Lakehouse as Parquet file\n","df.write.mode(\"overwrite\").parquet(\"Files/workspaces_copy.parquet\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"12eab307-c54e-48e4-b8c7-db4748d1758c"},{"cell_type":"code","source":["#using jdbc on Lakehouse READ\n","\n","from pyspark.sql import SparkSession\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com'\n","#server_name = 'od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a'\n","\n","database_name = 'FabricAdmin_Lakehouse'\n","table_name = 'workspaces'\n","\n","# Construct the JDBC URL\n","jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};authentication=ActiveDirectoryServicePrincipal;user={client_id};password={client_secret};encrypt=true;trustServerCertificate=true;\"\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read data from Lakehouse using JDBC\") \\\n","    .getOrCreate()\n","\n","# JDBC connection properties\n","jdbc_properties = {\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n","}\n","\n","# Read data from the table using JDBC\n","df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n","\n","# Show the DataFrame\n","df.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2176b710-4fe0-43cf-b2d9-2bb25c01c1b5"},{"cell_type":"code","source":["#using read csv and write to Lakehouse using abfss\n","#working code\n","\n","from pyspark.sql import SparkSession\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read CSV and Write Data to Microsoft Fabric Lakehouse using ABFSS and SPN\") \\\n","    .getOrCreate()\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","storage_account_name = 'onelake'\n","container_name = '7a21dc44-c8b8-446e-9e80-59458a88ece8'\n","directory_name = '51872361-e484-4aa7-a0b4-853eaa971e47/Files'\n","\n","# Set up the Hadoop configuration with SPN credentials\n","spark.conf.set(f'fs.azure.account.auth.type.{storage_account_name}.dfs.fabric.microsoft.com', 'OAuth')\n","spark.conf.set(f'fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.fabric.microsoft.com', 'org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider')\n","spark.conf.set(f'fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.fabric.microsoft.com', client_id)\n","spark.conf.set(f'fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.fabric.microsoft.com', client_secret)\n","spark.conf.set(f'fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.fabric.microsoft.com', f'https://login.microsoftonline.com/{tenant_id}/oauth2/token')\n","\n","# Read data from the CSV file\n","#csv_file_path = f\"abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/sales.csv\"\n","csv_file_path = f\"abfss://{container_name}@{storage_account_name}.dfs.fabric.microsoft.com/{directory_name}/sales2.csv\"\n","\n","# Read data from the CSV file\n","df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n","\n","# Show the DataFrame\n","df.show()\n","\n","# Optionally, display the schema\n","df.printSchema()\n","\n","# Define the path where you want to write the Parquet file in the Lakehouse\n","lakehouse_path = f\"abfss://{container_name}@{storage_account_name}.dfs.fabric.microsoft.com/{directory_name}/sales2.parquet\"\n","\n","# Write the DataFrame to the Lakehouse in Parquet format\n","df.write.mode('overwrite').parquet(lakehouse_path)\n","#df.write.mode(\"overwrite\").parquet(\"Files/sales2.parquet\")\n","\n","print(\"Data read from CSV and written to Lakehouse successfully!\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8155c7dd-59a6-490d-a925-2b5bc12e5d7d"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from azure.identity import DefaultAzureCredential\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Read CSV and Write Data to Microsoft Fabric Lakehouse using ABFSS and DefaultAzureCredential\") \\\n","    .getOrCreate()\n","\n","# Create the DefaultAzureCredential\n","credential = DefaultAzureCredential()\n","\n","# Read data from the CSV file\n","csv_file_path = 'abfss://7a21dc44-c8b8-446e-9e80-59458a88ece8@onelake.dfs.fabric.microsoft.com/51872361-e484-4aa7-a0b4-853eaa971e47/Files/sales.csv'\n","#csv_file_path = 'Files/sales.csv'\n","df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n","\n","df.write\n","\n","df.show()\n","\n","\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"41b0415d-fda0-4001-b57b-781c7fc7ae7e"},{"cell_type":"code","source":["#Get all workspaces even if workspace count is more than 10,000 rows using continuation_token.\n","\n","import pandas as pd\n","import sempy.fabric as fabric\n","import logging\n","from pyspark.sql import SparkSession\n","from azure.identity import ClientSecretCredential\n","from pyspark.sql.functions import col, substring\n","from pyspark.sql.types import StringType\n","\n","# Initialize FabricRestClient for authentication\n","client = fabric.FabricRestClient()\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Function to fetch all workspaces\n","def fetch_all_workspaces():\n","    all_data = []\n","    continuation_token = None\n","    \n","    while True:\n","        try:\n","            # Construct the URL\n","            if continuation_token:\n","                url = f\"v1/admin/workspaces?continuationToken={continuation_token}\"\n","            else:\n","                url = \"v1/admin/workspaces\"\n","            \n","            # Make API request to fetch workspaces\n","            response = client.get(url)\n","            response.raise_for_status()  # Raise an error for bad status codes\n","            data = response.json()\n","            \n","            # Append the data if the structure is correct\n","            if 'workspaces' in data:\n","                all_data.extend(data['workspaces'])\n","            else:\n","                logger.error(\"Key 'workspaces' not found in response\")\n","                break\n","            \n","            # Check for continuation token\n","            continuation_token = data.get('continuationToken')\n","            if not continuation_token:\n","                break\n","        \n","        except Exception as e:\n","            logger.error(f\"An error occurred: {e}\")\n","            break\n","    \n","    df = pd.DataFrame(all_data)\n","    \n","    # Convert nvarchar(max) columns to a specific length (e.g., nvarchar(255))\n","    for col in df.select_dtypes(include=['object']).columns:\n","        df[col] = df[col].astype(str).str.slice(0, 255)\n","    \n","    return df\n","\n","# Fetch workspaces and display DataFrame\n","df = fetch_all_workspaces()\n","logger.info(f\"Total number of rows: {df.shape[0]}\")\n","#print(df)\n","\n","# Rename columns\n","df.columns = ['WorkspaceID', 'WorkspaceName', 'WorkspaceStatus', 'WorkspaceType', 'CapacityID']\n","\n","# Display DataFrame with custom column names\n","#print(df)\n","#display(df)\n","\n","# Convert pandas DataFrame to PySpark DataFrame\n","spark_df = spark.createDataFrame(df)\n","\n","# Convert nvarchar(max) columns to a specific length (e.g., nvarchar(255)) # doesnt work with overwrite since its behaviour is drop and create\n","for col_name in spark_df.columns:\n","    if spark_df.schema[col_name].dataType == StringType():\n","        spark_df = spark_df.withColumn(col_name, substring(col(col_name), 1, 255))\n","\n","# Show the resulting DataFrame\n","spark_df.show()\n","\n","# Check data types\n","spark_df.printSchema()\n","\n","# Display sample data\n","spark_df.show(5)\n","\n","# Check column lengths\n","for col_name in spark_df.columns:\n","    if spark_df.schema[col_name].dataType == StringType():\n","        max_length = spark_df.selectExpr(f\"max(length({col_name})) as max_len\").collect()[0]['max_len']\n","        logger.info(f\"Max length of column {col_name}: {max_length}\")\n","\n","# Define the Service Principal credentials\n","tenant_id = '2d51fc70-177a-4852-ba7e-54d34883bb15'\n","client_id = 'f9d144fe-dc6f-4d3e-98e0-456e84bc6e23'\n","client_secret = 'H4y8Q~bS4eBU1lyoJnV4M~nSgB.Not4QB9KZRcUb'\n","\n","# Create the credential\n","credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n","\n","# JDBC connection properties\n","#jdbc_url = \"jdbc:sqlserver://your_server_name.database.windows.net:1433;database=your_database_name\"\n","jdbc_url = \"jdbc:sqlserver://od6fcll2c5jerot6ktjura53cu-itocc6vyzbxejhualfcyvchm5a.datawarehouse.fabric.microsoft.com:1433;database=FabricAdmin_Warehouse\"\n","\n","jdbc_properties = {\n","    \"user\": client_id,  # Using the client ID as the user\n","    \"password\": client_secret,  # Using the client secret as the password\n","    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n","    \"authentication\": \"ActiveDirectoryServicePrincipal\",\n","    \"authenticationProperties\": f\"TenantId={tenant_id}\",\n","    \"socketTimeout\": \"30000\",\n","    \"loginTimeout\": \"30\"\n","}\n","\n","table_name = \"dbo.workspaces\"\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"TruncateTable\") \\\n","    .getOrCreate()\n","\n","# Truncate table using Spark DataFrame # truncate NOT supported in MS Fabric Warehouse \n","#truncate_query = f\"TRUNCATE TABLE {table_name}\"\n","\n","# Truncate table using Spark DataFrame \n","#truncate_query = f\"DELETE FROM {table_name}\"\n","\n","# Execute the query\n","#spark.sql(truncate_query).write \\\n","#    .format(\"jdbc\") \\\n","#    .option(\"url\", jdbc_url) \\\n","#    .option(\"dbtable\", table_name) \\\n","#    .options(**jdbc_properties) \\\n","#    .mode(\"append\") \\\n","#    .save()\n","\n","# Write the DataFrame to the Warehouse table using JDBC\n","spark_df.write.jdbc(url=jdbc_url, table=table_name, mode='append', properties=jdbc_properties)\n","#df.write.jdbc(url=jdbc_url, table=table_name, mode='overwrite', properties=jdbc_properties) # doesnt work since its behaviour is drop and create and nvarchar(mas) is not supported\n","\n","print(\"Data written to Warehouse successfully!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"3a0a5204-555a-48fd-8a66-57b41457c4cf","normalized_state":"finished","queued_time":"2024-12-19T21:48:50.2748772Z","session_start_time":"2024-12-19T21:48:50.4875096Z","execution_start_time":"2024-12-19T21:49:41.8670993Z","execution_finish_time":"2024-12-19T22:19:32.3222824Z","parent_msg_id":"dfbe32d9-a0a6-489f-92a5-81722b4605ac"},"text/plain":"StatementMeta(, 3a0a5204-555a-48fd-8a66-57b41457c4cf, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+---------------+-------------+--------------------+\n|         WorkspaceID|       WorkspaceName|WorkspaceStatus|WorkspaceType|          CapacityID|\n+--------------------+--------------------+---------------+-------------+--------------------+\n|47298625-cc8c-496...|IAI - BI Delivery...|         Active|    Workspace|665D4010-E0FD-482...|\n|2022d60a-0ea0-460...|      IAI Management|         Active|    Workspace|6BA0A957-48B9-4AD...|\n|49ec4821-c19f-4a6...|       EDI Analytics|         Active|    Workspace|646712A9-B160-4E8...|\n|aad173a4-46ac-4b6...|Github 8/26/2024 ...|         Active|    Workspace|6F2BAF05-9C63-440...|\n|63875610-1ed5-40c...|                Test|         Active|    Workspace|646712A9-B160-4E8...|\n|9805fa22-5867-47d...|  OneConnect Central|         Active|    Workspace|665D4010-E0FD-482...|\n|d919cb15-4a25-43b...|  Staging Compliance|         Active|    Workspace|6BA0A957-48B9-4AD...|\n|46fe26ce-3646-49e...|Office365 PowerBI...|         Active|    Workspace|4B2FD937-BE2D-4AD...|\n|9bb0abd2-c784-4fe...|Microsoft Project...|         Active|    Workspace|B9750DA0-6EE7-445...|\n|4591659d-be20-48f...|tyGraph for Share...|         Active|    Workspace|12E5359C-6E4D-4B5...|\n|63e3f8b7-a6fa-45f...|Microsoft Dynamic...|         Active|    Workspace|335562E4-1800-43E...|\n|bb8db3f3-eed5-4dc...|      BravoAnalytics|         Active|    Workspace|646712A9-B160-4E8...|\n|0ad9567d-3ea3-4a0...|                 MVM|         Active|    Workspace|4B2FD937-BE2D-4AD...|\n|d5cb3e2f-f368-466...|Sales and Marketi...|         Active|    Workspace|12E5359C-6E4D-4B5...|\n|999c5cde-3649-49d...|SR-Test POC Works...|         Active|    Workspace|30E225C1-39C1-48D...|\n|81a81687-36d2-449...|MDA Remote Work A...|         Active|    Workspace|665D4010-E0FD-482...|\n|6fef7da9-ab43-494...|              Nadeem|         Active|    Workspace|AA41F1B4-6A56-446...|\n|96c7b32d-115a-4bb...| Free Goods (PHACTS)|         Active|    Workspace|30E225C1-39C1-48D...|\n|b2793ebe-a3f6-467...|       Trial Project|         Active|    Workspace|646712A9-B160-4E8...|\n|00c369e4-45ee-4d1...|             Scripps|         Active|    Workspace|6BA0A957-48B9-4AD...|\n+--------------------+--------------------+---------------+-------------+--------------------+\nonly showing top 20 rows\n\nroot\n |-- WorkspaceID: string (nullable = true)\n |-- WorkspaceName: string (nullable = true)\n |-- WorkspaceStatus: string (nullable = true)\n |-- WorkspaceType: string (nullable = true)\n |-- CapacityID: string (nullable = true)\n\n+--------------------+--------------------+---------------+-------------+--------------------+\n|         WorkspaceID|       WorkspaceName|WorkspaceStatus|WorkspaceType|          CapacityID|\n+--------------------+--------------------+---------------+-------------+--------------------+\n|47298625-cc8c-496...|IAI - BI Delivery...|         Active|    Workspace|665D4010-E0FD-482...|\n|2022d60a-0ea0-460...|      IAI Management|         Active|    Workspace|6BA0A957-48B9-4AD...|\n|49ec4821-c19f-4a6...|       EDI Analytics|         Active|    Workspace|646712A9-B160-4E8...|\n|aad173a4-46ac-4b6...|Github 8/26/2024 ...|         Active|    Workspace|6F2BAF05-9C63-440...|\n|63875610-1ed5-40c...|                Test|         Active|    Workspace|646712A9-B160-4E8...|\n+--------------------+--------------------+---------------+-------------+--------------------+\nonly showing top 5 rows\n\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o6043.jdbc.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Read timed out\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:4026)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:2164)\n\tat com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:6830)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:8075)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:646)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:567)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7675)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4137)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:272)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:485)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:68)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:148)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:208)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:905)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:413)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:802)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1197)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1183)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1197)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1183)\n\tat java.base/sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:484)\n\tat java.base/sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:478)\n\tat java.base/sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(SSLSocketInputRecord.java:70)\n\tat java.base/sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1459)\n\tat java.base/sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:1070)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1197)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1183)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:2155)\n\t... 55 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 140\u001b[0m\n\u001b[1;32m    120\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncateTable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Truncate table using Spark DataFrame # truncate NOT supported in MS Fabric Warehouse \u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#truncate_query = f\"TRUNCATE TABLE {table_name}\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Write the DataFrame to the Warehouse table using JDBC\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39mjdbc_url, table\u001b[38;5;241m=\u001b[39mtable_name, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m, properties\u001b[38;5;241m=\u001b[39mjdbc_properties)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m#df.write.jdbc(url=jdbc_url, table=table_name, mode='overwrite', properties=jdbc_properties) # doesnt work since its behaviour is drop and create and nvarchar(mas) is not supported\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData written to Warehouse successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mjdbc(url, table, jprop)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o6043.jdbc.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Read timed out\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:4026)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:2164)\n\tat com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:6830)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:8075)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:646)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:567)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7675)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4137)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:272)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:485)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:68)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:88)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:148)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:208)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:905)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:413)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:802)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1197)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1183)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1197)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1183)\n\tat java.base/sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:484)\n\tat java.base/sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:478)\n\tat java.base/sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(SSLSocketInputRecord.java:70)\n\tat java.base/sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1459)\n\tat java.base/sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:1070)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1197)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1183)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:2155)\n\t... 55 more\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"51c5eb1d-b845-4015-803f-7e2e66d0f603\",\"activityId\":\"3a0a5204-555a-48fd-8a66-57b41457c4cf\",\"applicationId\":\"application_1734644162672_0001\",\"jobGroupId\":\"3\",\"advices\":{\"error\":1}}"}},"id":"d00df073-44ea-497f-b562-7e7e6317546d"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"},"warehouse":{}}},"nbformat":4,"nbformat_minor":5}