{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Gateways - Get Datasources\n","# Command:  GET https://api.powerbi.com/v1.0/myorg/gateways/{gatewayId}/datasources\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/power-bi/gateways/get-datasources\n","\n","# Loads table: pbi_gateways_datasources\n","\n","# Note: this queries the pbi_gateways table to get a list of gatewayId values for the API calls."],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"e69eb60a-ecf1-4ce9-8d24-d091e8c1cbc6","normalized_state":"finished","queued_time":"2025-07-16T19:08:16.8769172Z","session_start_time":"2025-07-16T19:08:16.8778842Z","execution_start_time":"2025-07-16T19:08:29.0033699Z","execution_finish_time":"2025-07-16T19:08:29.4458477Z","parent_msg_id":"951f7537-ff63-4fac-887c-879a94e141de"},"text/plain":"StatementMeta(, e69eb60a-ecf1-4ce9-8d24-d091e8c1cbc6, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ecd5d6c2-0760-4cf9-a90f-11770096a618"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Power BI Gateway Datasources to Delta Lake - PySpark Notebook\n","# This notebook retrieves Power BI gateway datasources for all gateways and loads them \n","# into a Delta Lake table with optimization for analytics workloads\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"PowerBIGatewayDatasourcesToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.powerbi.com/v1.0/myorg\",\n","    \"DATASOURCES_ENDPOINT\": \"/gateways/{gatewayId}/datasources\",  # Endpoint for gateway datasources\n","    \"MAX_RETRIES\": 5,  # Increased number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"DATASOURCES_TABLE_NAME\": \"pbi_gateways_datasources\",  # Name of the target Delta table\n","    \"GATEWAYS_TABLE_NAME\": \"pbi_gateways\",  # Name of the source gateways table\n","    \"LAKEHOUSE_PATH\": \"Tables\",  # Default Tables folder in Fabric Lakehouse\n","    \"DEBUG_MODE\": True  # Set to True to enable extra debugging output\n","}\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Power BI API authentication.\n","    \n","    This function tries multiple methods to obtain an access token:\n","    1. mssparkutils.credentials.getToken() - Primary method for Fabric\n","    2. Alternative token endpoints if the primary fails\n","    3. Manual token input as fallback\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        If automatic token retrieval fails, you may need to manually provide a token\n","        or check your Fabric workspace permissions.\n","    \"\"\"\n","    \n","    # Method 1: Try the standard mssparkutils approach\n","    try:\n","        logger.info(\"Attempting to get token using mssparkutils.credentials.getToken()...\")\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.powerbi.com\")\n","        logger.info(\"Successfully obtained token using mssparkutils\")\n","        return token_response\n","    except Exception as e:\n","        logger.warning(f\"Primary token method failed: {str(e)}\")\n","        logger.info(\"Trying alternative token methods...\")\n","    \n","    # Method 2: Try alternative Power BI API endpoint\n","    try:\n","        logger.info(\"Attempting to get token using alternative endpoint...\")\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n","        logger.info(\"Successfully obtained token using alternative endpoint\")\n","        return token_response\n","    except Exception as e:\n","        logger.warning(f\"Alternative endpoint failed: {str(e)}\")\n","    \n","    # Method 3: Try getting token for general Power Platform\n","    try:\n","        logger.info(\"Attempting to get token for Power Platform...\")\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://api.powerplatform.com\")\n","        logger.info(\"Successfully obtained token for Power Platform\")\n","        return token_response\n","    except Exception as e:\n","        logger.warning(f\"Power Platform token failed: {str(e)}\")\n","    \n","    # Method 4: Try the generic Microsoft Graph endpoint\n","    try:\n","        logger.info(\"Attempting to get token for Microsoft Graph...\")\n","        from notebookutils import mssparkutils\n","        token_response = mssparkutils.credentials.getToken(\"https://graph.microsoft.com\")\n","        logger.info(\"Successfully obtained token for Microsoft Graph\")\n","        return token_response\n","    except Exception as e:\n","        logger.warning(f\"Microsoft Graph token failed: {str(e)}\")\n","    \n","    # Method 5: Manual token input (fallback)\n","    logger.error(\"All automatic token methods failed.\")\n","    logger.error(\"Please try one of the following solutions:\")\n","    logger.error(\"1. Wait a few minutes and retry - this might be a temporary Fabric service issue\")\n","    logger.error(\"2. Restart your Fabric notebook kernel\")\n","    logger.error(\"3. Check your workspace permissions and access to Power BI APIs\")\n","    logger.error(\"4. Contact your Fabric administrator\")\n","    logger.error(\"\")\n","    logger.error(\"As a temporary workaround, you can manually provide a token:\")\n","    logger.error(\"- Go to https://learn.microsoft.com/en-us/rest/api/power-bi/\")\n","    logger.error(\"- Click 'Try It' on any API\")\n","    logger.error(\"- Copy the Bearer token from the Authorization header\")\n","    logger.error(\"- Uncomment and modify the line below with your token:\")\n","    logger.error(\"\")\n","    logger.error(\"# MANUAL_TOKEN = 'your_bearer_token_here'\")\n","    logger.error(\"# return MANUAL_TOKEN\")\n","    \n","    # Uncomment the lines below and add your manual token if needed\n","    # MANUAL_TOKEN = \"your_bearer_token_here\"\n","    # if MANUAL_TOKEN and MANUAL_TOKEN != \"your_bearer_token_here\":\n","    #     logger.info(\"Using manually provided token\")\n","    #     return MANUAL_TOKEN\n","    \n","    raise Exception(\"Unable to obtain access token using any available method. Please check the error messages above for solutions.\")\n","# ==================================\n","\n","\n","# CELL 7 - API Call Function\n","# ==================================\n","def validate_access_token(access_token: str) -> bool:\n","    \"\"\"\n","    Validate that the access token works with the Power BI API.\n","    \n","    This function makes a simple test call to verify the token is valid\n","    and has the necessary permissions.\n","    \n","    Args:\n","        access_token: The access token to validate\n","    \n","    Returns:\n","        bool: True if token is valid, False otherwise\n","    \"\"\"\n","    try:\n","        logger.info(\"Validating access token...\")\n","        \n","        # Make a simple test call to validate the token\n","        test_url = f\"{CONFIG['API_BASE_URL']}/groups\"  # Simple endpoint to test auth\n","        headers = {\n","            \"Authorization\": f\"Bearer {access_token}\",\n","            \"Content-Type\": \"application/json\"\n","        }\n","        \n","        response = requests.get(test_url, headers=headers, timeout=10)\n","        \n","        if response.status_code == 200:\n","            logger.info(\"Access token validation successful\")\n","            return True\n","        elif response.status_code == 401:\n","            logger.error(\"Access token is invalid or expired\")\n","            return False\n","        elif response.status_code == 403:\n","            logger.error(\"Access token is valid but lacks required permissions\")\n","            logger.error(\"Required scopes: Dataset.ReadWrite.All or Dataset.Read.All\")\n","            return False\n","        else:\n","            logger.warning(f\"Token validation returned status {response.status_code}: {response.text}\")\n","            return False\n","            \n","    except Exception as e:\n","        logger.error(f\"Token validation failed: {str(e)}\")\n","        return False\n","# ==================================\n","# ==================================\n","def call_powerbi_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Power BI with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Power BI API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/gateways/123/datasources\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            # Log the full URL with parameters for debugging\n","            logger.info(f\"Making API call to: {url} with params: {params} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Log the full response for debugging\n","            logger.info(f\"Response status: {response.status_code}\")\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # Log the response for debugging in case of errors\n","            if response.status_code >= 400:\n","                logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                logger.error(f\"Request URL: {response.request.url}\")\n","                logger.error(f\"Request headers: {response.request.headers}\")\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            # Log a sample of the response content for debugging\n","            try:\n","                response_json = response.json()\n","                if \"value\" in response_json and isinstance(response_json[\"value\"], list):\n","                    logger.info(f\"Response contains {len(response_json['value'])} items in 'value' array\")\n","                return response_json\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Failed to parse response as JSON: {str(e)}\")\n","                logger.error(f\"Response content: {response.text[:1000]}\")  # Log first 1000 chars of response\n","                raise\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 8 - Get Gateway IDs Function\n","# ==================================\n","def get_gateway_ids() -> List[str]:\n","    \"\"\"\n","    Retrieve gateway IDs from the existing pbi_gateways table.\n","    \n","    This function queries the pbi_gateways table to get all gateway IDs\n","    that we need to iterate through to collect datasources.\n","    \n","    Returns:\n","        list: A list of gateway ID strings\n","    \"\"\"\n","    try:\n","        logger.info(f\"Querying {CONFIG['GATEWAYS_TABLE_NAME']} table for gateway IDs...\")\n","        \n","        # Query the gateways table to get all gateway IDs\n","        gateway_ids_df = spark.sql(f\"SELECT id FROM {CONFIG['GATEWAYS_TABLE_NAME']}\")\n","        \n","        # Convert to list of strings\n","        gateway_ids = [row.id for row in gateway_ids_df.collect()]\n","        \n","        logger.info(f\"Found {len(gateway_ids)} gateways to process\")\n","        \n","        if CONFIG['DEBUG_MODE'] and gateway_ids:\n","            logger.info(f\"Sample gateway IDs: {gateway_ids[:5]}\")  # Show first 5\n","        \n","        return gateway_ids\n","        \n","    except Exception as e:\n","        logger.error(f\"Failed to retrieve gateway IDs from {CONFIG['GATEWAYS_TABLE_NAME']}: {str(e)}\")\n","        logger.error(\"Make sure the pbi_gateways table exists and contains gateway data\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 9 - Get Datasources for Gateway Function\n","# ==================================\n","def get_datasources_for_gateway(gateway_id: str, access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve datasources for a specific gateway from the Power BI API.\n","    \n","    This function makes a request to the Get Datasources API endpoint for\n","    a specific gateway and returns all datasources configured for that gateway.\n","    \n","    Args:\n","        gateway_id: The unique identifier of the gateway\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of datasource objects for the specified gateway\n","    \"\"\"\n","    try:\n","        # Build the endpoint URL for this specific gateway\n","        endpoint = CONFIG['DATASOURCES_ENDPOINT'].format(gatewayId=gateway_id)\n","        \n","        logger.info(f\"Retrieving datasources for gateway: {gateway_id}\")\n","        \n","        # Make the API call\n","        response_data = call_powerbi_api(endpoint, access_token)\n","        \n","        # Extract datasources from the response\n","        datasources = response_data.get(\"value\", [])\n","        \n","        logger.info(f\"Retrieved {len(datasources)} datasources for gateway {gateway_id}\")\n","        \n","        # Log first datasource for debugging\n","        if CONFIG['DEBUG_MODE'] and datasources:\n","            logger.info(f\"Sample datasource: {json.dumps(datasources[0], indent=2)}\")\n","        \n","        return datasources\n","        \n","    except requests.exceptions.RequestException as e:\n","        # Log the error but don't fail the entire process\n","        logger.warning(f\"Failed to retrieve datasources for gateway {gateway_id}: {str(e)}\")\n","        return []\n","    except Exception as e:\n","        logger.error(f\"Unexpected error retrieving datasources for gateway {gateway_id}: {str(e)}\")\n","        return []\n","# ==================================\n","\n","\n","# CELL 10 - Get All Datasources Function\n","# ==================================\n","def get_all_datasources(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve datasources from all gateways.\n","    \n","    This function:\n","    1. Gets all gateway IDs from the pbi_gateways table\n","    2. Iterates through each gateway to collect its datasources\n","    3. Combines all datasources into a single list\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all datasource objects from all gateways\n","    \"\"\"\n","    all_datasources = []\n","    \n","    # Get all gateway IDs\n","    gateway_ids = get_gateway_ids()\n","    \n","    if not gateway_ids:\n","        logger.warning(\"No gateway IDs found. Cannot retrieve datasources.\")\n","        return []\n","    \n","    # Process each gateway\n","    total_gateways = len(gateway_ids)\n","    for i, gateway_id in enumerate(gateway_ids, 1):\n","        logger.info(f\"Processing gateway {i}/{total_gateways}: {gateway_id}\")\n","        \n","        # Get datasources for this gateway\n","        gateway_datasources = get_datasources_for_gateway(gateway_id, access_token)\n","        \n","        if gateway_datasources:\n","            all_datasources.extend(gateway_datasources)\n","            logger.info(f\"Added {len(gateway_datasources)} datasources. Running total: {len(all_datasources)}\")\n","        else:\n","            logger.info(f\"No datasources found for gateway {gateway_id}\")\n","        \n","        # Add a small delay between gateway calls to be respectful to the API\n","        if i < total_gateways:  # Don't sleep after the last gateway\n","            time.sleep(0.5)\n","    \n","    logger.info(f\"Finished retrieving datasources from all gateways. Total count: {len(all_datasources)}\")\n","    return all_datasources\n","# ==================================\n","\n","\n","# CELL 11 - Create Enhanced DataFrame Function\n","# ==================================\n","def create_enhanced_datasources_dataframe(datasources_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the datasources data into an enhanced PySpark DataFrame for Delta Lake.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the datasource data\n","    - Extracts the required fields (datasource_id, gateway_id, etc.)\n","    - Adds metadata columns for tracking\n","    \n","    Args:\n","        datasources_data: List of datasource dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: An enhanced PySpark DataFrame ready for Delta Lake\n","    \"\"\"\n","    # Extract only the fields we need from each datasource\n","    simplified_datasources = []\n","    \n","    for datasource in datasources_data:\n","        # Extract the core fields based on the API response structure\n","        simplified_datasource = {\n","            \"datasource_id\": datasource.get(\"id\"),\n","            \"gateway_id\": datasource.get(\"gatewayId\"),\n","            \"datasource_name\": datasource.get(\"datasourceName\"),\n","            \"datasource_type\": datasource.get(\"datasourceType\"),\n","            \"credential_type\": datasource.get(\"credentialType\"),\n","            \"connection_details\": datasource.get(\"connectionDetails\")  # Keep as raw JSON string\n","        }\n","        simplified_datasources.append(simplified_datasource)\n","    \n","    # Define the schema with the specific fields we need\n","    schema = StructType([\n","        StructField(\"datasource_id\", StringType(), False),    # False = not nullable (PRIMARY KEY)\n","        StructField(\"gateway_id\", StringType(), False),       # False = not nullable (FOREIGN KEY)\n","        StructField(\"datasource_name\", StringType(), True),   # True = nullable\n","        StructField(\"datasource_type\", StringType(), True),   # True = nullable\n","        StructField(\"credential_type\", StringType(), True),   # True = nullable\n","        StructField(\"connection_details\", StringType(), True), # True = nullable\n","        StructField(\"extraction_timestamp\", TimestampType(), False)  # False = not nullable\n","    ])\n","    \n","    # Convert the data to a DataFrame\n","    if not simplified_datasources:\n","        logger.warning(\"No datasources found. Creating empty DataFrame.\")\n","        # Create an empty DataFrame with the schema\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        enhanced_df = spark.createDataFrame(empty_rdd, schema)\n","        return enhanced_df\n","    \n","    # Convert to pandas DataFrame first\n","    pandas_df = pd.DataFrame(simplified_datasources)\n","    \n","    # Create the initial Spark DataFrame\n","    # We don't include extraction_timestamp here as we'll add it next\n","    required_columns = [\"datasource_id\", \"gateway_id\", \"datasource_name\", \"datasource_type\", \"credential_type\", \"connection_details\"]\n","    \n","    # Ensure all columns exist in the pandas DataFrame\n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            pandas_df[col_name] = None\n","    \n","    spark_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Add metadata column for tracking when this data was extracted\n","    enhanced_df = spark_df.withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    return enhanced_df\n","# ==================================\n","\n","\n","# CELL 12 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists, creating it if necessary.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n","        logger.info(f\"Delta table '{table_name}' already exists\")\n","    except Exception:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}'\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table (without partitioning)\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_data_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new datasource data into the Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if datasource ID matches\n","    - Inserts new records if datasource ID doesn't exist\n","    \n","    Args:\n","        source_df: DataFrame with new data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"datasource_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    if spark.table(table_name).count() == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Perform the merge operation\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING datasource_updates AS source\n","    ON target.datasource_id = source.datasource_id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.gateway_id = source.gateway_id,\n","            target.datasource_name = source.datasource_name,\n","            target.datasource_type = source.datasource_type,\n","            target.credential_type = source.credential_type,\n","            target.connection_details = source.connection_details,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT *\n","    \"\"\"\n","    \n","    spark.sql(merge_query)\n","    logger.info(\"Merge operation completed successfully\")\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Updates table statistics for query optimization\n","    - Uses a more compatible method for Microsoft Fabric\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # Note: In Microsoft Fabric, Delta table optimization may be handled automatically\n","        # or through different commands than traditional Delta Lake\n","        # The standard OPTIMIZE and ZORDER commands might not be available\n","        \n","        # Alternative approach: Use Delta table properties to hint at optimization\n","        delta_table = DeltaTable.forName(spark, table_name)\n","        delta_table_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n","        \n","        logger.info(\"Delta table optimization completed via statistics computation\")\n","        logger.info(\"Note: Microsoft Fabric may automatically optimize Delta tables\")\n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 13 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all datasources from all gateways via the API\n","    3. Creates an enhanced PySpark DataFrame with the datasource data\n","    4. Loads data into a Delta Lake table\n","    5. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Power BI Gateway Datasources to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 1.5: Validate the token\n","        if not validate_access_token(access_token):\n","            raise Exception(\"Access token validation failed. Please check your permissions and try again.\")\n","        \n","        # Step 2: Retrieve all datasources from all gateways\n","        logger.info(\"Retrieving datasources from Power BI API...\")\n","        datasources_data = get_all_datasources(access_token)\n","        \n","        if not datasources_data:\n","            logger.warning(\"No datasources found. Please check your permissions and gateway access.\")\n","            # Create empty dataframe with schema for consistent table structure\n","            empty_schema = StructType([\n","                StructField(\"datasource_id\", StringType(), False),\n","                StructField(\"gateway_id\", StringType(), False),\n","                StructField(\"datasource_name\", StringType(), True),\n","                StructField(\"datasource_type\", StringType(), True),\n","                StructField(\"credential_type\", StringType(), True),\n","                StructField(\"connection_details\", StringType(), True),\n","                StructField(\"extraction_timestamp\", TimestampType(), False)\n","            ])\n","            datasources_df = spark.createDataFrame([], empty_schema)\n","        else:\n","            # Step 3: Create enhanced DataFrame\n","            logger.info(f\"Creating DataFrame for {len(datasources_data)} datasources...\")\n","            datasources_df = create_enhanced_datasources_dataframe(datasources_data)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of enhanced datasources data:\")\n","        datasources_df.show(5, truncate=False)\n","        \n","        # Step 4: Prepare Delta table\n","        table_name = CONFIG[\"DATASOURCES_TABLE_NAME\"]\n","        ensure_delta_table_exists(table_name, datasources_df.schema)\n","        \n","        # Step 5: Merge data into Delta table (if we have data)\n","        if datasources_data:\n","            merge_data_to_delta(datasources_df, table_name)\n","            \n","            # Step 6: Optimize the Delta table\n","            optimize_delta_table(table_name)\n","        \n","        # Step 7: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        spark.sql(f\"DESCRIBE DETAIL {table_name}\").show(truncate=False)\n","        \n","        # Show row count\n","        row_count = spark.table(table_name).count()\n","        logger.info(f\"Total rows in {table_name}: {row_count}\")\n","        \n","        # Show summary statistics\n","        summary_stats = spark.sql(f\"\"\"\n","            SELECT \n","                COUNT(*) as total_datasources,\n","                COUNT(DISTINCT gateway_id) as unique_gateways,\n","                COUNT(DISTINCT datasource_type) as datasource_types,\n","                COUNT(DISTINCT credential_type) as credential_types,\n","                MAX(extraction_timestamp) as last_updated\n","            FROM {table_name}\n","        \"\"\")\n","        \n","        logger.info(\"Summary statistics:\")\n","        summary_stats.show(truncate=False)\n","        \n","        # Optional: Show distribution by datasource type\n","        type_distribution = spark.sql(f\"\"\"\n","            SELECT \n","                datasource_type,\n","                COUNT(*) as count\n","            FROM {table_name}\n","            GROUP BY datasource_type\n","            ORDER BY count DESC\n","        \"\"\")\n","        \n","        logger.info(\"Datasource distribution by type:\")\n","        type_distribution.show(truncate=False)\n","        \n","        # Optional: Show distribution by credential type\n","        credential_distribution = spark.sql(f\"\"\"\n","            SELECT \n","                credential_type,\n","                COUNT(*) as count\n","            FROM {table_name}\n","            GROUP BY credential_type\n","            ORDER BY count DESC\n","        \"\"\")\n","        \n","        logger.info(\"Datasource distribution by credential type:\")\n","        credential_distribution.show(truncate=False)\n","        \n","        return datasources_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 14 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    datasources_df = main()\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"e69eb60a-ecf1-4ce9-8d24-d091e8c1cbc6","normalized_state":"finished","queued_time":"2025-07-16T19:08:16.9688321Z","session_start_time":null,"execution_start_time":"2025-07-16T19:08:29.4492503Z","execution_finish_time":"2025-07-16T19:10:17.4399568Z","parent_msg_id":"e82e12f8-1b03-4397-955b-d2f556612d90"},"text/plain":"StatementMeta(, e69eb60a-ecf1-4ce9-8d24-d091e8c1cbc6, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-16 19:08:30,066 - INFO - Starting Power BI Gateway Datasources to Delta Lake process\n2025-07-16 19:08:30,068 - INFO - Getting access token...\n2025-07-16 19:08:30,069 - INFO - Attempting to get token using mssparkutils.credentials.getToken()...\n2025-07-16 19:09:28,914 - WARNING - Primary token method failed: An error occurred while calling z:mssparkutils.credentials.getToken.\n: java.io.IOException: 500 {\"code\":\"INTERNAL_ERROR\",\"subCode\":0,\"message\":\"An internal error occurred.\",\"timeStamp\":\"2025-07-16T19:09:28.8583809Z\",\"httpStatusCode\":500,\"hresult\":-2147467259,\"details\":[{\"code\":\"RootActivityId\",\"message\":\"95c3c4e8-a4c0-4229-967c-7c8fb5c7872f\"},{\"code\":\"Category\",\"message\":\"System\"},{\"code\":\"Source\",\"message\":\"TM\"}]}\n\tat com.microsoft.azure.trident.tokenlibrary.TokenLibrary.getAccessToken(TokenLibrary.scala:532)\n\tat com.microsoft.azure.trident.tokenlibrary.TokenLibrary.getAccessToken(TokenLibrary.scala:448)\n\tat com.microsoft.azure.trident.tokenlibrary.TokenLibrary$.getAccessToken(TokenLibrary.scala:1324)\n\tat mssparkutils.credentials$.getToken(credentials.scala:41)\n\tat mssparkutils.credentials.getToken(credentials.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n2025-07-16 19:09:28,916 - INFO - Trying alternative token methods...\n2025-07-16 19:09:28,916 - INFO - Attempting to get token using alternative endpoint...\n2025-07-16 19:09:29,627 - INFO - Successfully obtained token using alternative endpoint\n2025-07-16 19:09:29,628 - INFO - Successfully obtained access token\n2025-07-16 19:09:29,628 - INFO - Validating access token...\n2025-07-16 19:09:29,929 - INFO - Access token validation successful\n2025-07-16 19:09:29,932 - INFO - Retrieving datasources from Power BI API...\n2025-07-16 19:09:29,932 - INFO - Querying pbi_gateways table for gateway IDs...\n2025-07-16 19:09:44,161 - INFO - Found 14 gateways to process\n2025-07-16 19:09:44,162 - INFO - Sample gateway IDs: ['211a986c-1e4a-4029-a064-126e534f40bb', '2d42f13d-1ae8-4d16-9726-82385a865ddf', 'ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194', '47d3ea9b-fb98-4e41-8516-2aa7c02b7eca', '59ce4f51-a88c-4609-a60a-848454a36b90']\n2025-07-16 19:09:44,162 - INFO - Processing gateway 1/14: 211a986c-1e4a-4029-a064-126e534f40bb\n2025-07-16 19:09:44,163 - INFO - Retrieving datasources for gateway: 211a986c-1e4a-4029-a064-126e534f40bb\n2025-07-16 19:09:44,164 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/211a986c-1e4a-4029-a064-126e534f40bb/datasources with params: None (Attempt 1)\n2025-07-16 19:09:44,400 - INFO - Response status: 200\n2025-07-16 19:09:44,400 - INFO - Response contains 0 items in 'value' array\n2025-07-16 19:09:44,403 - INFO - Retrieved 0 datasources for gateway 211a986c-1e4a-4029-a064-126e534f40bb\n2025-07-16 19:09:44,404 - INFO - No datasources found for gateway 211a986c-1e4a-4029-a064-126e534f40bb\n2025-07-16 19:09:44,904 - INFO - Processing gateway 2/14: 2d42f13d-1ae8-4d16-9726-82385a865ddf\n2025-07-16 19:09:44,907 - INFO - Retrieving datasources for gateway: 2d42f13d-1ae8-4d16-9726-82385a865ddf\n2025-07-16 19:09:44,907 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/2d42f13d-1ae8-4d16-9726-82385a865ddf/datasources with params: None (Attempt 1)\n2025-07-16 19:09:45,093 - INFO - Response status: 200\n2025-07-16 19:09:45,094 - INFO - Response contains 2 items in 'value' array\n2025-07-16 19:09:45,097 - INFO - Retrieved 2 datasources for gateway 2d42f13d-1ae8-4d16-9726-82385a865ddf\n2025-07-16 19:09:45,098 - INFO - Sample datasource: {\n  \"id\": \"748c4bb9-c2db-4bb3-b7fd-669e2d50b618\",\n  \"gatewayId\": \"2d42f13d-1ae8-4d16-9726-82385a865ddf\",\n  \"datasourceType\": \"Web\",\n  \"connectionDetails\": \"{\\\"url\\\":\\\"https://mdandersonorg.sharepoint.com/teams/COVIDSurgeRadOncClinicalStaffReduction2/Shared%20Documents/General/Daily%20Clinical%20Staff%20Update.xlsx\\\"}\",\n  \"credentialType\": \"OAuth2\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"https://mdandersonorg.sharepoint.com/teams/COVIDSurgeRadOncClinicalStaffReduction2/Shared%20Documents/General/Daily%20Clinical%20Staff%20Update.xlsx\"\n}\n2025-07-16 19:09:45,098 - INFO - Added 2 datasources. Running total: 2\n2025-07-16 19:09:45,598 - INFO - Processing gateway 3/14: ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194\n2025-07-16 19:09:45,599 - INFO - Retrieving datasources for gateway: ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194\n2025-07-16 19:09:45,599 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194/datasources with params: None (Attempt 1)\n2025-07-16 19:09:45,879 - INFO - Response status: 200\n2025-07-16 19:09:45,880 - INFO - Response contains 1 items in 'value' array\n2025-07-16 19:09:45,883 - INFO - Retrieved 1 datasources for gateway ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194\n2025-07-16 19:09:45,884 - INFO - Sample datasource: {\n  \"id\": \"afb36076-eb38-4245-ae63-201de90ef6bd\",\n  \"gatewayId\": \"ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"d1pwlcpttprpts\\\",\\\"database\\\":\\\"ReminderMsgSMS\\\"}\",\n  \"credentialType\": \"Basic\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"SQL\"\n}\n2025-07-16 19:09:45,884 - INFO - Added 1 datasources. Running total: 3\n2025-07-16 19:09:46,384 - INFO - Processing gateway 4/14: 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca\n2025-07-16 19:09:46,385 - INFO - Retrieving datasources for gateway: 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca\n2025-07-16 19:09:46,385 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/47d3ea9b-fb98-4e41-8516-2aa7c02b7eca/datasources with params: None (Attempt 1)\n2025-07-16 19:09:46,600 - INFO - Response status: 200\n2025-07-16 19:09:46,601 - INFO - Response contains 93 items in 'value' array\n2025-07-16 19:09:46,604 - INFO - Retrieved 93 datasources for gateway 47d3ea9b-fb98-4e41-8516-2aa7c02b7eca\n2025-07-16 19:09:46,604 - INFO - Sample datasource: {\n  \"id\": \"ad0309e2-d547-49b2-8233-f4579069af42\",\n  \"gatewayId\": \"47d3ea9b-fb98-4e41-8516-2aa7c02b7eca\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"CDWPRD\\\",\\\"database\\\":\\\"CDW_Reporting\\\"}\",\n  \"credentialType\": \"Windows\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"EDEA_CDW_PRD\"\n}\n2025-07-16 19:09:46,605 - INFO - Added 93 datasources. Running total: 96\n2025-07-16 19:09:47,106 - INFO - Processing gateway 5/14: 59ce4f51-a88c-4609-a60a-848454a36b90\n2025-07-16 19:09:47,107 - INFO - Retrieving datasources for gateway: 59ce4f51-a88c-4609-a60a-848454a36b90\n2025-07-16 19:09:47,107 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/59ce4f51-a88c-4609-a60a-848454a36b90/datasources with params: None (Attempt 1)\n2025-07-16 19:09:47,364 - INFO - Response status: 200\n2025-07-16 19:09:47,365 - INFO - Response contains 1 items in 'value' array\n2025-07-16 19:09:47,368 - INFO - Retrieved 1 datasources for gateway 59ce4f51-a88c-4609-a60a-848454a36b90\n2025-07-16 19:09:47,368 - INFO - Sample datasource: {\n  \"id\": \"e665c60b-6826-4d8f-bf41-cb9506b77722\",\n  \"gatewayId\": \"59ce4f51-a88c-4609-a60a-848454a36b90\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"ROADSQASQL\\\",\\\"database\\\":\\\"ROADS\\\"}\",\n  \"credentialType\": \"Windows\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"ROADSQASQL\"\n}\n2025-07-16 19:09:47,369 - INFO - Added 1 datasources. Running total: 97\n2025-07-16 19:09:47,869 - INFO - Processing gateway 6/14: 08beb069-2a73-4be8-ac0b-1f76bbc12c0c\n2025-07-16 19:09:47,870 - INFO - Retrieving datasources for gateway: 08beb069-2a73-4be8-ac0b-1f76bbc12c0c\n2025-07-16 19:09:47,871 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/08beb069-2a73-4be8-ac0b-1f76bbc12c0c/datasources with params: None (Attempt 1)\n2025-07-16 19:09:48,123 - INFO - Response status: 200\n2025-07-16 19:09:48,124 - INFO - Response contains 2 items in 'value' array\n2025-07-16 19:09:48,126 - INFO - Retrieved 2 datasources for gateway 08beb069-2a73-4be8-ac0b-1f76bbc12c0c\n2025-07-16 19:09:48,127 - INFO - Sample datasource: {\n  \"id\": \"41ca696f-ba89-48df-8f76-a7ab33cc87db\",\n  \"gatewayId\": \"08beb069-2a73-4be8-ac0b-1f76bbc12c0c\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"ROADSSTGSQL\\\",\\\"database\\\":\\\"ROADS\\\"}\",\n  \"credentialType\": \"Windows\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"ROADSSTGSQL\"\n}\n2025-07-16 19:09:48,127 - INFO - Added 2 datasources. Running total: 99\n2025-07-16 19:09:48,628 - INFO - Processing gateway 7/14: c4d85121-85cf-4cad-8fd5-cd375d1781f1\n2025-07-16 19:09:48,629 - INFO - Retrieving datasources for gateway: c4d85121-85cf-4cad-8fd5-cd375d1781f1\n2025-07-16 19:09:48,630 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/c4d85121-85cf-4cad-8fd5-cd375d1781f1/datasources with params: None (Attempt 1)\n2025-07-16 19:09:48,813 - INFO - Response status: 200\n2025-07-16 19:09:48,813 - INFO - Response contains 2 items in 'value' array\n2025-07-16 19:09:48,816 - INFO - Retrieved 2 datasources for gateway c4d85121-85cf-4cad-8fd5-cd375d1781f1\n2025-07-16 19:09:48,817 - INFO - Sample datasource: {\n  \"id\": \"5c63e61f-ddbd-46d4-b099-18d6064b9177\",\n  \"gatewayId\": \"c4d85121-85cf-4cad-8fd5-cd375d1781f1\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"doswiadmindb3\\\",\\\"database\\\":\\\"IPNurseSurvey\\\"}\",\n  \"credentialType\": \"Windows\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"New data source\"\n}\n2025-07-16 19:09:48,817 - INFO - Added 2 datasources. Running total: 101\n2025-07-16 19:09:49,318 - INFO - Processing gateway 8/14: 96262afb-757f-4d58-a258-b8faddca905f\n2025-07-16 19:09:49,319 - INFO - Retrieving datasources for gateway: 96262afb-757f-4d58-a258-b8faddca905f\n2025-07-16 19:09:49,319 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/96262afb-757f-4d58-a258-b8faddca905f/datasources with params: None (Attempt 1)\n2025-07-16 19:09:49,596 - INFO - Response status: 200\n2025-07-16 19:09:49,597 - INFO - Response contains 32 items in 'value' array\n2025-07-16 19:09:49,600 - INFO - Retrieved 32 datasources for gateway 96262afb-757f-4d58-a258-b8faddca905f\n2025-07-16 19:09:49,600 - INFO - Sample datasource: {\n  \"id\": \"aa96cfbb-7dd5-4b08-a803-c5cc5389f2f6\",\n  \"gatewayId\": \"96262afb-757f-4d58-a258-b8faddca905f\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"altirispdb\\\",\\\"database\\\":\\\"mda_workflow\\\"}\",\n  \"credentialType\": \"Windows\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"Recv_Altirispdb_PRDDB_PRD\"\n}\n2025-07-16 19:09:49,600 - INFO - Added 32 datasources. Running total: 133\n2025-07-16 19:09:50,101 - INFO - Processing gateway 9/14: ebbbe00f-2b98-441b-92c0-1d820f987d8a\n2025-07-16 19:09:50,101 - INFO - Retrieving datasources for gateway: ebbbe00f-2b98-441b-92c0-1d820f987d8a\n2025-07-16 19:09:50,102 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/ebbbe00f-2b98-441b-92c0-1d820f987d8a/datasources with params: None (Attempt 1)\n2025-07-16 19:09:50,457 - INFO - Response status: 200\n2025-07-16 19:09:50,458 - INFO - Response contains 9 items in 'value' array\n2025-07-16 19:09:50,461 - INFO - Retrieved 9 datasources for gateway ebbbe00f-2b98-441b-92c0-1d820f987d8a\n2025-07-16 19:09:50,461 - INFO - Sample datasource: {\n  \"id\": \"217817d0-0663-4512-83a4-315e1de92a26\",\n  \"gatewayId\": \"ebbbe00f-2b98-441b-92c0-1d820f987d8a\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"ehrclrdbsupq\\\",\\\"database\\\":\\\"CLARITY\\\"}\",\n  \"credentialType\": \"Windows\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"None\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"ehrclrdbsupq;CLARITY\"\n}\n2025-07-16 19:09:50,462 - INFO - Added 9 datasources. Running total: 142\n2025-07-16 19:09:50,963 - INFO - Processing gateway 10/14: 354cf43d-a895-4cab-b3ae-0c622559760b\n2025-07-16 19:09:50,963 - INFO - Retrieving datasources for gateway: 354cf43d-a895-4cab-b3ae-0c622559760b\n2025-07-16 19:09:50,963 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/354cf43d-a895-4cab-b3ae-0c622559760b/datasources with params: None (Attempt 1)\n2025-07-16 19:09:51,184 - INFO - Response status: 200\n2025-07-16 19:09:51,185 - INFO - Response contains 7 items in 'value' array\n2025-07-16 19:09:51,187 - INFO - Retrieved 7 datasources for gateway 354cf43d-a895-4cab-b3ae-0c622559760b\n2025-07-16 19:09:51,188 - INFO - Sample datasource: {\n  \"id\": \"ebaea47b-e8db-4573-9b1e-ea9505418fb7\",\n  \"gatewayId\": \"354cf43d-a895-4cab-b3ae-0c622559760b\",\n  \"datasourceType\": \"Sql\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"fpa_prod\\\",\\\"database\\\":\\\"fpa_jduf_p\\\"}\",\n  \"credentialType\": \"Windows\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"FPA_JDUF_PRDDB\"\n}\n2025-07-16 19:09:51,189 - INFO - Added 7 datasources. Running total: 149\n2025-07-16 19:09:51,690 - INFO - Processing gateway 11/14: 98fd034b-823c-4e36-8088-c85b8bdd8950\n2025-07-16 19:09:51,690 - INFO - Retrieving datasources for gateway: 98fd034b-823c-4e36-8088-c85b8bdd8950\n2025-07-16 19:09:51,691 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/98fd034b-823c-4e36-8088-c85b8bdd8950/datasources with params: None (Attempt 1)\n2025-07-16 19:09:51,864 - INFO - Response status: 200\n2025-07-16 19:09:51,865 - INFO - Response contains 1 items in 'value' array\n2025-07-16 19:09:51,867 - INFO - Retrieved 1 datasources for gateway 98fd034b-823c-4e36-8088-c85b8bdd8950\n2025-07-16 19:09:51,868 - INFO - Sample datasource: {\n  \"id\": \"26b2278c-86c0-4cd1-9188-f0279a3117b6\",\n  \"gatewayId\": \"98fd034b-823c-4e36-8088-c85b8bdd8950\",\n  \"datasourceType\": \"Oracle\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"oncoreprd\\\"}\",\n  \"credentialType\": \"Basic\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"CR_OnCore_ProdDB_PRD\"\n}\n2025-07-16 19:09:51,868 - INFO - Added 1 datasources. Running total: 150\n2025-07-16 19:09:52,369 - INFO - Processing gateway 12/14: 538397fa-2a60-4114-a959-b59cd76eb5e6\n2025-07-16 19:09:52,369 - INFO - Retrieving datasources for gateway: 538397fa-2a60-4114-a959-b59cd76eb5e6\n2025-07-16 19:09:52,369 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/538397fa-2a60-4114-a959-b59cd76eb5e6/datasources with params: None (Attempt 1)\n2025-07-16 19:09:52,518 - INFO - Response status: 200\n2025-07-16 19:09:52,518 - INFO - Response contains 2 items in 'value' array\n2025-07-16 19:09:52,521 - INFO - Retrieved 2 datasources for gateway 538397fa-2a60-4114-a959-b59cd76eb5e6\n2025-07-16 19:09:52,522 - INFO - Sample datasource: {\n  \"id\": \"20b2a954-e851-4e8f-9652-e2e17248c600\",\n  \"gatewayId\": \"538397fa-2a60-4114-a959-b59cd76eb5e6\",\n  \"datasourceType\": \"Oracle\",\n  \"connectionDetails\": \"{\\\"server\\\":\\\"oncoretrn\\\"}\",\n  \"credentialType\": \"Basic\",\n  \"credentialDetails\": {\n    \"privacyLevel\": \"Organizational\",\n    \"useEndUserOAuth2Credentials\": false\n  },\n  \"datasourceName\": \"CR_OnCore_TrnDB_STG\"\n}\n2025-07-16 19:09:52,522 - INFO - Added 2 datasources. Running total: 152\n2025-07-16 19:09:53,022 - INFO - Processing gateway 13/14: 7505cd2d-8438-4fac-9b32-d43a1182d32e\n2025-07-16 19:09:53,023 - INFO - Retrieving datasources for gateway: 7505cd2d-8438-4fac-9b32-d43a1182d32e\n2025-07-16 19:09:53,023 - INFO - Making API call to: https://api.powerbi.com/v1.0/myorg/gateways/7505cd2d-8438-4fac-9b32-d43a1182d32e/datasources with params: None (Attempt 1)\n2025-07-16 19:09:57,323 - INFO - Delta table 'pbi_gateways_datasources' already exists\n2025-07-16 19:09:57,323 - INFO - Starting merge operation for pbi_gateways_datasources\n2025-07-16 19:10:10,024 - INFO - Merge operation completed successfully\n2025-07-16 19:10:10,025 - INFO - Optimizing Delta table 'pbi_gateways_datasources'\n2025-07-16 19:10:12,728 - INFO - Table statistics updated successfully\n"]},{"output_type":"stream","name":"stdout","text":["+------------------------------------+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+---------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+\n|datasource_id                       |gateway_id                          |datasource_name                                                                                                                                     |datasource_type|credential_type|connection_details                                                                                                                                            |extraction_timestamp      |\n+------------------------------------+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+---------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+\n|748c4bb9-c2db-4bb3-b7fd-669e2d50b618|2d42f13d-1ae8-4d16-9726-82385a865ddf|https://mdandersonorg.sharepoint.com/teams/COVIDSurgeRadOncClinicalStaffReduction2/Shared%20Documents/General/Daily%20Clinical%20Staff%20Update.xlsx|Web            |OAuth2         |{\"url\":\"https://mdandersonorg.sharepoint.com/teams/COVIDSurgeRadOncClinicalStaffReduction2/Shared%20Documents/General/Daily%20Clinical%20Staff%20Update.xlsx\"}|2025-07-16 19:09:56.433739|\n|1ce8fb0c-9519-41ac-ab0e-2ab3058af6f2|2d42f13d-1ae8-4d16-9726-82385a865ddf|https://mdandersonorg-my.sharepoint.com/personal/kdnguyen1_mdanderson_org                                                                           |Web            |OAuth2         |{\"url\":\"https://mdandersonorg-my.sharepoint.com/personal/kdnguyen1_mdanderson_org\"}                                                                           |2025-07-16 19:09:56.433739|\n|afb36076-eb38-4245-ae63-201de90ef6bd|ee59c3da-2d84-49ce-bd6f-9ebe8ed2f194|SQL                                                                                                                                                 |Sql            |Basic          |{\"server\":\"d1pwlcpttprpts\",\"database\":\"ReminderMsgSMS\"}                                                                                                       |2025-07-16 19:09:56.433739|\n|ad0309e2-d547-49b2-8233-f4579069af42|47d3ea9b-fb98-4e41-8516-2aa7c02b7eca|EDEA_CDW_PRD                                                                                                                                        |Sql            |Windows        |{\"server\":\"CDWPRD\",\"database\":\"CDW_Reporting\"}                                                                                                                |2025-07-16 19:09:56.433739|\n|a9731e5d-288b-4f59-940b-24f8b256a158|47d3ea9b-fb98-4e41-8516-2aa7c02b7eca|Recv_Altirispdb_DEV                                                                                                                                 |Sql            |Windows        |{\"server\":\"altirispdb\",\"database\":\"MDA_Workflow\"}                                                                                                             |2025-07-16 19:09:56.433739|\n+------------------------------------+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+---------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+\nonly showing top 5 rows\n\n+----------------+-----+\n|datasource_type |count|\n+----------------+-----+\n|Sql             |103  |\n|Oracle          |35   |\n|Folder          |4    |\n|PostgreSql      |3    |\n|AnalysisServices|3    |\n|Web             |3    |\n|File            |2    |\n|Extension       |1    |\n|MySql           |1    |\n|OData           |1    |\n|Essbase         |1    |\n+----------------+-----+\n\n+---------------+-----+\n|credential_type|count|\n+---------------+-----+\n|Windows        |88   |\n|Basic          |65   |\n|OAuth2         |3    |\n|Anonymous      |1    |\n+---------------+-----+\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"60fa7194-8331-404a-9280-14cce73c9a2b"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE pbi_gateways_datasources\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"e69eb60a-ecf1-4ce9-8d24-d091e8c1cbc6","normalized_state":"finished","queued_time":"2025-07-16T19:08:17.0579895Z","session_start_time":null,"execution_start_time":"2025-07-16T19:10:17.4419907Z","execution_finish_time":"2025-07-16T19:10:18.2992706Z","parent_msg_id":"ed14aea7-e634-4375-bd3e-c07002aec6b0"},"text/plain":"StatementMeta(, e69eb60a-ecf1-4ce9-8d24-d091e8c1cbc6, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1395468a-a285-4eb3-b614-15df40d2373a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}