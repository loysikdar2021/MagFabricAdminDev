{"cells":[{"cell_type":"markdown","source":["This is for the Power BI API\n","\n","Admin - Reports GetReportsAsAdmin\n","\n","https://learn.microsoft.com/en-us/rest/api/power-bi/admin/reports-get-reports-as-admin\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ec5657be-0fd8-4b30-8faf-457c62f7e977"},{"cell_type":"code","source":["import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, from_json, when, isnotnull, to_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","from datetime import datetime"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":46,"statement_ids":[46],"state":"finished","livy_statement_state":"available","session_id":"91348d4f-537b-489b-87ac-8aad18f36676","normalized_state":"finished","queued_time":"2025-06-06T21:18:45.6666819Z","session_start_time":null,"execution_start_time":"2025-06-06T21:18:45.6678455Z","execution_finish_time":"2025-06-06T21:18:46.1316744Z","parent_msg_id":"53b18a43-7c9d-47e1-b824-2e4095d854dc"},"text/plain":"StatementMeta(, 91348d4f-537b-489b-87ac-8aad18f36676, 46, Finished, Available, Finished)"},"metadata":{}}],"execution_count":44,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9e3cf6d5-c834-4509-8cfa-72c246912b57"},{"cell_type":"code","source":["lakehouseTableName = \"pbi_admin_reports\"\n","\n","url = f\"https://api.powerbi.com/v1.0/myorg/admin/reports\"\n","\n","schema = StructType([\n","  StructField(\"appId\", StringType(), True),\n","  StructField(\"createdBy\", StringType(), True),\n","  StructField(\"createdDateTime\", TimestampType(), True),\n","  StructField(\"datasetId\", StringType(), True),\n","  StructField(\"description\", StringType(), True),\n","  StructField(\"embedUrl\", StringType(), True),\n","  StructField(\"id\", StringType(), True),\n","  StructField(\"modifiedBy\", StringType(), True),\n","  StructField(\"modifiedDateTime\", TimestampType(), True),\n","  StructField(\"name\", StringType(), True),\n","  StructField(\"originalReportId\", StringType(), True),\n","  StructField(\"reportType\", StringType(), True),\n","  StructField(\"webUrl\", StringType(), True),\n","  StructField(\"workspaceId\", StringType(), True),\n","  StructField(\"extraction_timestamp\", TimestampType(), True),\n","])\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":47,"statement_ids":[47],"state":"finished","livy_statement_state":"available","session_id":"91348d4f-537b-489b-87ac-8aad18f36676","normalized_state":"finished","queued_time":"2025-06-06T21:18:45.7478932Z","session_start_time":null,"execution_start_time":"2025-06-06T21:18:46.1336279Z","execution_finish_time":"2025-06-06T21:18:46.5635079Z","parent_msg_id":"90d67e14-032f-4c35-b845-17414aa9e683"},"text/plain":"StatementMeta(, 91348d4f-537b-489b-87ac-8aad18f36676, 47, Finished, Available, Finished)"},"metadata":{}}],"execution_count":45,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4528c4d0-186b-446d-aea8-f84d4b039162"},{"cell_type":"code","source":["logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Just making a dictionary of names from the schema above\n","# This will be used:\n","#   As a set of \"required fields\"\n","#   Sets the order of fields in the dataframe\n","#     and lakehouse\n","columns = {field.name for field in schema}\n","\n","# ==================================================================================================\n","# The reason for this function, is that I wanted the Lakehouse to actually use a date/time datatype for dates and times rather than just strings.\n","# Because of that, the dataframes used to put data into the Lakehouse need to have their schema be something other than all StringType()\n","# TimestampType() works, but expects the string date to be able to convert by using something like to_timestamp()\n","# to_timestamp() has expectations as well.\n","# The string date coming ouf of the API is not in a good and standard format for these things.\n","# So, this function is to try and clean up the data BEFORE it gets to the dataframes\n","# It uses standard python functions to try and clean up the strings. Like:\n","#   Removing \"Z\" at the end of the date string (00:00:00.00Z -> 00:00:00.00)\n","#   Making sure the fraction of seconds:\n","#     Is 6 numbers long, so it pads zeros (00:00:00.00 -> 00:00:00.000000)\n","#     Exists by adding in missing zeros (00:00:00 -> 00:00:00.000000)\n","# Finally, it converts that cleaned up string to a python datetime type and returns it.\n","def clean_for_timestamp(dtstring):\n","    if dtstring is None:\n","        return None\n","    else:\n","        dtstring = dtstring.rstrip(\"Z\")\n","        parts = dtstring.split(\".\")\n","\n","        if len(parts) > 1:\n","            clean = f\"{parts[0]}.{parts[1].ljust(6, '0')}\"\n","        else:\n","            clean = f\"{dtstring}.000000\"\n","\n","        return datetime.strptime(clean, \"%Y-%m-%dT%H:%M:%S.%f\")\n","\n","# ==================================================================================================\n","# This function is used to prepare the response data by converting strings to types before loading a dataframe\n","def clean_response_data(input_list):\n","    # Sometimes an API might not return all possible fields. Fill in any missing ones with None.\n","    normalized_data = [{key: d.get(key, None) for key in columns} for d in input_list]\n","\n","    # Go through the API schema and get a list of columns that are not StringType() or something special.\n","    # For now, we are only handling TimestampType().\n","    # So, if the schema has a timestamp, then it'll try to convert the corrosponding key's data in\n","    #   each dictionary (each dictionary in the list is a row of data coming out of the API)\n","\n","    # Get the ScructFields from the schema that are TimestampType\n","    timestamp_fields = [field for field in schema if isinstance(field.dataType, TimestampType)]\n","\n","    for d in normalized_data:\n","        for field in timestamp_fields:\n","            d[field.name] = clean_for_timestamp(d[field.name])\n","\n","    return normalized_data"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":48,"statement_ids":[48],"state":"finished","livy_statement_state":"available","session_id":"91348d4f-537b-489b-87ac-8aad18f36676","normalized_state":"finished","queued_time":"2025-06-06T21:18:45.7815713Z","session_start_time":null,"execution_start_time":"2025-06-06T21:18:46.565817Z","execution_finish_time":"2025-06-06T21:18:46.9498307Z","parent_msg_id":"f908f264-5aa9-49b4-82a4-51d484976688"},"text/plain":"StatementMeta(, 91348d4f-537b-489b-87ac-8aad18f36676, 48, Finished, Available, Finished)"},"metadata":{}}],"execution_count":46,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"efb00a3a-0571-4aab-86d0-efc184c6c320"},{"cell_type":"code","source":["# Keeping the actual API call in a separate cell\n","# That way, during debugging, you can skip running this cell and prevent hitting an API call limit\n","# Just manually run each cell needed and skip this one\n","# The data in the response variable will stay there for a while (until the session goes idle??)\n","\n","token = notebookutils.credentials.getToken(\"pbi\")\n","headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n","response = requests.get(url, headers=headers)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":49,"statement_ids":[49],"state":"finished","livy_statement_state":"available","session_id":"91348d4f-537b-489b-87ac-8aad18f36676","normalized_state":"finished","queued_time":"2025-06-06T21:18:45.83029Z","session_start_time":null,"execution_start_time":"2025-06-06T21:18:46.9519477Z","execution_finish_time":"2025-06-06T21:18:50.717796Z","parent_msg_id":"0f159728-32ab-464a-b35d-3bff171ca611"},"text/plain":"StatementMeta(, 91348d4f-537b-489b-87ac-8aad18f36676, 49, Finished, Available, Finished)"},"metadata":{}}],"execution_count":47,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2ea75531-20a6-48ff-b0a5-0f3123376ff7"},{"cell_type":"code","source":["# response.json() returns a dictionary {}\n","response_dictionary = response.json()\n","\n","# get the \"value\" item from the dictionary. \"value\" is a list of dictionaries [{},{},{},...]\n","response_list = response_dictionary[\"value\"]\n","\n","for item in response_list:\n","    item[\"extraction_timestamp\"] = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n","\n","# clean up and convert any data while keeping it list/dictionary form\n","cleaned_data = clean_response_data(response_list)\n","\n","# create the dataframe\n","spark = SparkSession.builder.getOrCreate()\n","df = spark.createDataFrame(cleaned_data, schema=schema)\n","\n","# Show the dataframe or overwrite the table in the lakehouse.\n","#df.show()\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(lakehouseTableName)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":50,"statement_ids":[50],"state":"finished","livy_statement_state":"available","session_id":"91348d4f-537b-489b-87ac-8aad18f36676","normalized_state":"finished","queued_time":"2025-06-06T21:18:45.8641051Z","session_start_time":null,"execution_start_time":"2025-06-06T21:18:50.7197235Z","execution_finish_time":"2025-06-06T21:18:58.5848206Z","parent_msg_id":"234200ff-5879-4483-ac91-73ff12547836"},"text/plain":"StatementMeta(, 91348d4f-537b-489b-87ac-8aad18f36676, 50, Finished, Available, Finished)"},"metadata":{}}],"execution_count":48,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2c2d11c-f3f3-448d-b70b-ac62c42fb947"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}