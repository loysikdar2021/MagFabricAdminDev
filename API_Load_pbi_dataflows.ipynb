{"cells":[{"cell_type":"code","source":["# DO NOT DELETE THIS CELL\n","\n","# API Name: Admin - Dataflows GetDataflowsAsAdmin\n","# Command:  GET https://api.powerbi.com/v1.0/myorg/admin/dataflows\n","# Doc:      https://learn.microsoft.com/en-us/rest/api/power-bi/admin/dataflows-get-dataflows-as-admin\n","\n","# Loads table: pbi_dataflows"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"08f7bbd2-3093-4584-a86a-d1d10b5b1bdd","normalized_state":"finished","queued_time":"2025-07-17T15:23:24.9526028Z","session_start_time":"2025-07-17T15:23:24.9536468Z","execution_start_time":"2025-07-17T15:23:36.3731767Z","execution_finish_time":"2025-07-17T15:23:36.7847393Z","parent_msg_id":"6bc72396-48c2-43e9-af45-483d8ef84f3e"},"text/plain":"StatementMeta(, 08f7bbd2-3093-4584-a86a-d1d10b5b1bdd, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c08298d-c6c9-4b4d-a240-991632aafa93"},{"cell_type":"code","source":["# CELL 1 - Title and Introduction\n","# ==================================\n","# Power BI Dataflows to Delta Lake - PySpark Notebook\n","# This notebook retrieves Power BI dataflows using GetDataflowsAsAdmin API and loads them \n","# into a Delta Lake table with optimization for analytics workloads\n","# \n","# Table Created:\n","# - pbi_dataflows: Core dataflow metadata\n","# ==================================\n","\n","\n","# CELL 2 - Import Libraries\n","# ==================================\n","# Import required libraries\n","import requests\n","import json\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, current_timestamp, lit, explode, when, coalesce\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, ArrayType\n","import logging\n","from typing import Dict, List, Optional\n","from delta.tables import DeltaTable\n","import random\n","import time\n","import uuid\n","from datetime import datetime\n","# ==================================\n","\n","\n","# CELL 3 - Configure Logging and Initialize Spark\n","# ==================================\n","# Configure logging\n","# This helps us track what's happening in our code and debug issues\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Spark Session with Delta Lake configurations\n","# In Fabric notebooks, Spark is pre-configured with Delta support\n","spark = SparkSession.builder \\\n","    .appName(\"PowerBIDataflowsToDelta\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Set optimal configurations for Delta operations\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","# ==================================\n","\n","\n","# CELL 4 - Configuration Parameters\n","# ==================================\n","# Configuration Parameters\n","# These are the settings we'll use throughout the notebook\n","CONFIG = {\n","    \"API_BASE_URL\": \"https://api.powerbi.com/v1.0/myorg\",\n","    \"DATAFLOWS_ENDPOINT\": \"/admin/dataflows\",  # GetDataflowsAsAdmin endpoint\n","    \"MAX_RETRIES\": 5,  # Number of retries for handling rate limits\n","    \"INITIAL_BACKOFF_SEC\": 1,  # Initial backoff time in seconds\n","    \"MAX_BACKOFF_SEC\": 60,  # Maximum backoff time in seconds\n","    \"BACKOFF_FACTOR\": 2,  # Exponential backoff multiplier\n","    \"JITTER_FACTOR\": 0.1,  # Random jitter to add to backoff (as a fraction)\n","    \"TIMEOUT\": 30,  # API request timeout in seconds\n","    \"DATAFLOWS_TABLE_NAME\": \"pbi_dataflows\",  # Primary table for dataflow metadata\n","    \"LAKEHOUSE_PATH\": \"Tables\",  # Default Tables folder in Fabric Lakehouse\n","    \"DEBUG_MODE\": True,  # Set to True to enable extra debugging output\n","    \"MAX_REQUESTS_PER_HOUR\": 200  # API rate limit\n","}\n","\n","# Generate a unique batch ID for this extraction run (removed - no longer needed)\n","logger.info(\"Starting Power BI Dataflows extraction\")\n","# ==================================\n","\n","\n","# CELL 5 - Authentication Function\n","# ==================================\n","def get_access_token():\n","    \"\"\"\n","    Get Azure AD access token for Power BI API authentication.\n","    \n","    In a Fabric notebook, the token is automatically available through mssparkutils.\n","    This function retrieves the token that's needed to authenticate with the Power BI REST API.\n","    \n","    Returns:\n","        str: The access token\n","    \n","    Note:\n","        mssparkutils is a utility library provided by Microsoft Fabric\n","        that handles authentication automatically.\n","    \"\"\"\n","    try:\n","        # In Fabric notebooks, we can get the token using mssparkutils\n","        from notebookutils import mssparkutils\n","        # For Power BI API, we need the Power BI service resource\n","        token_response = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n","        return token_response\n","    except Exception as e:\n","        logger.error(f\"Failed to get access token: {str(e)}\")\n","        raise\n","# ==================================\n","\n","\n","# CELL 6 - API Call Function\n","# ==================================\n","def call_powerbi_api(endpoint: str, access_token: str, params: Optional[Dict] = None) -> Dict:\n","    \"\"\"\n","    Make a REST API call to Power BI with advanced rate limit handling.\n","    \n","    This function handles the HTTP request to the Power BI API, including:\n","    - Setting up authentication headers\n","    - Managing retries with intelligent backoff for rate limiting (429 errors)\n","    - Implementing jitter to avoid synchronized retries\n","    - Detailed error handling and logging\n","    - Respecting the 200 requests per hour limit\n","    \n","    Args:\n","        endpoint: The API endpoint path (e.g., \"/admin/dataflows\")\n","        access_token: The Azure AD access token\n","        params: Optional query parameters for the API call\n","    \n","    Returns:\n","        dict: The JSON response from the API\n","    \n","    Raises:\n","        requests.exceptions.RequestException: If the API call fails after all retries\n","    \"\"\"\n","    url = f\"{CONFIG['API_BASE_URL']}{endpoint}\"\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    # Initialize backoff time\n","    backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","    \n","    # Retry logic with intelligent backoff\n","    for attempt in range(CONFIG['MAX_RETRIES']):\n","        try:\n","            # Log the full URL with parameters for debugging\n","            logger.info(f\"Making API call to: {url} with params: {params} (Attempt {attempt + 1})\")\n","            \n","            response = requests.get(\n","                url,\n","                headers=headers,\n","                params=params,\n","                timeout=CONFIG['TIMEOUT']\n","            )\n","            \n","            # Log the full response for debugging\n","            logger.info(f\"Response status: {response.status_code}\")\n","            \n","            # Rate limit handling (429 Too Many Requests)\n","            if response.status_code == 429:\n","                # Get retry-after header if available, otherwise use our backoff\n","                retry_after = response.headers.get('Retry-After')\n","                \n","                if retry_after and retry_after.isdigit():\n","                    # If server specified a wait time, use it\n","                    wait_time = int(retry_after)\n","                else:\n","                    # Calculate wait time with exponential backoff and jitter\n","                    jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                    wait_time = backoff_time + jitter\n","                    # Update backoff for next attempt\n","                    backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.warning(f\"Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","                continue  # Skip to next retry attempt without raising exception\n","            \n","            # Log the response for debugging in case of errors\n","            if response.status_code >= 400:\n","                logger.error(f\"API error: Status {response.status_code}, Response: {response.text}\")\n","                logger.error(f\"Request URL: {response.request.url}\")\n","                logger.error(f\"Request headers: {response.request.headers}\")\n","            \n","            # For all other status codes, use raise_for_status to handle\n","            response.raise_for_status()\n","            \n","            # If we get here, the request was successful\n","            # Reset backoff for next API call (not next attempt)\n","            backoff_time = CONFIG['INITIAL_BACKOFF_SEC']\n","            \n","            # Log a sample of the response content for debugging\n","            try:\n","                response_json = response.json()\n","                if \"value\" in response_json and isinstance(response_json[\"value\"], list):\n","                    logger.info(f\"Response contains {len(response_json['value'])} items in 'value' array\")\n","                return response_json\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Failed to parse response as JSON: {str(e)}\")\n","                logger.error(f\"Response content: {response.text[:1000]}\")  # Log first 1000 chars of response\n","                raise\n","            \n","        except requests.exceptions.RequestException as e:\n","            last_attempt = attempt == CONFIG['MAX_RETRIES'] - 1\n","            \n","            # Special handling for non-429 errors\n","            if not (hasattr(e, 'response') and e.response is not None and e.response.status_code == 429):\n","                logger.warning(f\"API call failed (Attempt {attempt + 1}): {str(e)}\")\n","                \n","                if last_attempt:\n","                    logger.error(f\"All retry attempts failed for endpoint: {endpoint}\")\n","                    logger.error(f\"Final error: {str(e)}\")\n","                    raise\n","                \n","                # Calculate wait time with exponential backoff and jitter\n","                jitter = random.uniform(0, CONFIG['JITTER_FACTOR'] * backoff_time)\n","                wait_time = backoff_time + jitter\n","                # Update backoff for next attempt\n","                backoff_time = min(backoff_time * CONFIG['BACKOFF_FACTOR'], CONFIG['MAX_BACKOFF_SEC'])\n","                \n","                logger.info(f\"Waiting {wait_time:.2f} seconds before retry.\")\n","                time.sleep(wait_time)\n","# ==================================\n","\n","\n","# CELL 7 - Get Dataflows Function\n","# ==================================\n","def get_dataflows(access_token: str) -> List[Dict]:\n","    \"\"\"\n","    Retrieve all dataflows from the Power BI API using GetDataflowsAsAdmin.\n","    \n","    This function makes a request to the GetDataflowsAsAdmin API endpoint to retrieve \n","    all dataflows. The API returns all dataflows in a single response without pagination.\n","    The API has a rate limit of 200 requests per hour.\n","    \n","    Args:\n","        access_token: The Azure AD access token\n","    \n","    Returns:\n","        list: A list of all dataflow objects\n","    \"\"\"\n","    logger.info(\"Retrieving all dataflows from Power BI API...\")\n","    \n","    try:\n","        # Make the API call without any pagination parameters\n","        # The GetDataflowsAsAdmin API returns all dataflows in a single response\n","        response_data = call_powerbi_api(CONFIG['DATAFLOWS_ENDPOINT'], access_token)\n","    except requests.exceptions.RequestException as e:\n","        logger.error(f\"API call failed: {str(e)}\")\n","        raise\n","    \n","    # Log the response structure for debugging\n","    if CONFIG['DEBUG_MODE']:\n","        logger.info(f\"Response keys: {list(response_data.keys())}\")\n","    \n","    # Extract dataflows from the response\n","    # Power BI API returns data in a \"value\" array\n","    dataflows = response_data.get(\"value\", [])\n","    \n","    if dataflows:\n","        logger.info(f\"Retrieved {len(dataflows)} dataflows from API\")\n","        \n","        # Log first dataflow for debugging\n","        if CONFIG['DEBUG_MODE'] and dataflows:\n","            logger.info(f\"Sample dataflow: {json.dumps(dataflows[0], indent=2)}\")\n","    else:\n","        logger.warning(\"No dataflows found in API response\")\n","    \n","    logger.info(f\"Finished retrieving all dataflows. Total count: {len(dataflows)}\")\n","    return dataflows\n","# ==================================\n","\n","\n","# CELL 8 - Create Dataflows DataFrame Function\n","# ==================================\n","def create_dataflows_dataframe(dataflows_data: List[Dict]) -> \"DataFrame\":\n","    \"\"\"\n","    Convert the dataflows data into a PySpark DataFrame for the pbi_dataflows table.\n","    \n","    This function:\n","    - Creates a structured DataFrame with the core dataflow metadata\n","    - Maps objectId to dataflowId as specified\n","    - Adds metadata columns for tracking\n","    - Handles nullable fields appropriately\n","    - Ensures all expected columns exist even if missing from API response\n","    - Maintains proper data types to avoid VOID column issues\n","    \n","    Args:\n","        dataflows_data: List of dataflow dictionaries from the API\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame ready for the pbi_dataflows Delta table\n","    \"\"\"\n","    # Define the schema first to ensure proper data types\n","    dataflows_schema = StructType([\n","        StructField(\"dataflowId\", StringType(), False),     # Primary key, not nullable\n","        StructField(\"name\", StringType(), True),            # Nullable\n","        StructField(\"description\", StringType(), True),     # Nullable\n","        StructField(\"modelUrl\", StringType(), True),        # Nullable\n","        StructField(\"configuredBy\", StringType(), True),    # Nullable\n","        StructField(\"workspaceId\", StringType(), True),     # Nullable\n","        StructField(\"extraction_timestamp\", TimestampType(), False)    # Not nullable\n","    ])\n","    \n","    # Handle empty data case\n","    if not dataflows_data:\n","        logger.warning(\"No dataflows found. Creating empty DataFrame.\")\n","        empty_rdd = spark.sparkContext.emptyRDD()\n","        dataflows_df = spark.createDataFrame(empty_rdd, dataflows_schema)\n","        return dataflows_df\n","    \n","    # Extract core dataflow fields\n","    simplified_dataflows = []\n","    \n","    for dataflow in dataflows_data:\n","        # Map the API response fields to our table schema\n","        # Handle cases where fields might be missing from the API response\n","        simplified_dataflow = {\n","            \"dataflowId\": dataflow.get(\"objectId\"),  # Rename objectId to dataflowId\n","            \"name\": dataflow.get(\"name\"),\n","            \"description\": dataflow.get(\"description\"),\n","            \"modelUrl\": dataflow.get(\"modelUrl\"),  # This field might not always be present\n","            \"configuredBy\": dataflow.get(\"configuredBy\"),\n","            \"workspaceId\": dataflow.get(\"workspaceId\")\n","        }\n","        simplified_dataflows.append(simplified_dataflow)\n","    \n","    # Convert to pandas DataFrame first for easier handling\n","    pandas_df = pd.DataFrame(simplified_dataflows)\n","    \n","    # Ensure all required columns exist - add missing columns with None values\n","    required_columns = [\"dataflowId\", \"name\", \"description\", \"modelUrl\", \"configuredBy\", \"workspaceId\"]\n","    for col_name in required_columns:\n","        if col_name not in pandas_df.columns:\n","            logger.warning(f\"Column '{col_name}' not found in API response. Adding as NULL column.\")\n","            pandas_df[col_name] = None\n","    \n","    # Log the actual columns we received for debugging\n","    if CONFIG['DEBUG_MODE']:\n","        logger.info(f\"Pandas DataFrame columns: {list(pandas_df.columns)}\")\n","        logger.info(f\"Sample record: {pandas_df.iloc[0].to_dict() if len(pandas_df) > 0 else 'No records'}\")\n","    \n","    # Create the Spark DataFrame with explicit schema to avoid VOID type issues\n","    # We'll create it step by step to ensure proper typing\n","    \n","    # First, create a basic DataFrame from pandas\n","    temp_df = spark.createDataFrame(pandas_df[required_columns])\n","    \n","    # Now cast all columns to the correct types to avoid VOID issues\n","    typed_df = temp_df.select(\n","        col(\"dataflowId\").cast(StringType()).alias(\"dataflowId\"),\n","        col(\"name\").cast(StringType()).alias(\"name\"),\n","        col(\"description\").cast(StringType()).alias(\"description\"),\n","        col(\"modelUrl\").cast(StringType()).alias(\"modelUrl\"),  # Explicitly cast to StringType\n","        col(\"configuredBy\").cast(StringType()).alias(\"configuredBy\"),\n","        col(\"workspaceId\").cast(StringType()).alias(\"workspaceId\")\n","    )\n","    \n","    # Add metadata columns\n","    dataflows_df = typed_df \\\n","        .withColumn(\"extraction_timestamp\", current_timestamp())\n","    \n","    # Log the final DataFrame schema for debugging\n","    if CONFIG['DEBUG_MODE']:\n","        logger.info(f\"Final DataFrame schema: {dataflows_df.schema}\")\n","        logger.info(f\"DataFrame count: {dataflows_df.count()}\")\n","    \n","    return dataflows_df\n","# ==================================\n","\n","\n","# CELL 9 - Delta Lake Operations Functions\n","# ==================================\n","def ensure_delta_table_exists(table_name: str, df_schema):\n","    \"\"\"\n","    Ensure the Delta table exists with the correct schema, creating it if necessary.\n","    If the table exists but has VOID columns, recreate it with proper types.\n","    \n","    Args:\n","        table_name: Name of the Delta table\n","        df_schema: Schema of the DataFrame\n","    \"\"\"\n","    try:\n","        # Check if table exists\n","        existing_table = spark.table(table_name)\n","        existing_schema = existing_table.schema\n","        \n","        # Check for VOID columns in existing schema\n","        has_void_columns = any(field.dataType.typeName() == 'void' for field in existing_schema.fields)\n","        \n","        if has_void_columns:\n","            logger.warning(f\"Table '{table_name}' exists but has VOID columns. Recreating with proper schema.\")\n","            \n","            # Drop the existing table\n","            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","            logger.info(f\"Dropped existing table '{table_name}' with VOID columns\")\n","            \n","            # Create new table with correct schema\n","            empty_df = spark.createDataFrame([], df_schema)\n","            empty_df.write \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .saveAsTable(table_name)\n","            \n","            logger.info(f\"Recreated Delta table '{table_name}' with proper schema\")\n","        else:\n","            logger.info(f\"Delta table '{table_name}' already exists with correct schema\")\n","            \n","    except Exception as table_check_error:\n","        # Table doesn't exist, create it\n","        logger.info(f\"Creating Delta table '{table_name}' - {str(table_check_error)}\")\n","        \n","        # Create an empty DataFrame with the schema\n","        empty_df = spark.createDataFrame([], df_schema)\n","        \n","        # Create the Delta table\n","        empty_df.write \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","        \n","        logger.info(f\"Delta table '{table_name}' created successfully\")\n","\n","\n","def merge_dataflows_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    Merge new dataflow data into the pbi_dataflows Delta table using MERGE operation.\n","    \n","    This function performs an upsert operation:\n","    - Updates existing records if dataflowId matches\n","    - Inserts new records if dataflowId doesn't exist\n","    - Handles schema compatibility issues\n","    \n","    Args:\n","        source_df: DataFrame with new dataflow data\n","        table_name: Name of the target Delta table\n","    \"\"\"\n","    logger.info(f\"Starting merge operation for {table_name}\")\n","    \n","    # First, let's check if we have any data to merge\n","    source_count = source_df.count()\n","    if source_count == 0:\n","        logger.warning(\"Source DataFrame is empty. Skipping merge operation.\")\n","        return\n","    \n","    logger.info(f\"Source DataFrame has {source_count} records\")\n","    \n","    # Get the target table\n","    target_df = spark.table(table_name)\n","    target_count = target_df.count()\n","    \n","    logger.info(f\"Target table has {target_count} records\")\n","    \n","    # Create a temporary view for the merge operation\n","    source_df.createOrReplaceTempView(\"dataflow_updates\")\n","    \n","    # If the table is empty, just insert all records\n","    if target_count == 0:\n","        logger.info(f\"Table {table_name} is empty. Inserting all records.\")\n","        source_df.write.mode(\"append\").saveAsTable(table_name)\n","        return\n","    \n","    # Perform the merge operation with explicit column mapping\n","    merge_query = f\"\"\"\n","    MERGE INTO {table_name} AS target\n","    USING dataflow_updates AS source\n","    ON target.dataflowId = source.dataflowId\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.name = source.name,\n","            target.description = source.description,\n","            target.modelUrl = source.modelUrl,\n","            target.configuredBy = source.configuredBy,\n","            target.workspaceId = source.workspaceId,\n","            target.extraction_timestamp = source.extraction_timestamp\n","    WHEN NOT MATCHED THEN\n","        INSERT (dataflowId, name, description, modelUrl, configuredBy, workspaceId, extraction_timestamp)\n","        VALUES (source.dataflowId, source.name, source.description, source.modelUrl, source.configuredBy, source.workspaceId, source.extraction_timestamp)\n","    \"\"\"\n","    \n","    try:\n","        spark.sql(merge_query)\n","        logger.info(\"Merge operation completed successfully\")\n","    except Exception as merge_error:\n","        logger.error(f\"Merge operation failed: {str(merge_error)}\")\n","        logger.info(\"Attempting to recreate table and insert all data...\")\n","        \n","        # If merge fails, we'll recreate the table and insert all data\n","        # This handles cases where schema is incompatible\n","        try:\n","            # Save current data from target table if it exists and has valid schema\n","            current_data = None\n","            try:\n","                current_data = spark.table(table_name)\n","                current_schema_valid = not any(field.dataType.typeName() == 'void' for field in current_data.schema.fields)\n","                if not current_schema_valid:\n","                    current_data = None\n","                    logger.info(\"Current table has invalid schema, will not preserve data\")\n","            except:\n","                current_data = None\n","            \n","            # Drop and recreate table\n","            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","            \n","            # Create new table with source data\n","            source_df.write \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .saveAsTable(table_name)\n","            \n","            # If we had valid current data, append it (avoiding duplicates)\n","            if current_data is not None and current_data.count() > 0:\n","                logger.info(\"Attempting to preserve existing valid data...\")\n","                try:\n","                    # Anti-join to get records that don't exist in source\n","                    existing_unique = current_data.join(\n","                        source_df.select(\"dataflowId\"), \n","                        on=\"dataflowId\", \n","                        how=\"left_anti\"\n","                    )\n","                    \n","                    if existing_unique.count() > 0:\n","                        existing_unique.write.mode(\"append\").saveAsTable(table_name)\n","                        logger.info(f\"Preserved {existing_unique.count()} existing records\")\n","                except Exception as preserve_error:\n","                    logger.warning(f\"Could not preserve existing data: {str(preserve_error)}\")\n","            \n","            logger.info(\"Table recreated successfully with new data\")\n","            \n","        except Exception as recreate_error:\n","            logger.error(f\"Failed to recreate table: {str(recreate_error)}\")\n","            raise\n","\n","\n","def merge_users_to_delta(source_df, table_name: str):\n","    \"\"\"\n","    This function has been removed as the pbi_dataflows_users table is no longer needed.\n","    \"\"\"\n","    pass\n","\n","\n","def optimize_delta_table(table_name: str):\n","    \"\"\"\n","    Optimize the Delta table for better query performance.\n","    \n","    This function:\n","    - Updates table statistics for query optimization\n","    - Uses a more compatible method for Microsoft Fabric\n","    \n","    Args:\n","        table_name: Name of the Delta table to optimize\n","    \"\"\"\n","    logger.info(f\"Optimizing Delta table '{table_name}'\")\n","    \n","    try:\n","        # Update table statistics for better query planning\n","        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n","        logger.info(\"Table statistics updated successfully\")\n","        \n","        # Note: In Microsoft Fabric, Delta table optimization may be handled automatically\n","        # or through different commands than traditional Delta Lake\n","        logger.info(\"Delta table optimization completed via statistics computation\")\n","        logger.info(\"Note: Microsoft Fabric may automatically optimize Delta tables\")\n","    except Exception as e:\n","        logger.warning(f\"Table optimization step encountered an issue: {str(e)}\")\n","        logger.info(\"Continuing with process - optimization is not critical for functionality\")\n","# ==================================\n","\n","\n","# CELL 10 - Main Execution Function\n","# ==================================\n","def main():\n","    \"\"\"\n","    Main execution function that orchestrates the entire process.\n","    \n","    This function:\n","    1. Gets the authentication token\n","    2. Retrieves all dataflows from the Power BI API\n","    3. Creates DataFrame for the dataflows table\n","    4. Loads data into Delta Lake table\n","    5. Optimizes the table for analytics\n","    \"\"\"\n","    try:\n","        logger.info(\"Starting Power BI Dataflows to Delta Lake process\")\n","        \n","        # Step 1: Get authentication token\n","        logger.info(\"Getting access token...\")\n","        access_token = get_access_token()\n","        logger.info(\"Successfully obtained access token\")\n","        \n","        # Step 2: Retrieve all dataflows\n","        logger.info(\"Retrieving dataflows from Power BI API...\")\n","        dataflows_data = get_dataflows(access_token)\n","        \n","        if not dataflows_data:\n","            logger.warning(\"No dataflows found. Please check your permissions and API access.\")\n","            # Create empty dataframe with schema for consistent table structure\n","            dataflows_df = create_dataflows_dataframe([])\n","        else:\n","            # Step 3: Create DataFrame\n","            logger.info(f\"Creating DataFrame for {len(dataflows_data)} dataflows...\")\n","            dataflows_df = create_dataflows_dataframe(dataflows_data)\n","        \n","        # Show sample data\n","        logger.info(\"Sample of dataflows data:\")\n","        dataflows_df.show(5, truncate=False)\n","        \n","        # Step 4: Prepare Delta table\n","        dataflows_table = CONFIG[\"DATAFLOWS_TABLE_NAME\"]\n","        ensure_delta_table_exists(dataflows_table, dataflows_df.schema)\n","        \n","        # Step 5: Merge data into Delta table (if we have data)\n","        if dataflows_data:\n","            # Load dataflows data\n","            merge_dataflows_to_delta(dataflows_df, dataflows_table)\n","            \n","            # Step 6: Optimize the Delta table\n","            optimize_delta_table(dataflows_table)\n","        \n","        # Step 7: Display final statistics\n","        logger.info(\"Loading completed successfully!\")\n","        \n","        # Show table information\n","        logger.info(f\"\\n=== {dataflows_table} Table Details ===\")\n","        spark.sql(f\"DESCRIBE DETAIL {dataflows_table}\").show(truncate=False)\n","        \n","        # Show row count\n","        dataflows_count = spark.table(dataflows_table).count()\n","        logger.info(f\"Total rows in {dataflows_table}: {dataflows_count}\")\n","        \n","        # Show comprehensive summary statistics\n","        try:\n","            summary_stats = spark.sql(f\"\"\"\n","                SELECT \n","                    COUNT(*) as total_dataflows,\n","                    COUNT(DISTINCT workspaceId) as unique_workspaces,\n","                    COUNT(DISTINCT configuredBy) as unique_owners,\n","                    COUNT(CASE WHEN description IS NOT NULL AND description != '' THEN 1 END) as dataflows_with_description,\n","                    COUNT(CASE WHEN modelUrl IS NOT NULL AND modelUrl != '' THEN 1 END) as dataflows_with_model_url,\n","                    MAX(extraction_timestamp) as last_updated\n","                FROM {dataflows_table}\n","            \"\"\")\n","            \n","            logger.info(f\"\\n=== {dataflows_table} Summary Statistics ===\")\n","            summary_stats.show(truncate=False)\n","        except Exception as e:\n","            logger.warning(f\"Error generating summary statistics: {str(e)}\")\n","            # Show basic count instead\n","            logger.info(f\"Basic row count for {dataflows_table}: {dataflows_count}\")\n","        \n","        # Show workspace distribution for dataflows\n","        try:\n","            workspace_distribution = spark.sql(f\"\"\"\n","                SELECT \n","                    workspaceId,\n","                    COUNT(*) as dataflow_count\n","                FROM {dataflows_table}\n","                WHERE workspaceId IS NOT NULL\n","                GROUP BY workspaceId\n","                ORDER BY dataflow_count DESC\n","                LIMIT 10\n","            \"\"\")\n","            \n","            logger.info(f\"\\n=== Top 10 Workspaces by Dataflow Count ===\")\n","            workspace_distribution.show(truncate=False)\n","        except Exception as e:\n","            logger.warning(f\"Error generating workspace distribution: {str(e)}\")\n","        \n","        # Return DataFrame for further analysis if needed\n","        return dataflows_df\n","        \n","    except Exception as e:\n","        logger.error(f\"Error in main execution: {str(e)}\")\n","        raise\n","# ==================================.show(truncate=False)\n","    except Exception as e:\n","                logger.warning(f\"Error generating principal type distribution: {str(e)}\")\n","        \n","# CELL 11 - Execute Main Function\n","# ==================================\n","# Execute the main function\n","if __name__ == \"__main__\":\n","    dataflows_df = main()\n","# =================================="],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"08f7bbd2-3093-4584-a86a-d1d10b5b1bdd","normalized_state":"finished","queued_time":"2025-07-17T15:23:25.0168725Z","session_start_time":null,"execution_start_time":"2025-07-17T15:23:36.7884269Z","execution_finish_time":"2025-07-17T15:24:11.3234159Z","parent_msg_id":"f1cd70d2-261e-4cd1-9c67-52bac8270c64"},"text/plain":"StatementMeta(, 08f7bbd2-3093-4584-a86a-d1d10b5b1bdd, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------------------------------------+----------------------------+----------------------------------------------------------+--------+------------------------+------------------------------------+--------------------------+\n|dataflowId                          |name                        |description                                               |modelUrl|configuredBy            |workspaceId                         |extraction_timestamp      |\n+------------------------------------+----------------------------+----------------------------------------------------------+--------+------------------------+------------------------------------+--------------------------+\n|aa0029b7-4f7e-47a2-9310-3434c3c7d41f|Master Nova Notes           |This dataflow is connecting to the Master Nova Notes list.|NULL    |eepena@mdanderson.org   |9beed1e6-1368-4a64-9cb9-1a2a9630eb92|2025-07-17 15:23:39.724431|\n|b0e50acc-edb0-4b21-b1e2-0562d9115888|EDI MOSAIQ Tx Start Dataflow|EDI MOSAIQ Tx Start Dataflow                              |NULL    |SAbraham7@mdanderson.org|49ec4821-c19f-4a69-ab00-53ed07fdc830|2025-07-17 15:23:39.724431|\n|f105292b-4a5b-4c0f-a471-984a1b069fc4|V_RadOnc_Worker             |                                                          |NULL    |SAbraham7@mdanderson.org|49ec4821-c19f-4a69-ab00-53ed07fdc830|2025-07-17 15:23:39.724431|\n|e305522c-46f2-4f72-bcf5-f82f929bf1c0|ROADS DEV SERVER STATS      |                                                          |NULL    |SAbraham7@mdanderson.org|49ec4821-c19f-4a69-ab00-53ed07fdc830|2025-07-17 15:23:39.724431|\n|c23e2ba5-95a5-4e98-b1b1-f430e5dec720|SATURN_MISSION              |                                                          |NULL    |SAbraham7@mdanderson.org|512b837e-b824-42f0-893d-ba83037425c5|2025-07-17 15:23:39.724431|\n+------------------------------------+----------------------------+----------------------------------------------------------+--------+------------------------+------------------------------------+--------------------------+\nonly showing top 5 rows\n\n+---------------+-----------------+-------------+--------------------------+------------------------+--------------------------+\n|total_dataflows|unique_workspaces|unique_owners|dataflows_with_description|dataflows_with_model_url|last_updated              |\n+---------------+-----------------+-------------+--------------------------+------------------------+--------------------------+\n|258            |96               |46           |44                        |0                       |2025-07-17 15:23:53.531813|\n+---------------+-----------------+-------------+--------------------------+------------------------+--------------------------+\n\n+------------------------------------+--------------+\n|workspaceId                         |dataflow_count|\n+------------------------------------+--------------+\n|8fc8156a-a7d1-40ae-8c3d-9e586cca79a9|18            |\n|f8509758-5233-42ef-89bb-6eb7709128e0|17            |\n|8e1d8f1d-2fcb-4b72-b6f3-ac228bc9d88c|13            |\n|21c0515f-4846-4031-8f7f-fa710636f7ec|11            |\n|f9f25eb2-e5ee-49c5-b734-f16a9d06cc46|10            |\n|cd674753-2093-4a01-8b4b-71106bbdc2d3|10            |\n|ae6f9a52-b1e2-41ab-b6c6-ca21c9451e10|8             |\n|ab61685b-f757-4b57-a897-e9dba2dbf293|7             |\n|93361046-f97a-4666-918c-bcddc636979b|6             |\n|d4dd798c-fb45-451a-a4dc-a7562ac4415a|6             |\n+------------------------------------+--------------+\n\n"]},{"output_type":"stream","name":"stderr","text":["2025-07-17 15:23:49,625 - INFO - Delta table 'pbi_dataflows' already exists with correct schema\n2025-07-17 15:23:49,626 - INFO - Starting merge operation for pbi_dataflows\n2025-07-17 15:23:49,803 - INFO - Source DataFrame has 257 records\n2025-07-17 15:23:53,156 - INFO - Target table has 241 records\n2025-07-17 15:24:03,972 - INFO - Merge operation completed successfully\n2025-07-17 15:24:03,973 - INFO - Optimizing Delta table 'pbi_dataflows'\n2025-07-17 15:24:07,902 - INFO - Total rows in pbi_dataflows: 258\n2025-07-17 15:24:08,113 - INFO - \n=== pbi_dataflows Summary Statistics ===\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f042c43f-91aa-44b7-990f-e8160b852f74"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# create Spark session\n","spark = SparkSession.builder.appName(\"Refresh SQL Endpoint Metadata\").getOrCreate()\n","\n","# refresh the specific table\n","spark.sql(\"REFRESH TABLE pbi_dataflows\")\n","print(\"Metadata refresh triggered successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"08f7bbd2-3093-4584-a86a-d1d10b5b1bdd","normalized_state":"finished","queued_time":"2025-07-17T15:23:25.128493Z","session_start_time":null,"execution_start_time":"2025-07-17T15:24:11.3259098Z","execution_finish_time":"2025-07-17T15:24:12.178335Z","parent_msg_id":"31bec8c5-a5d9-47e3-9153-c55eef101b72"},"text/plain":"StatementMeta(, 08f7bbd2-3093-4584-a86a-d1d10b5b1bdd, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metadata refresh triggered successfully.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0c4ee38d-394b-4598-bf89-4b9865a2a8cb"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}