{"cells":[{"cell_type":"code","source":["# This gets details about capacities and writes them to a table in the Lakehouse\n","\n","# Import necessary libraries\n","import requests\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","import json\n","\n","# Note: Microsoft Fabric notebooks automatically authenticate your requests\n","# The authentication token is available through the Fabric runtime\n","\n","def get_fabric_capacities():\n","    \"\"\"\n","    Retrieves a list of Microsoft Fabric capacities using the REST API.\n","    This function handles authentication, API calls, and data processing.\n","    \n","    Returns:\n","        DataFrame: A PySpark DataFrame containing capacity information\n","    \"\"\"\n","    \n","    # Step 1: Get the authentication token from Fabric\n","    # In Fabric notebooks, the authentication is handled automatically\n","    # The token is available through the mssparkutils library\n","    try:\n","        # Access the token using mssparkutils (available in Fabric notebooks)\n","        token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","    except Exception as e:\n","        print(f\"Error getting authentication token: {e}\")\n","        print(\"Make sure you're running this in a Microsoft Fabric notebook\")\n","        return None\n","    \n","    # Step 2: Set up the API endpoint and headers\n","    # The base URL for Fabric REST API\n","    base_url = \"https://api.fabric.microsoft.com/v1\"\n","    \n","    # The specific endpoint for listing capacities\n","    endpoint = \"/capacities\"\n","    \n","    # Complete URL\n","    url = base_url + endpoint\n","    \n","    # Headers required for the API call\n","    headers = {\n","        \"Authorization\": f\"Bearer {token}\",  # Authentication token\n","        \"Content-Type\": \"application/json\"   # Specify we want JSON response\n","    }\n","    \n","    # Step 3: Initialize variables for pagination\n","    # The API may return results in multiple pages\n","    all_capacities = []  # List to store all capacities\n","    continuation_token = None  # Token for getting next page of results\n","    \n","    # Step 4: Make API calls with pagination support\n","    while True:\n","        try:\n","            # Prepare the request parameters\n","            params = {}\n","            if continuation_token:\n","                # If we have a continuation token, add it to get the next page\n","                params['continuationToken'] = continuation_token\n","            \n","            # Make the GET request to the API\n","            response = requests.get(url, headers=headers, params=params)\n","            \n","            # Check if the request was successful\n","            if response.status_code == 200:\n","                # Parse the JSON response\n","                data = response.json()\n","                \n","                # Extract the capacity data\n","                capacities = data.get('value', [])\n","                all_capacities.extend(capacities)\n","                \n","                # Check if there are more pages\n","                continuation_token = data.get('continuationToken')\n","                \n","                # If no continuation token, we've got all the data\n","                if not continuation_token:\n","                    break\n","                    \n","                print(f\"Retrieved {len(capacities)} capacities. Fetching more...\")\n","                \n","            else:\n","                # If the request failed, print error information\n","                print(f\"Error: HTTP {response.status_code}\")\n","                print(f\"Response: {response.text}\")\n","                return None\n","                \n","        except Exception as e:\n","            print(f\"Error making API request: {e}\")\n","            return None\n","    \n","    # Step 5: Convert the data to a PySpark DataFrame\n","    if all_capacities:\n","        print(f\"Total capacities retrieved: {len(all_capacities)}\")\n","        \n","        # Get or create a SparkSession\n","        spark = SparkSession.builder.appName(\"FabricCapacities\").getOrCreate()\n","        \n","        # Convert the list of dictionaries to a PySpark DataFrame\n","        df = spark.createDataFrame(all_capacities)\n","        \n","        return df\n","    else:\n","        print(\"No capacities found\")\n","        return None\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Call the function to get capacities\n","    print(\"Retrieving Microsoft Fabric capacities...\")\n","    capacities_df = get_fabric_capacities()\n","    \n","    # Display the results if we got data\n","    if capacities_df:\n","        # Show the schema of the DataFrame\n","        print(\"\\nDataFrame Schema:\")\n","        capacities_df.printSchema()\n","        \n","        # Display the first few records\n","        print(\"\\nFirst 10 capacities:\")\n","        capacities_df.show(10, truncate=False)\n","        \n","        # Show total count\n","        total_count = capacities_df.count()\n","        print(f\"\\nTotal number of capacities: {total_count}\")\n","        \n","        # Step 6: Write the data to Lakehouse file named BHCap\n","        try:\n","            # Define the path for the Lakehouse table\n","            # In Fabric, the default lakehouse is automatically mounted\n","            table_name = \"BHCap\"\n","            \n","            # Write the DataFrame to the Lakehouse as a managed table\n","            # This creates a Delta table in the default Lakehouse\n","            print(f\"\\nWriting data to Lakehouse table: {table_name}\")\n","            \n","            capacities_df.write \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .saveAsTable(table_name)\n","            \n","            print(f\"Successfully wrote {total_count} records to table: {table_name}\")\n","            \n","            # Verify the data was written by reading it back\n","            print(f\"\\nVerifying data in table: {table_name}\")\n","            verification_df = spark.read.table(table_name)\n","            print(f\"Records in table: {verification_df.count()}\")\n","            \n","            # Optionally, also write as a parquet file for direct file access\n","            # This writes to the Files section of the Lakehouse\n","            file_path = f\"Files/{table_name}\"\n","            \n","            print(f\"\\nAlso writing as parquet file to: {file_path}\")\n","            capacities_df.write \\\n","                .mode(\"overwrite\") \\\n","                .parquet(file_path)\n","            \n","            print(f\"Successfully wrote parquet file to: {file_path}\")\n","            \n","        except Exception as e:\n","            print(f\"Error writing to Lakehouse: {e}\")\n","            print(\"Make sure you have a default Lakehouse attached to this notebook\")\n","            \n","        # Example: Query the newly created table\n","        print(\"\\nExample query from the new table:\")\n","        spark.sql(f\"SELECT id, displayName, sku, state FROM {table_name} LIMIT 5\").show()\n","        \n","    else:\n","        print(\"Failed to retrieve capacities\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"3291f23c-f15a-4074-9db2-7dc45c97b96d","normalized_state":"finished","queued_time":"2025-05-13T16:16:23.0029932Z","session_start_time":null,"execution_start_time":"2025-05-13T16:16:23.0046464Z","execution_finish_time":"2025-05-13T16:17:09.7630524Z","parent_msg_id":"5b19d0bf-7177-4129-b409-1d9cf66e8480"},"text/plain":"StatementMeta(, 3291f23c-f15a-4074-9db2-7dc45c97b96d, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Retrieving Microsoft Fabric capacities...\nTotal capacities retrieved: 8\n\nDataFrame Schema:\nroot\n |-- displayName: string (nullable = true)\n |-- id: string (nullable = true)\n |-- region: string (nullable = true)\n |-- sku: string (nullable = true)\n |-- state: string (nullable = true)\n\n\nFirst 10 capacities:\n+---------------------------------+------------------------------------+----------------+---+--------+\n|displayName                      |id                                  |region          |sku|state   |\n+---------------------------------+------------------------------------+----------------+---+--------+\n|f64nonprodsouthcentral001        |ab3b62c5-cff1-4341-a584-4ef86a529e8a|South Central US|F64|Inactive|\n|f8nonprodsouthcentral001         |250aef2d-b24b-43a8-8564-8fefc5152522|South Central US|F8 |Active  |\n|f64x002                          |d94bc350-4bb9-4f24-9d89-fd633996eb28|South Central US|F64|Active  |\n|f64x001                          |56125c55-2f69-4fa3-bac0-e9407fc17374|South Central US|F64|Active  |\n|f32x001                          |8e0020ba-3162-4e4d-9d3f-83b6ce695c5d|South Central US|F32|Active  |\n|f16x001                          |4dc39e58-c232-494c-b629-45298de2fa27|South Central US|F16|Active  |\n|MDA Institutional Capacity - PROD|c73a5223-9ef6-4514-83cc-3e70297ee377|West US         |P1 |Active  |\n|Premium Per User - Reserved      |db9a247d-9b0d-4c13-8ad3-443fa3a6b50a|West US         |PP3|Active  |\n+---------------------------------+------------------------------------+----------------+---+--------+\n\n\nTotal number of capacities: 8\n\nWriting data to Lakehouse table: BHCap\nSuccessfully wrote 8 records to table: BHCap\n\nVerifying data in table: BHCap\nRecords in table: 8\n\nAlso writing as parquet file to: Files/BHCap\nSuccessfully wrote parquet file to: Files/BHCap\n\nExample query from the new table:\n+--------------------+--------------------+---+--------+\n|                  id|         displayName|sku|   state|\n+--------------------+--------------------+---+--------+\n|d94bc350-4bb9-4f2...|             f64x002|F64|  Active|\n|56125c55-2f69-4fa...|             f64x001|F64|  Active|\n|8e0020ba-3162-4e4...|             f32x001|F32|  Active|\n|4dc39e58-c232-494...|             f16x001|F16|  Active|\n|ab3b62c5-cff1-434...|f64nonprodsouthce...|F64|Inactive|\n+--------------------+--------------------+---+--------+\n\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d6f5e11-a0a2-4c14-b53d-2e6939991179"},{"cell_type":"code","source":["# This simply gets details about Gateway servers.\n","\n","# Import required libraries\n","import requests\n","import json\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, ArrayType, BooleanType, IntegerType\n","import pandas as pd\n","\n","# Initialize Spark session (already available in Fabric Notebooks as 'spark')\n","# spark = SparkSession.builder.appName(\"FabricGatewayAPI\").getOrCreate()\n","\n","# Configuration\n","# The base URL for Fabric API endpoints\n","BASE_URL = \"https://api.fabric.microsoft.com/v1\"\n","GATEWAYS_ENDPOINT = \"/gateways\"\n","# Filter for OnPremises gateways only\n","GATEWAY_TYPE_FILTER = \"OnPremises\"\n","\n","# Authentication setup\n","# In Microsoft Fabric notebooks, you can use the built-in token provider\n","# This function gets the access token automatically\n","from notebookutils import mssparkutils\n","\n","try:\n","    access_token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n","    print(f\"Successfully obtained access token (length: {len(access_token)})\")\n","except Exception as e:\n","    print(f\"Failed to get access token: {str(e)}\")\n","    access_token = None\n","\n","# Set up headers with authentication\n","headers = {\n","    \"Authorization\": f\"Bearer {access_token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# Debug: Print headers (without showing full token)\n","print(f\"Headers prepared. Authorization header present: {'Authorization' in headers}\")\n","\n","# Function to make API call and handle pagination\n","def get_onpremises_gateways(base_url, endpoint, headers, max_rows=100):\n","    \"\"\"\n","    Retrieves OnPremises gateway data from Fabric API\n","    \n","    Parameters:\n","    - base_url: The base URL for the API\n","    - endpoint: The specific endpoint for gateways\n","    - headers: Authentication headers\n","    - max_rows: Maximum number of rows to retrieve (default: 100)\n","    \n","    Returns:\n","    - List of OnPremises gateway objects\n","    \"\"\"\n","    \n","    all_gateways = []\n","    next_link = f\"{base_url}{endpoint}\"\n","    rows_retrieved = 0\n","    \n","    while next_link and rows_retrieved < max_rows:\n","        try:\n","            # Make the API request\n","            print(f\"Requesting: {next_link}\")\n","            response = requests.get(next_link, headers=headers)\n","            \n","            # Check if request was successful\n","            if response.status_code == 200:\n","                data = response.json()\n","                print(f\"Response received with {len(data.get('value', []))} items\")\n","                \n","                # Extract gateways from the response\n","                gateways = data.get('value', [])\n","                \n","                # Filter for OnPremises gateways only\n","                onpremises_gateways = [g for g in gateways if g.get('type') == 'OnPremises']\n","                print(f\"Found {len(onpremises_gateways)} OnPremises gateways in this batch\")\n","                \n","                # Calculate how many rows to add\n","                remaining_rows = max_rows - rows_retrieved\n","                rows_to_add = len(onpremises_gateways) if len(onpremises_gateways) < remaining_rows else remaining_rows\n","                all_gateways.extend(onpremises_gateways[:rows_to_add])\n","                rows_retrieved += rows_to_add\n","                \n","                # Get the next page URL if it exists\n","                next_link = data.get('@odata.nextLink', None)\n","                \n","                # Exit if we've retrieved enough rows\n","                if rows_retrieved >= max_rows:\n","                    break\n","                    \n","            else:\n","                print(f\"Error: API request failed with status code {response.status_code}\")\n","                print(f\"Response: {response.text}\")\n","                break\n","                \n","        except Exception as e:\n","            print(f\"Exception occurred: {str(e)}\")\n","            import traceback\n","            print(traceback.format_exc())\n","            break\n","    \n","    return all_gateways\n","\n","# Call the API to get gateway data\n","print(\"Fetching OnPremises gateway data from Fabric API...\")\n","gateways_data = get_onpremises_gateways(BASE_URL, GATEWAYS_ENDPOINT, headers, max_rows=100)\n","\n","# Check if we got any data\n","if not gateways_data:\n","    print(\"No OnPremises gateway data retrieved. Please check your permissions and API connectivity.\")\n","else:\n","    print(f\"Successfully retrieved {len(gateways_data)} OnPremises gateway records\")\n","    \n","    # Convert to DataFrame for easier manipulation\n","    # First, let's examine the structure of the data\n","    if gateways_data:\n","        print(\"\\nSample OnPremises gateway record structure:\")\n","        print(json.dumps(gateways_data[0], indent=2))\n","    \n","    # Create a Spark DataFrame from the data\n","    # Define schema for OnPremises gateways based on the exact API documentation\n","    schema = StructType([\n","        StructField(\"allowCloudConnectionRefresh\", BooleanType(), True),\n","        StructField(\"allowCustomConnectors\", BooleanType(), True),\n","        StructField(\"displayName\", StringType(), True),\n","        StructField(\"id\", StringType(), True),  # uuid format\n","        StructField(\"loadBalancingSetting\", StringType(), True),  # It's just a string value\n","        StructField(\"numberOfMemberGateways\", StringType(), True),  # Should be integer but API returns string\n","        StructField(\"type\", StringType(), True),\n","        StructField(\"version\", StringType(), True)\n","    ])\n","    \n","    # Create Spark DataFrame\n","    df = spark.createDataFrame(gateways_data, schema=schema)\n","    \n","    # Display ALL columns with no truncation\n","    print(\"\\nShowing all OnPremises gateway data (core properties only):\")\n","    df.show(truncate=False)\n","    \n","    # Display all columns and their values in a more readable format\n","    print(\"\\nDetailed view of OnPremises gateways:\")\n","    for row in df.collect():\n","        print(\"\\n\" + \"=\"*50)\n","        print(f\"Gateway Display Name: {row.displayName}\")\n","        print(f\"Gateway ID: {row.id}\")\n","        print(\"=\"*50)\n","        \n","        # Core OnPremises properties\n","        print(f\"Type: {row.type}\")\n","        print(f\"Version: {row.version}\")\n","        print(f\"Allow Cloud Connection Refresh: {row.allowCloudConnectionRefresh}\")\n","        print(f\"Allow Custom Connectors: {row.allowCustomConnectors}\")\n","        print(f\"Number of Member Gateways: {row.numberOfMemberGateways}\")\n","        \n","        # Load Balancing Setting\n","        print(f\"Load Balancing Setting: {row.loadBalancingSetting}\")\n","    \n","    # Summary statistics for OnPremises gateways\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"SUMMARY STATISTICS FOR ONPREMISES GATEWAYS\")\n","    print(\"=\"*50)\n","    \n","    # Total count\n","    print(f\"\\nTotal OnPremises Gateways: {df.count()}\")\n","    \n","    # Cloud connection refresh enabled\n","    cloud_refresh_count = df.filter(df.allowCloudConnectionRefresh == True).count()\n","    print(f\"\\nGateways with Cloud Connection Refresh Enabled: {cloud_refresh_count}\")\n","    \n","    # Custom connectors enabled\n","    custom_connectors_count = df.filter(df.allowCustomConnectors == True).count()\n","    print(f\"Gateways with Custom Connectors Enabled: {custom_connectors_count}\")\n","    \n","    # Version distribution\n","    print(\"\\nGateways by Version:\")\n","    df.groupBy(\"version\").count().orderBy(\"count\", ascending=False).show()\n","    \n","    # Member gateway statistics\n","    print(\"\\nGateway Member Count Statistics:\")\n","    from pyspark.sql.functions import col, avg, max as spark_max, min as spark_min\n","    \n","    member_stats = df.agg(\n","        avg(col(\"numberOfMemberGateways\").cast(\"integer\")).alias(\"avg_members\"),\n","        spark_max(col(\"numberOfMemberGateways\").cast(\"integer\")).alias(\"max_members\"),\n","        spark_min(col(\"numberOfMemberGateways\").cast(\"integer\")).alias(\"min_members\")\n","    ).collect()[0]\n","    \n","    print(f\"Average Member Gateways: {member_stats.avg_members:.2f}\")\n","    print(f\"Maximum Member Gateways: {member_stats.max_members}\")\n","    print(f\"Minimum Member Gateways: {member_stats.min_members}\")\n","    \n","    # Display only the exact properties in a clean table format\n","    print(\"\\nOnPremises Gateways - Core Properties Only:\")\n","    selected_columns = [\n","        \"id\",\n","        \"displayName\", \n","        \"type\",\n","        \"version\",\n","        \"allowCloudConnectionRefresh\",\n","        \"allowCustomConnectors\",\n","        \"numberOfMemberGateways\"\n","    ]\n","    df.select(selected_columns).show(truncate=False)\n","    \n","    # Export to Pandas for detailed analysis\n","    pandas_df = df.toPandas()\n","    print(\"\\n\\nData exported to Pandas DataFrame for further analysis\")\n","    \n","    # Create a simple summary DataFrame with just the documented fields\n","    summary_df = df.select(selected_columns)\n","    \n","    # Save to Fabric table if needed\n","    # summary_df.write.mode(\"overwrite\").saveAsTable(\"onpremises_gateway_metadata\")\n","\n","print(\"\\nScript execution completed successfully!\")\n","\n","# Create a final clean display of just the documented OnPremises properties\n","if gateways_data:\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"ONPREMISES GATEWAYS - DOCUMENTED PROPERTIES ONLY\")\n","    print(\"=\"*50)\n","    \n","    # Clean display of documented properties in a structured format\n","    for i, gateway in enumerate(gateways_data):\n","        print(f\"\\nGateway {i+1}:\")\n","        print(f\"  allowCloudConnectionRefresh: {gateway.get('allowCloudConnectionRefresh', 'N/A')}\")\n","        print(f\"  allowCustomConnectors: {gateway.get('allowCustomConnectors', 'N/A')}\")\n","        print(f\"  displayName: {gateway.get('displayName', 'N/A')}\")\n","        print(f\"  id: {gateway.get('id', 'N/A')}\")\n","        print(f\"  loadBalancingSetting: {gateway.get('loadBalancingSetting', 'N/A')}\")\n","        print(f\"  numberOfMemberGateways: {gateway.get('numberOfMemberGateways', 'N/A')}\")\n","        print(f\"  type: {gateway.get('type', 'N/A')}\")\n","        print(f\"  version: {gateway.get('version', 'N/A')}\")\n","        \n","        if i >= 9:  # Show first 10 gateways for brevity\n","            remaining = len(gateways_data) - 10\n","            if remaining > 0:\n","                print(f\"\\n... and {remaining} more gateways\")\n","            break"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"2871b78c-e614-4223-a674-3ad3c4c4e8cc","normalized_state":"finished","queued_time":"2025-05-13T19:08:39.1346764Z","session_start_time":null,"execution_start_time":"2025-05-13T19:08:39.1364356Z","execution_finish_time":"2025-05-13T19:08:59.8288329Z","parent_msg_id":"a9387c67-0c7a-47ec-a2f1-94b9d0074016"},"text/plain":"StatementMeta(, 2871b78c-e614-4223-a674-3ad3c4c4e8cc, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully obtained access token (length: 2412)\nHeaders prepared. Authorization header present: True\nFetching OnPremises gateway data from Fabric API...\nRequesting: https://api.fabric.microsoft.com/v1/gateways\nResponse received with 2 items\nFound 2 OnPremises gateways in this batch\nSuccessfully retrieved 2 OnPremises gateway records\n\nSample OnPremises gateway record structure:\n{\n  \"displayName\": \"FABDGWAY1-POC\",\n  \"numberOfMemberGateways\": 3,\n  \"loadBalancingSetting\": \"DistributeEvenly\",\n  \"allowCloudConnectionRefresh\": false,\n  \"allowCustomConnectors\": false,\n  \"publicKey\": {\n    \"exponent\": \"AQAB\",\n    \"modulus\": \"tX3pIxSCNQky3usQp8ufCXXiNO5Nrsx9AJPq/lMoqPJmEAH/Qu3UEJlfsuHjw64cWXr2zs+D4z/CKntoYknKpTkf23EhMFR2pa+IOgNEUYfo8adebbEXlMlp6aiJ5ZYG+2zRsC/GkgfBFXeDeIhmnXgOgHKAr4DkjxmvJ5DCIMELlutEvrU+VsoemT1zE4dYFOVvKjDn88Q1yClZ4CayblxQh6jT4W6ziAghCdqBXV0Psp44nZwrMJw4ysWTptiVtD36ZkH6hF1rOOFaFL+zfB+vGzFy4AuSg2fuEyWPOg+u6wCiqFi/N77/+LFsX77Zk6J8iYII89hywgQM2sZmSQ==\"\n  },\n  \"version\": \"3000.266.4\",\n  \"id\": \"ebbbe00f-2b98-441b-92c0-1d820f987d8a\",\n  \"type\": \"OnPremises\"\n}\n\nShowing all OnPremises gateway data (core properties only):\n+---------------------------+---------------------+-------------+------------------------------------+--------------------+----------------------+----------+----------+\n|allowCloudConnectionRefresh|allowCustomConnectors|displayName  |id                                  |loadBalancingSetting|numberOfMemberGateways|type      |version   |\n+---------------------------+---------------------+-------------+------------------------------------+--------------------+----------------------+----------+----------+\n|false                      |false                |FABDGWAY1-POC|ebbbe00f-2b98-441b-92c0-1d820f987d8a|DistributeEvenly    |3                     |OnPremises|3000.266.4|\n|false                      |false                |FABDGWAY1-PRD|354cf43d-a895-4cab-b3ae-0c622559760b|DistributeEvenly    |3                     |OnPremises|3000.266.4|\n+---------------------------+---------------------+-------------+------------------------------------+--------------------+----------------------+----------+----------+\n\n\nDetailed view of OnPremises gateways:\n\n==================================================\nGateway Display Name: FABDGWAY1-POC\nGateway ID: ebbbe00f-2b98-441b-92c0-1d820f987d8a\n==================================================\nType: OnPremises\nVersion: 3000.266.4\nAllow Cloud Connection Refresh: False\nAllow Custom Connectors: False\nNumber of Member Gateways: 3\nLoad Balancing Setting: DistributeEvenly\n\n==================================================\nGateway Display Name: FABDGWAY1-PRD\nGateway ID: 354cf43d-a895-4cab-b3ae-0c622559760b\n==================================================\nType: OnPremises\nVersion: 3000.266.4\nAllow Cloud Connection Refresh: False\nAllow Custom Connectors: False\nNumber of Member Gateways: 3\nLoad Balancing Setting: DistributeEvenly\n\n==================================================\nSUMMARY STATISTICS FOR ONPREMISES GATEWAYS\n==================================================\n\nTotal OnPremises Gateways: 2\n\nGateways with Cloud Connection Refresh Enabled: 0\nGateways with Custom Connectors Enabled: 0\n\nGateways by Version:\n+----------+-----+\n|   version|count|\n+----------+-----+\n|3000.266.4|    2|\n+----------+-----+\n\n\nGateway Member Count Statistics:\nAverage Member Gateways: 3.00\nMaximum Member Gateways: 3\nMinimum Member Gateways: 3\n\nOnPremises Gateways - Core Properties Only:\n+------------------------------------+-------------+----------+----------+---------------------------+---------------------+----------------------+\n|id                                  |displayName  |type      |version   |allowCloudConnectionRefresh|allowCustomConnectors|numberOfMemberGateways|\n+------------------------------------+-------------+----------+----------+---------------------------+---------------------+----------------------+\n|ebbbe00f-2b98-441b-92c0-1d820f987d8a|FABDGWAY1-POC|OnPremises|3000.266.4|false                      |false                |3                     |\n|354cf43d-a895-4cab-b3ae-0c622559760b|FABDGWAY1-PRD|OnPremises|3000.266.4|false                      |false                |3                     |\n+------------------------------------+-------------+----------+----------+---------------------------+---------------------+----------------------+\n\n\n\nData exported to Pandas DataFrame for further analysis\n\nScript execution completed successfully!\n\n==================================================\nONPREMISES GATEWAYS - DOCUMENTED PROPERTIES ONLY\n==================================================\n\nGateway 1:\n  allowCloudConnectionRefresh: False\n  allowCustomConnectors: False\n  displayName: FABDGWAY1-POC\n  id: ebbbe00f-2b98-441b-92c0-1d820f987d8a\n  loadBalancingSetting: DistributeEvenly\n  numberOfMemberGateways: 3\n  type: OnPremises\n  version: 3000.266.4\n\nGateway 2:\n  allowCloudConnectionRefresh: False\n  allowCustomConnectors: False\n  displayName: FABDGWAY1-PRD\n  id: 354cf43d-a895-4cab-b3ae-0c622559760b\n  loadBalancingSetting: DistributeEvenly\n  numberOfMemberGateways: 3\n  type: OnPremises\n  version: 3000.266.4\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bae44ee3-d0ae-4eaf-8159-965d3638e8f7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"51872361-e484-4aa7-a0b4-853eaa971e47","known_lakehouses":[{"id":"51872361-e484-4aa7-a0b4-853eaa971e47"}],"default_lakehouse_name":"FabricAdmin_Lakehouse","default_lakehouse_workspace_id":"7a21dc44-c8b8-446e-9e80-59458a88ece8"}}},"nbformat":4,"nbformat_minor":5}